{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "!pip uninstall -y numpy transformers datasets\n",
    "!pip install numpy --force-reinstall --no-cache-dir\n",
    "!pip install transformers datasets --force-reinstall --no-cache-dir\n",
    "\n",
    "\n",
    "os.kill(os.getpid(), 9)  # Restart the Colab runtime (REQUIRED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/babylm/baseline-pretraining.git\n",
    "%cd baseline-pretraining\n",
    "!wget -O babylm_data.zip \"https://files.osf.io/v1/resources/ad7qg/providers/osfstorage/661517db943bee3731dfec25/?zip=\"\n",
    "!unzip babylm_data.zip -d babylm_data\n",
    "!unzip babylm_data/train_10M.zip -d babylm_data/train_10M\n",
    "!unzip babylm_data/dev.zip -d babylm_data/dev\n",
    "!unzip babylm_data/test.zip -d babylm_data/test\n",
    "!cat babylm_data/train_10M/train_10M/*.train > babylm_data/babylm_train.txt\n",
    "!cat babylm_data/dev/dev/*.dev > babylm_data/babylm_dev.txt\n",
    "!cat babylm_data/test/test/*.test > babylm_data/babylm_test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ENHANCED DOMAIN-AWARE T5 TRAINING SYSTEM WITH ROBUST SAVE/LOAD\n",
    "# =============================================================================\n",
    "\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import hashlib\n",
    "import json\n",
    "import time\n",
    "from typing import Dict, List, Tuple, Optional, Union, Any\n",
    "from collections import Counter, defaultdict\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from functools import lru_cache\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_random_seeds(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_random_seeds()\n",
    "\n",
    "# =============================================================================\n",
    "# ROBUST DATA MANAGEMENT SYSTEM\n",
    "# =============================================================================\n",
    "class RobustDataManager:\n",
    "    \"\"\"Enhanced data management with pattern matching for datasets\"\"\"\n",
    "\n",
    "    def __init__(self, base_dir: str, project_name: str = \"t5_training\"):\n",
    "        self.base_dir = Path(base_dir)\n",
    "        self.project_name = project_name\n",
    "\n",
    "        # Create structured directory hierarchy\n",
    "        self.structure = {\n",
    "            'root': self.base_dir / project_name,\n",
    "            'datasets': self.base_dir / project_name / 'datasets',\n",
    "            'preprocessed': self.base_dir / project_name / 'datasets' / 'preprocessed',\n",
    "            'complexity_cache': self.base_dir / project_name / 'datasets' / 'complexity_cache',\n",
    "            'models': self.base_dir / project_name / 'models',\n",
    "            'checkpoints': self.base_dir / project_name / 'models' / 'checkpoints',\n",
    "            'final_models': self.base_dir / project_name / 'models' / 'final',\n",
    "            'logs': self.base_dir / project_name / 'logs',\n",
    "            'metadata': self.base_dir / project_name / 'metadata'\n",
    "        }\n",
    "\n",
    "        # Create all directories\n",
    "        for name, path in self.structure.items():\n",
    "            path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Version for cache invalidation\n",
    "        self.version = \"v3.0_stable\"\n",
    "\n",
    "        print(f\"Data Manager initialized at: {self.structure['root']}\")\n",
    "        self._log_directory_structure()\n",
    "\n",
    "    def _log_directory_structure(self):\n",
    "        \"\"\"Log the created directory structure\"\"\"\n",
    "        print(\"Directory structure:\")\n",
    "        for name, path in self.structure.items():\n",
    "            print(f\"  {name}: {path}\")\n",
    "\n",
    "    def get_path(self, path_type: str) -> Path:\n",
    "        \"\"\"Get path for specific data type\"\"\"\n",
    "        if path_type not in self.structure:\n",
    "            raise ValueError(f\"Unknown path type: {path_type}\")\n",
    "        return self.structure[path_type]\n",
    "\n",
    "    def generate_cache_key(self, data_source: str, config_params: Dict) -> str:\n",
    "        \"\"\"Generate unique cache key based on data source and configuration\"\"\"\n",
    "        try:\n",
    "            if os.path.exists(data_source):\n",
    "                file_stat = os.stat(data_source)\n",
    "                source_hash = f\"size_{file_stat.st_size}\"\n",
    "            else:\n",
    "                source_hash = \"no_file\"\n",
    "        except:\n",
    "            source_hash = \"error\"\n",
    "\n",
    "        essential_config = {\n",
    "            'max_source_length': config_params.get('max_source_length', 512),\n",
    "            'max_target_length': config_params.get('max_target_length', 256),\n",
    "            'split': config_params.get('split', 'train'),\n",
    "            'max_examples': config_params.get('max_examples', None)\n",
    "        }\n",
    "\n",
    "        config_str = json.dumps(essential_config, sort_keys=True)\n",
    "        combined = f\"{source_hash}_{config_str}_{self.version}\"\n",
    "\n",
    "        return hashlib.md5(combined.encode()).hexdigest()[:16]\n",
    "\n",
    "    def save_preprocessed_data(self, data: Any, cache_key: str, data_type: str) -> bool:\n",
    "        \"\"\"Save preprocessed data with integrity checks\"\"\"\n",
    "        try:\n",
    "            file_path = self.get_path('preprocessed') / f\"{data_type}_{cache_key}.pkl\"\n",
    "            file_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            # Save data\n",
    "            with open(file_path, 'wb') as f:\n",
    "                pickle.dump(data, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "            # Save metadata\n",
    "            metadata = {\n",
    "                'cache_key': cache_key,\n",
    "                'data_type': data_type,\n",
    "                'timestamp': datetime.now().isoformat(),\n",
    "                'version': self.version,\n",
    "                'file_size': os.path.getsize(file_path),\n",
    "                'data_info': self._get_data_info(data),\n",
    "                'absolute_path': str(file_path.absolute())\n",
    "            }\n",
    "\n",
    "            metadata_path = self.get_path('metadata') / f\"{data_type}_{cache_key}_meta.json\"\n",
    "            metadata_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            with open(metadata_path, 'w') as f:\n",
    "                json.dump(metadata, f, indent=2)\n",
    "\n",
    "            print(f\"âœ“ Saved {data_type} data: {file_path.name} ({os.path.getsize(file_path)/1024/1024:.1f}MB)\")\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âœ— Failed to save {data_type} data: {e}\")\n",
    "            return False\n",
    "\n",
    "    def load_preprocessed_data(self, cache_key: str, data_type: str) -> Optional[Any]:\n",
    "        \"\"\"FIXED: Load preprocessed data with flexible pattern matching\"\"\"\n",
    "        try:\n",
    "            # First try exact match\n",
    "            file_path = self.get_path('preprocessed') / f\"{data_type}_{cache_key}.pkl\"\n",
    "            print(f\"ðŸ” Looking for exact match: {file_path.name}\")\n",
    "\n",
    "            if file_path.exists():\n",
    "                return self._load_cache_file(file_path, data_type)\n",
    "\n",
    "            # FIXED: More flexible pattern matching for datasets\n",
    "            print(f\"ðŸ” Searching for pattern: {data_type}_*.pkl\")\n",
    "\n",
    "            preprocessed_dir = self.get_path('preprocessed')\n",
    "            if not preprocessed_dir.exists():\n",
    "                print(f\"   Preprocessed directory doesn't exist\")\n",
    "                return None\n",
    "\n",
    "            # Look for files that start with the data_type prefix\n",
    "            pattern = f\"{data_type}_*.pkl\"\n",
    "            cache_files = list(preprocessed_dir.glob(pattern))\n",
    "\n",
    "            print(f\"   Found {len(cache_files)} potential cache files\")\n",
    "\n",
    "            if not cache_files:\n",
    "                return None\n",
    "\n",
    "            # FIXED: Better file selection - prioritize by modification time and compatibility\n",
    "            compatible_files = []\n",
    "\n",
    "            for cache_file in cache_files:\n",
    "                print(f\"   Examining: {cache_file.name}\")\n",
    "                # Try to extract info from filename\n",
    "                file_parts = cache_file.stem.split('_')\n",
    "                if len(file_parts) >= 2:\n",
    "                    file_data_type = file_parts[0]\n",
    "                    if file_data_type == data_type.split('_')[0]:  # Match base type (e.g., 'dataset' from 'dataset_train')\n",
    "                        compatible_files.append(cache_file)\n",
    "                        print(f\"     âœ“ Compatible file found\")\n",
    "\n",
    "            if not compatible_files:\n",
    "                # If no compatible files by name, try all files with the pattern\n",
    "                compatible_files = cache_files\n",
    "                print(f\"   No name-compatible files, trying all {len(compatible_files)} files\")\n",
    "\n",
    "            # Sort by modification time (newest first)\n",
    "            compatible_files.sort(key=lambda f: f.stat().st_mtime, reverse=True)\n",
    "\n",
    "            # Try loading each file until one works\n",
    "            for cache_file in compatible_files:\n",
    "                print(f\"   Attempting to load: {cache_file.name}\")\n",
    "                data = self._load_cache_file(cache_file, data_type)\n",
    "                if data is not None:\n",
    "                    print(f\"âœ… Successfully loaded: {cache_file.name}\")\n",
    "                    return data\n",
    "\n",
    "            print(f\"   No compatible cache files could be loaded\")\n",
    "            return None\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"   Cache loading error: {e}\")\n",
    "            return None\n",
    "    def _find_and_load_best_match(self, data_type: str, cache_key: str) -> Optional[Any]:\n",
    "        \"\"\"FIXED: Find and load best matching cache file\"\"\"\n",
    "        try:\n",
    "            preprocessed_dir = self.get_path('preprocessed')\n",
    "            pattern = f\"{data_type}_*.pkl\"\n",
    "\n",
    "            cache_files = list(preprocessed_dir.glob(pattern))\n",
    "            print(f\"   Found {len(cache_files)} potential cache files\")\n",
    "\n",
    "            if not cache_files:\n",
    "                return None\n",
    "\n",
    "            # Sort by modification time (newest first)\n",
    "            cache_files.sort(key=lambda f: f.stat().st_mtime, reverse=True)\n",
    "\n",
    "            # Try loading each file until one works\n",
    "            for cache_file in cache_files:\n",
    "                print(f\"   Trying: {cache_file.name}\")\n",
    "                data = self._load_cache_file(cache_file, data_type)\n",
    "                if data is not None:\n",
    "                    print(f\"âœ“ Successfully loaded: {cache_file.name}\")\n",
    "                    return data\n",
    "\n",
    "            print(f\"   No compatible cache files found\")\n",
    "            return None\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"   Pattern matching failed: {e}\")\n",
    "            return None\n",
    "\n",
    "    def _load_cache_file(self, file_path: Path, data_type: str) -> Optional[Any]:\n",
    "        \"\"\"Load a specific cache file\"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            print(f\"   Failed to load {file_path.name}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def _get_data_info(self, data: Any) -> Dict:\n",
    "        \"\"\"Get information about data for metadata\"\"\"\n",
    "        if hasattr(data, '__len__'):\n",
    "            return {'length': len(data), 'type': type(data).__name__}\n",
    "        elif isinstance(data, dict):\n",
    "            return {'keys': list(data.keys()), 'type': 'dict'}\n",
    "        else:\n",
    "            return {'type': type(data).__name__}\n",
    "\n",
    "    def save_model_checkpoint(self, model, tokenizer, optimizer, scheduler,\n",
    "                            epoch: int, metrics: Dict, is_best: bool = False) -> bool:\n",
    "        \"\"\"Save model checkpoint with comprehensive metadata\"\"\"\n",
    "        try:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "            if is_best:\n",
    "                checkpoint_dir = self.get_path('checkpoints') / 'best_model'\n",
    "            else:\n",
    "                checkpoint_dir = self.get_path('checkpoints') / f'epoch_{epoch+1}_{timestamp}'\n",
    "\n",
    "            checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            # Save model and tokenizer\n",
    "            model.save_pretrained(checkpoint_dir)\n",
    "            tokenizer.save_pretrained(checkpoint_dir)\n",
    "\n",
    "            # Save training state\n",
    "            training_state = {\n",
    "                'epoch': epoch,\n",
    "                'optimizer_state': optimizer.state_dict(),\n",
    "                'scheduler_state': scheduler.state_dict() if scheduler else None,\n",
    "                'metrics': metrics,\n",
    "                'timestamp': timestamp,\n",
    "                'global_step': getattr(self, 'global_step', 0)\n",
    "            }\n",
    "\n",
    "            torch.save(training_state, checkpoint_dir / 'training_state.pt')\n",
    "\n",
    "            # Save checkpoint metadata\n",
    "            metadata = {\n",
    "                'checkpoint_name': checkpoint_dir.name,\n",
    "                'epoch': epoch + 1,\n",
    "                'timestamp': timestamp,\n",
    "                'is_best': is_best,\n",
    "                'metrics': metrics,\n",
    "                'model_config': model.config.to_dict() if hasattr(model.config, 'to_dict') else {},\n",
    "                'directory_size_mb': self._get_directory_size_mb(checkpoint_dir),\n",
    "                'absolute_path': str(checkpoint_dir.absolute())\n",
    "            }\n",
    "\n",
    "            with open(checkpoint_dir / 'checkpoint_metadata.json', 'w') as f:\n",
    "                json.dump(metadata, f, indent=2)\n",
    "\n",
    "            print(f\"âœ“ Checkpoint saved: {checkpoint_dir.name}\")\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âœ— Failed to save checkpoint: {e}\")\n",
    "            return False\n",
    "\n",
    "    def _get_directory_size_mb(self, directory: Path) -> float:\n",
    "        \"\"\"Get directory size in MB\"\"\"\n",
    "        try:\n",
    "            total_size = sum(f.stat().st_size for f in directory.rglob('*') if f.is_file())\n",
    "            return total_size / (1024 * 1024)\n",
    "        except:\n",
    "            return 0.0\n",
    "\n",
    "\n",
    "    def list_checkpoints(self) -> List[Dict]:\n",
    "        \"\"\"FIXED: Better checkpoint listing focusing on epoch folders\"\"\"\n",
    "        checkpoints = []\n",
    "        checkpoint_dir = self.get_path('checkpoints')\n",
    "\n",
    "        print(f\"ðŸ” Scanning for checkpoints in: {checkpoint_dir}\")\n",
    "\n",
    "        if not checkpoint_dir.exists():\n",
    "            print(\"   Checkpoint directory doesn't exist\")\n",
    "            return checkpoints\n",
    "\n",
    "        # FIXED: Separate different types of checkpoint folders\n",
    "        epoch_dirs = []\n",
    "        best_dirs = []\n",
    "        other_dirs = []\n",
    "\n",
    "        for subdir in checkpoint_dir.iterdir():\n",
    "            if subdir.is_dir():\n",
    "                dir_name = subdir.name.lower()\n",
    "                if dir_name.startswith('epoch_'):\n",
    "                    epoch_dirs.append(subdir)\n",
    "                elif 'best' in dir_name:\n",
    "                    best_dirs.append(subdir)\n",
    "                else:\n",
    "                    other_dirs.append(subdir)\n",
    "\n",
    "        print(f\"   Found: {len(epoch_dirs)} epoch dirs, {len(best_dirs)} best dirs, {len(other_dirs)} other dirs\")\n",
    "\n",
    "        # Process in priority order: epoch folders first, then best, then others\n",
    "        all_dirs = epoch_dirs + best_dirs + other_dirs\n",
    "\n",
    "        for subdir in all_dirs:\n",
    "            print(f\"   Examining directory: {subdir.name}\")\n",
    "            checkpoint_info = self._analyze_checkpoint_directory(subdir)\n",
    "            if checkpoint_info:\n",
    "                checkpoints.append(checkpoint_info)\n",
    "                print(f\"     âœ“ Valid checkpoint: Epoch {checkpoint_info.get('epoch', 'N/A')}\")\n",
    "\n",
    "        print(f\"   Total valid checkpoints: {len(checkpoints)}\")\n",
    "        return sorted(checkpoints, key=lambda x: x.get('epoch', 0))\n",
    "    def _infer_checkpoint_metadata(self, checkpoint_dir: Path) -> Optional[Dict]:\n",
    "        \"\"\"FIXED: Infer checkpoint metadata and check for required files\"\"\"\n",
    "        try:\n",
    "            # FIXED: Check for either pytorch_model.bin OR model.safetensors\n",
    "            has_model = (\n",
    "                (checkpoint_dir / 'pytorch_model.bin').exists() or\n",
    "                (checkpoint_dir / 'model.safetensors').exists()\n",
    "            )\n",
    "\n",
    "            has_config = (checkpoint_dir / 'config.json').exists()\n",
    "            has_tokenizer = (checkpoint_dir / 'tokenizer.json').exists() or (checkpoint_dir / 'spiece.model').exists()\n",
    "\n",
    "            if not (has_model and has_config):\n",
    "                print(f\"     Missing required files in {checkpoint_dir.name}\")\n",
    "                return None\n",
    "\n",
    "            # Try to extract epoch from directory name\n",
    "            dir_name = checkpoint_dir.name\n",
    "            epoch = 0\n",
    "\n",
    "            # Look for epoch pattern\n",
    "            epoch_match = re.search(r'epoch_(\\d+)', dir_name)\n",
    "            if epoch_match:\n",
    "                epoch = int(epoch_match.group(1))\n",
    "\n",
    "            # Check for training state\n",
    "            training_state_exists = (checkpoint_dir / 'training_state.pt').exists()\n",
    "\n",
    "            return {\n",
    "                'checkpoint_name': dir_name,\n",
    "                'epoch': epoch,\n",
    "                'timestamp': datetime.now().strftime(\"%Y%m%d_%H%M%S\"),\n",
    "                'is_best': 'best' in dir_name.lower(),\n",
    "                'metrics': {'loss': 0.0},  # Placeholder\n",
    "                'path': str(checkpoint_dir),\n",
    "                'inferred': True,\n",
    "                'has_training_state': training_state_exists,\n",
    "                'has_safetensors': (checkpoint_dir / 'model.safetensors').exists()\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"     Failed to infer metadata: {e}\")\n",
    "            return None\n",
    "\n",
    "    def cleanup_old_checkpoints(self, keep_last_n: int = 5):\n",
    "        \"\"\"Clean up old checkpoints, keeping only the last N and best model\"\"\"\n",
    "        checkpoints = self.list_checkpoints()\n",
    "\n",
    "        # Separate best model from regular checkpoints\n",
    "        regular_checkpoints = [cp for cp in checkpoints if not cp.get('is_best', False)]\n",
    "\n",
    "        if len(regular_checkpoints) > keep_last_n:\n",
    "            to_remove = regular_checkpoints[:-keep_last_n]\n",
    "\n",
    "            for checkpoint in to_remove:\n",
    "                try:\n",
    "                    shutil.rmtree(checkpoint['path'])\n",
    "                    print(f\"âœ“ Removed old checkpoint: {Path(checkpoint['path']).name}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"âœ— Failed to remove checkpoint {checkpoint['path']}: {e}\")\n",
    "    def _analyze_checkpoint_directory(self, checkpoint_dir: Path) -> Optional[Dict]:\n",
    "        \"\"\"FIXED: Analyze checkpoint directory and validate required files\"\"\"\n",
    "        try:\n",
    "            # Check for required files with flexible model format support\n",
    "            has_safetensors = (checkpoint_dir / 'model.safetensors').exists()\n",
    "            has_pytorch_model = (checkpoint_dir / 'pytorch_model.bin').exists()\n",
    "            has_config = (checkpoint_dir / 'config.json').exists()\n",
    "            has_tokenizer = (\n",
    "                (checkpoint_dir / 'tokenizer.json').exists() or\n",
    "                (checkpoint_dir / 'spiece.model').exists() or\n",
    "                (checkpoint_dir / 'tokenizer_config.json').exists()\n",
    "            )\n",
    "            has_training_state = (checkpoint_dir / 'training_state.pt').exists()\n",
    "\n",
    "            print(f\"     File check for {checkpoint_dir.name}:\")\n",
    "            print(f\"       model.safetensors: {'âœ“' if has_safetensors else 'âœ—'}\")\n",
    "            print(f\"       pytorch_model.bin: {'âœ“' if has_pytorch_model else 'âœ—'}\")\n",
    "            print(f\"       config.json: {'âœ“' if has_config else 'âœ—'}\")\n",
    "            print(f\"       tokenizer files: {'âœ“' if has_tokenizer else 'âœ—'}\")\n",
    "            print(f\"       training_state.pt: {'âœ“' if has_training_state else 'âœ—'}\")\n",
    "\n",
    "            # FIXED: Accept either safetensors OR pytorch_model.bin\n",
    "            has_model = has_safetensors or has_pytorch_model\n",
    "\n",
    "            if not (has_model and has_config):\n",
    "                print(f\"     âŒ Missing essential files (model or config)\")\n",
    "                return None\n",
    "\n",
    "            # Extract epoch information from directory name or metadata\n",
    "            dir_name = checkpoint_dir.name\n",
    "            epoch = 0\n",
    "\n",
    "            # Try to get epoch from metadata first\n",
    "            metadata_file = checkpoint_dir / 'checkpoint_metadata.json'\n",
    "            if metadata_file.exists():\n",
    "                try:\n",
    "                    with open(metadata_file, 'r') as f:\n",
    "                        metadata = json.load(f)\n",
    "                    epoch = metadata.get('epoch', 0)\n",
    "                    print(f\"     ðŸ“‹ Metadata epoch: {epoch}\")\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            # Fallback to extracting from directory name\n",
    "            if epoch == 0:\n",
    "                epoch_match = re.search(r'epoch[_\\-]?(\\d+)', dir_name, re.I)\n",
    "                if epoch_match:\n",
    "                    epoch = int(epoch_match.group(1))\n",
    "                    print(f\"     ðŸ“ Directory name epoch: {epoch}\")\n",
    "\n",
    "            # Try to get loss from training_state\n",
    "            loss = 0.0\n",
    "            if has_training_state:\n",
    "                try:\n",
    "                    training_state = torch.load(checkpoint_dir / 'training_state.pt', map_location='cpu')\n",
    "                    loss = training_state.get('metrics', {}).get('loss', 0.0)\n",
    "                    print(f\"     ðŸ“Š Training state loss: {loss}\")\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            return {\n",
    "                'checkpoint_name': dir_name,\n",
    "                'epoch': epoch,\n",
    "                'timestamp': datetime.now().strftime(\"%Y%m%d_%H%M%S\"),\n",
    "                'is_best': 'best' in dir_name.lower(),\n",
    "                'metrics': {'loss': loss},\n",
    "                'path': str(checkpoint_dir),\n",
    "                'has_safetensors': has_safetensors,\n",
    "                'has_pytorch_model': has_pytorch_model,\n",
    "                'has_training_state': has_training_state,\n",
    "                'has_tokenizer': has_tokenizer,\n",
    "                'model_format': 'safetensors' if has_safetensors else 'pytorch_bin'\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"     âŒ Error analyzing checkpoint: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# ENHANCED COMPLEXITY ANALYZER\n",
    "# =============================================================================\n",
    "\n",
    "class EnhancedComplexityAnalyzer:\n",
    "    \"\"\"Improved complexity analyzer with efficient caching\"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer: T5Tokenizer, data_manager: RobustDataManager):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data_manager = data_manager\n",
    "\n",
    "        # Compile regex patterns for efficiency\n",
    "        self.patterns = {\n",
    "            'words': re.compile(r'\\b\\w+\\b'),\n",
    "            'sentences': re.compile(r'[.!?]+'),\n",
    "            'subordinate_clauses': re.compile(r'\\b(?:although|because|since|while|if|when|after|before|until|unless|whereas)\\b', re.I),\n",
    "            'relative_clauses': re.compile(r'\\b(?:which|that|who|whom|whose|where)\\s+\\w+', re.I),\n",
    "            'passive_voice': re.compile(r'\\b(?:was|were|is|are|been)\\s+\\w+ed\\b', re.I),\n",
    "            'complex_tenses': re.compile(r'\\b(?:have|has|had|will|would|could|should|might|must)\\s+(?:been|have)\\b', re.I),\n",
    "            'discourse_markers': re.compile(r'\\b(?:however|therefore|furthermore|moreover|nevertheless|consequently)\\b', re.I)\n",
    "        }\n",
    "\n",
    "        # Load frequency data\n",
    "        self.word_frequencies = self._load_or_create_frequency_data()\n",
    "\n",
    "        # Morphological complexity indicators\n",
    "        self.complex_suffixes = {\n",
    "            'derivational': ['tion', 'sion', 'ment', 'ness', 'ity', 'ism', 'ance', 'ence'],\n",
    "            'inflectional': ['ing', 'ed', 'er', 'est', 's', 'es'],\n",
    "            'academic': ['ological', 'istically', 'ification']\n",
    "        }\n",
    "\n",
    "        self.complex_prefixes = {'un', 're', 'pre', 'dis', 'in', 'im', 'non', 'over', 'under', 'mis', 'anti', 'inter', 'multi'}\n",
    "\n",
    "        # Cache for complexity scores\n",
    "        self._cache = {}\n",
    "        self._cache_stats = {'hits': 0, 'misses': 0}\n",
    "\n",
    "        print(\"Enhanced Complexity Analyzer initialized\")\n",
    "\n",
    "    def _load_or_create_frequency_data(self) -> Dict[str, int]:\n",
    "        \"\"\"Load or create word frequency data\"\"\"\n",
    "        cache_key = \"word_frequencies\"\n",
    "\n",
    "        frequencies = self.data_manager.load_preprocessed_data(cache_key, 'complexity')\n",
    "\n",
    "        if frequencies is None:\n",
    "            # Create basic frequency data\n",
    "            high_freq_words = [\n",
    "                'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by',\n",
    "                'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did',\n",
    "                'will', 'would', 'can', 'could', 'should', 'may', 'might', 'I', 'you', 'he', 'she', 'it', 'we', 'they',\n",
    "                'this', 'that', 'these', 'those', 'time', 'person', 'year', 'way', 'day', 'thing', 'man', 'world',\n",
    "                'life', 'hand', 'part', 'child', 'eye', 'woman', 'place', 'work', 'week', 'case', 'point'\n",
    "            ]\n",
    "\n",
    "            frequencies = {word.lower(): len(high_freq_words) - i for i, word in enumerate(high_freq_words)}\n",
    "\n",
    "            # Save for future use\n",
    "            self.data_manager.save_preprocessed_data(frequencies, cache_key, 'complexity')\n",
    "\n",
    "        return frequencies\n",
    "\n",
    "    def analyze_text_complexity_batch(self, texts: List[str]) -> List[Dict[str, float]]:\n",
    "        \"\"\"Analyze complexity for multiple texts with caching\"\"\"\n",
    "        results = []\n",
    "\n",
    "        for text in texts:\n",
    "            # Generate cache key from text hash\n",
    "            text_hash = hashlib.md5(text[:500].encode()).hexdigest()[:12]\n",
    "\n",
    "            if text_hash in self._cache:\n",
    "                self._cache_stats['hits'] += 1\n",
    "                results.append(self._cache[text_hash])\n",
    "            else:\n",
    "                self._cache_stats['misses'] += 1\n",
    "                result = self._analyze_single_text(text)\n",
    "                self._cache[text_hash] = result\n",
    "                results.append(result)\n",
    "\n",
    "        return results\n",
    "\n",
    "    def _analyze_single_text(self, text: str) -> Dict[str, float]:\n",
    "        \"\"\"Analyze complexity of single text with practical metrics\"\"\"\n",
    "        if not text or len(text.strip()) < 10:\n",
    "            return self._get_default_scores()\n",
    "\n",
    "        # Extract basic features\n",
    "        words = self.patterns['words'].findall(text.lower())\n",
    "        sentences = [s.strip() for s in self.patterns['sentences'].split(text) if s.strip()]\n",
    "\n",
    "        if not words or not sentences:\n",
    "            return self._get_default_scores()\n",
    "\n",
    "        # Calculate complexity dimensions\n",
    "        morphological = self._calculate_morphological_complexity(words)\n",
    "        syntactic = self._calculate_syntactic_complexity(text, words, sentences)\n",
    "        semantic = self._calculate_semantic_complexity(words)\n",
    "        lexical = self._calculate_lexical_complexity(words)\n",
    "\n",
    "        # Overall complexity as weighted combination\n",
    "        overall = (\n",
    "            morphological * 0.25 +\n",
    "            syntactic * 0.35 +\n",
    "            semantic * 0.25 +\n",
    "            lexical * 0.15\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'overall_complexity': self._normalize_score(overall),\n",
    "            'morphological_complexity': self._normalize_score(morphological),\n",
    "            'syntactic_complexity': self._normalize_score(syntactic),\n",
    "            'semantic_complexity': self._normalize_score(semantic),\n",
    "            'lexical_complexity': self._normalize_score(lexical),\n",
    "            'sentence_length_complexity': self._normalize_score(len(words) / len(sentences) / 15),\n",
    "            'vocabulary_diversity': self._normalize_score(len(set(words)) / len(words))\n",
    "        }\n",
    "\n",
    "    def _calculate_morphological_complexity(self, words: List[str]) -> float:\n",
    "        \"\"\"Calculate morphological complexity based on word structure\"\"\"\n",
    "        if not words:\n",
    "            return 0.1\n",
    "\n",
    "        complexity_score = 0.0\n",
    "\n",
    "        for word in words:\n",
    "            word_score = 0.0\n",
    "\n",
    "            # Length-based complexity\n",
    "            if len(word) > 8:\n",
    "                word_score += 0.3\n",
    "            elif len(word) > 6:\n",
    "                word_score += 0.15\n",
    "\n",
    "            # Suffix complexity\n",
    "            for suffix_type, suffixes in self.complex_suffixes.items():\n",
    "                for suffix in suffixes:\n",
    "                    if word.endswith(suffix) and len(word) > len(suffix) + 2:\n",
    "                        if suffix_type == 'derivational':\n",
    "                            word_score += 0.4\n",
    "                        elif suffix_type == 'academic':\n",
    "                            word_score += 0.6\n",
    "                        else:\n",
    "                            word_score += 0.2\n",
    "                        break\n",
    "\n",
    "            # Prefix complexity\n",
    "            for prefix in self.complex_prefixes:\n",
    "                if word.startswith(prefix) and len(word) > len(prefix) + 2:\n",
    "                    word_score += 0.2\n",
    "                    break\n",
    "\n",
    "            complexity_score += min(word_score, 1.0)\n",
    "\n",
    "        return complexity_score / len(words)\n",
    "\n",
    "    def _calculate_syntactic_complexity(self, text: str, words: List[str], sentences: List[str]) -> float:\n",
    "        \"\"\"Calculate syntactic complexity based on sentence structure\"\"\"\n",
    "        if not words:\n",
    "            return 0.1\n",
    "\n",
    "        complexity_score = 0.0\n",
    "\n",
    "        # Clause complexity\n",
    "        for pattern_name, pattern in self.patterns.items():\n",
    "            if pattern_name not in ['words', 'sentences']:\n",
    "                matches = len(pattern.findall(text))\n",
    "                complexity_score += matches / len(words) * 2\n",
    "\n",
    "        # Sentence length variation\n",
    "        if len(sentences) > 1:\n",
    "            sent_lengths = [len(s.split()) for s in sentences]\n",
    "            avg_length = np.mean(sent_lengths)\n",
    "            length_variation = np.std(sent_lengths) / (avg_length + 1)\n",
    "            complexity_score += length_variation * 0.5\n",
    "\n",
    "        # Average sentence length complexity\n",
    "        avg_sent_length = len(words) / len(sentences)\n",
    "        length_complexity = min((avg_sent_length - 10) / 20, 1.0)\n",
    "        complexity_score += max(length_complexity, 0) * 0.3\n",
    "\n",
    "        return complexity_score\n",
    "\n",
    "    def _calculate_semantic_complexity(self, words: List[str]) -> float:\n",
    "        \"\"\"Calculate semantic complexity based on vocabulary characteristics\"\"\"\n",
    "        if not words:\n",
    "            return 0.1\n",
    "\n",
    "        # Lexical diversity\n",
    "        unique_words = set(words)\n",
    "        diversity = len(unique_words) / len(words)\n",
    "\n",
    "        # Rare word density\n",
    "        rare_words = sum(1 for word in words if word not in self.word_frequencies)\n",
    "        rare_density = rare_words / len(words)\n",
    "\n",
    "        # Abstract concepts (simplified heuristic)\n",
    "        abstract_indicators = {'concept', 'idea', 'theory', 'principle', 'notion', 'aspect', 'factor', 'element'}\n",
    "        abstract_count = sum(1 for word in words if word in abstract_indicators)\n",
    "        abstract_density = abstract_count / len(words)\n",
    "\n",
    "        semantic_complexity = (\n",
    "            diversity * 0.4 +\n",
    "            min(rare_density * 3, 1.0) * 0.4 +\n",
    "            min(abstract_density * 5, 1.0) * 0.2\n",
    "        )\n",
    "\n",
    "        return semantic_complexity\n",
    "\n",
    "    def _calculate_lexical_complexity(self, words: List[str]) -> float:\n",
    "        \"\"\"Calculate lexical complexity based on word frequency and sophistication\"\"\"\n",
    "        if not words:\n",
    "            return 0.1\n",
    "\n",
    "        sophistication_score = 0.0\n",
    "\n",
    "        for word in words:\n",
    "            word_score = 0.0\n",
    "\n",
    "            # Frequency-based scoring\n",
    "            if word in self.word_frequencies:\n",
    "                freq_rank = self.word_frequencies[word]\n",
    "                if freq_rank > 1000:  # Low frequency = high complexity\n",
    "                    word_score += 0.5\n",
    "                elif freq_rank > 500:\n",
    "                    word_score += 0.3\n",
    "            else:\n",
    "                word_score += 0.7  # Unknown words are complex\n",
    "\n",
    "            # Length-based sophistication\n",
    "            if len(word) > 10:\n",
    "                word_score += 0.3\n",
    "            elif len(word) > 7:\n",
    "                word_score += 0.15\n",
    "\n",
    "            sophistication_score += min(word_score, 1.0)\n",
    "\n",
    "        return sophistication_score / len(words)\n",
    "\n",
    "    def _normalize_score(self, score: float) -> float:\n",
    "        \"\"\"Normalize score to [0.01, 1.0] range\"\"\"\n",
    "        return max(0.01, min(score, 1.0))\n",
    "\n",
    "    def _get_default_scores(self) -> Dict[str, float]:\n",
    "        \"\"\"Return default complexity scores for invalid texts\"\"\"\n",
    "        return {\n",
    "            'overall_complexity': 0.1,\n",
    "            'morphological_complexity': 0.1,\n",
    "            'syntactic_complexity': 0.1,\n",
    "            'semantic_complexity': 0.1,\n",
    "            'lexical_complexity': 0.1,\n",
    "            'sentence_length_complexity': 0.1,\n",
    "            'vocabulary_diversity': 0.1\n",
    "        }\n",
    "\n",
    "    def get_cache_stats(self) -> Dict:\n",
    "        \"\"\"Get cache performance statistics\"\"\"\n",
    "        total = self._cache_stats['hits'] + self._cache_stats['misses']\n",
    "        hit_rate = (self._cache_stats['hits'] / total * 100) if total > 0 else 0\n",
    "        return {\n",
    "            'cache_hits': self._cache_stats['hits'],\n",
    "            'cache_misses': self._cache_stats['misses'],\n",
    "            'hit_rate_percent': hit_rate,\n",
    "            'cache_size': len(self._cache)\n",
    "        }\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# IMPROVED TASK CREATOR\n",
    "# =============================================================================\n",
    "\n",
    "class ImprovedTaskCreator:\n",
    "    \"\"\"Enhanced task creator with diverse and practical tasks\"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer: T5Tokenizer, max_source_length: int = 512, max_target_length: int = 256):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_source_length = max_source_length\n",
    "        self.max_target_length = max_target_length\n",
    "\n",
    "        # Precompiled patterns\n",
    "        self.sentence_pattern = re.compile(r'(?<=[.!?])\\s+')\n",
    "        self.word_pattern = re.compile(r'\\b\\w+\\b')\n",
    "\n",
    "        # Task templates for better variety\n",
    "        self.task_templates = {\n",
    "            'morphological': [\n",
    "                ('inflect', 'inflect {word} with {suffix}:'),\n",
    "                ('derive', 'derive word from {word} meaning {meaning}:'),\n",
    "                ('analyze', 'analyze morphemes in {word}:')\n",
    "            ],\n",
    "            'syntactic': [\n",
    "                ('complete', 'complete sentence: {prefix}'),\n",
    "                ('transform', 'transform to {structure}: {sentence}'),\n",
    "                ('parse', 'identify main clause in: {sentence}')\n",
    "            ],\n",
    "            'semantic': [\n",
    "                ('continue', 'continue logically: {context}'),\n",
    "                ('summarize', 'summarize in one sentence: {text}'),\n",
    "                ('infer', 'what can be inferred from: {statement}')\n",
    "            ]\n",
    "        }\n",
    "\n",
    "        print(\"Improved Task Creator initialized\")\n",
    "\n",
    "    def create_domain_specific_tasks(self, text: str, complexity_analysis: Dict[str, float]) -> List[Dict]:\n",
    "        \"\"\"Create diverse tasks for all linguistic domains\"\"\"\n",
    "        if len(text) < 100:\n",
    "            return []\n",
    "\n",
    "        tasks = []\n",
    "        sentences = self._extract_sentences(text)\n",
    "\n",
    "        if len(sentences) < 1:\n",
    "            return []\n",
    "\n",
    "        # Determine primary domain based on complexity\n",
    "        primary_domain = self._determine_primary_domain(complexity_analysis)\n",
    "\n",
    "        # Always create span corruption (T5's core task)\n",
    "        span_task = self._create_span_corruption_task(text)\n",
    "        if span_task:\n",
    "            span_task.update({\n",
    "                'domain': 'general',\n",
    "                'primary_domain': primary_domain,\n",
    "                'complexity_score': complexity_analysis['overall_complexity']\n",
    "            })\n",
    "            tasks.append(span_task)\n",
    "\n",
    "        # Create domain-specific tasks\n",
    "        for domain in ['morphological', 'syntactic', 'semantic']:\n",
    "            task = self._create_domain_task(domain, text, sentences, complexity_analysis)\n",
    "            if task:\n",
    "                task.update({\n",
    "                    'domain': domain,\n",
    "                    'primary_domain': primary_domain,\n",
    "                    'complexity_score': complexity_analysis[f'{domain}_complexity']\n",
    "                })\n",
    "                tasks.append(task)\n",
    "\n",
    "        return tasks\n",
    "\n",
    "    def _extract_sentences(self, text: str) -> List[str]:\n",
    "        \"\"\"Extract well-formed sentences from text\"\"\"\n",
    "        sentences = [s.strip() for s in self.sentence_pattern.split(text) if s.strip()]\n",
    "        # Filter sentences with reasonable length\n",
    "        return [s for s in sentences if 5 <= len(s.split()) <= 40]\n",
    "\n",
    "    def _determine_primary_domain(self, complexity_analysis: Dict[str, float]) -> str:\n",
    "        \"\"\"Determine primary linguistic domain from complexity analysis\"\"\"\n",
    "        domain_scores = {\n",
    "            'morphological': complexity_analysis.get('morphological_complexity', 0.1),\n",
    "            'syntactic': complexity_analysis.get('syntactic_complexity', 0.1),\n",
    "            'semantic': complexity_analysis.get('semantic_complexity', 0.1)\n",
    "        }\n",
    "        return max(domain_scores, key=domain_scores.get)\n",
    "\n",
    "    def _create_span_corruption_task(self, text: str, corruption_rate: float = 0.15) -> Optional[Dict]:\n",
    "        \"\"\"Create T5-style span corruption task\"\"\"\n",
    "        try:\n",
    "            words = text.split()\n",
    "            if len(words) < 10:\n",
    "                return None\n",
    "\n",
    "            # Calculate number of spans to corrupt\n",
    "            num_spans = max(1, min(int(len(words) * corruption_rate), len(words) // 4))\n",
    "\n",
    "            # Select random spans\n",
    "            span_starts = sorted(random.sample(range(len(words) - 1), num_spans))\n",
    "\n",
    "            corrupted_words = words.copy()\n",
    "            target_spans = []\n",
    "\n",
    "            for i, start in enumerate(span_starts):\n",
    "                sentinel = f'<extra_id_{i}>'\n",
    "                span_length = random.randint(1, 3)\n",
    "                end = min(start + span_length, len(words))\n",
    "\n",
    "                original_span = ' '.join(words[start:end])\n",
    "                target_spans.append(f'{sentinel} {original_span}')\n",
    "\n",
    "                # Replace span with sentinel\n",
    "                for j in range(start, end):\n",
    "                    if j < len(corrupted_words):\n",
    "                        if j == start:\n",
    "                            corrupted_words[j] = sentinel\n",
    "                        else:\n",
    "                            corrupted_words[j] = ''\n",
    "\n",
    "            source_text = ' '.join(w for w in corrupted_words if w)\n",
    "            target_text = ' '.join(target_spans) + f' <extra_id_{num_spans}>'\n",
    "\n",
    "            return self._tokenize_example(source_text, target_text, 'span_corruption')\n",
    "\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "    def _create_domain_task(self, domain: str, text: str, sentences: List[str], complexity_analysis: Dict) -> Optional[Dict]:\n",
    "        \"\"\"Create task specific to linguistic domain\"\"\"\n",
    "        if domain == 'morphological':\n",
    "            return self._create_morphological_task(text, sentences)\n",
    "        elif domain == 'syntactic':\n",
    "            return self._create_syntactic_task(text, sentences)\n",
    "        elif domain == 'semantic':\n",
    "            return self._create_semantic_task(text, sentences)\n",
    "        return None\n",
    "\n",
    "    def _create_morphological_task(self, text: str, sentences: List[str]) -> Optional[Dict]:\n",
    "        \"\"\"Create morphological analysis task\"\"\"\n",
    "        words = self.word_pattern.findall(text.lower())\n",
    "\n",
    "        # Find base words suitable for inflection\n",
    "        base_words = [w for w in words if 3 <= len(w) <= 8 and w.isalpha()\n",
    "                     and not w.endswith(('ing', 'ed', 'er', 'ly', 'est'))]\n",
    "\n",
    "        if not base_words:\n",
    "            return None\n",
    "\n",
    "        base_word = random.choice(base_words)\n",
    "        inflections = ['ing', 'ed', 'er', 's']\n",
    "        suffix = random.choice(inflections)\n",
    "\n",
    "        template = random.choice(self.task_templates['morphological'])\n",
    "        source_text = template[1].format(word=base_word, suffix=suffix, meaning='action')\n",
    "        target_text = base_word + suffix\n",
    "\n",
    "        return self._tokenize_example(source_text, target_text, f'morphological_{template[0]}')\n",
    "\n",
    "    def _create_syntactic_task(self, text: str, sentences: List[str]) -> Optional[Dict]:\n",
    "        \"\"\"Create syntactic analysis task\"\"\"\n",
    "        if not sentences:\n",
    "            return None\n",
    "\n",
    "        sentence = random.choice(sentences)\n",
    "        words = sentence.split()\n",
    "\n",
    "        if len(words) < 6:\n",
    "            return None\n",
    "\n",
    "        # Create sentence completion task\n",
    "        split_point = random.randint(3, len(words) - 2)\n",
    "        prefix = ' '.join(words[:split_point])\n",
    "        completion = ' '.join(words[split_point:])\n",
    "\n",
    "        template = random.choice(self.task_templates['syntactic'])\n",
    "        source_text = template[1].format(prefix=prefix, sentence=sentence, structure='active')\n",
    "\n",
    "        return self._tokenize_example(source_text, completion, f'syntactic_{template[0]}')\n",
    "\n",
    "    def _create_semantic_task(self, text: str, sentences: List[str]) -> Optional[Dict]:\n",
    "        \"\"\"Create semantic understanding task\"\"\"\n",
    "        if len(sentences) < 2:\n",
    "            return None\n",
    "\n",
    "        # Create logical continuation task\n",
    "        context = sentences[0]\n",
    "        continuation = sentences[1] if len(sentences) > 1 else context[:50] + \"...\"\n",
    "\n",
    "        template = random.choice(self.task_templates['semantic'])\n",
    "        source_text = template[1].format(context=context, text=text[:200], statement=context)\n",
    "\n",
    "        return self._tokenize_example(source_text, continuation, f'semantic_{template[0]}')\n",
    "\n",
    "    def _tokenize_example(self, source_text: str, target_text: str, task_type: str) -> Optional[Dict]:\n",
    "        \"\"\"Tokenize example with error handling\"\"\"\n",
    "        try:\n",
    "            source_text = source_text.strip()\n",
    "            target_text = target_text.strip()\n",
    "\n",
    "            if not source_text or not target_text:\n",
    "                return None\n",
    "\n",
    "            # Length checks before tokenization\n",
    "            if len(source_text) > 2000 or len(target_text) > 1000:\n",
    "                return None\n",
    "\n",
    "            source_encoding = self.tokenizer(\n",
    "                source_text,\n",
    "                max_length=self.max_source_length,\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "\n",
    "            target_encoding = self.tokenizer(\n",
    "                target_text,\n",
    "                max_length=self.max_target_length,\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "\n",
    "            return {\n",
    "                'input_ids': source_encoding['input_ids'].squeeze(0),\n",
    "                'attention_mask': source_encoding['attention_mask'].squeeze(0),\n",
    "                'labels': target_encoding['input_ids'].squeeze(0),\n",
    "                'source_text': source_text,\n",
    "                'target_text': target_text,\n",
    "                'task_type': task_type\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Tokenization error for {task_type}: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# IMPROVED DATASET WITH ROBUST CACHING\n",
    "# =============================================================================\n",
    "\n",
    "class ImprovedDomainAwareDataset(Dataset):\n",
    "    \"\"\"Improved dataset with robust caching and better task distribution\"\"\"\n",
    "\n",
    "    def __init__(self, data_path: str, tokenizer: T5Tokenizer,\n",
    "                 complexity_analyzer: EnhancedComplexityAnalyzer,\n",
    "                 task_creator: ImprovedTaskCreator,\n",
    "                 data_manager: RobustDataManager,\n",
    "                 max_source_length: int = 512, max_target_length: int = 256,\n",
    "                 split: str = \"train\", max_examples: int = None):\n",
    "\n",
    "        self.data_path = data_path\n",
    "        self.tokenizer = tokenizer\n",
    "        self.complexity_analyzer = complexity_analyzer\n",
    "        self.task_creator = task_creator\n",
    "        self.data_manager = data_manager\n",
    "        self.max_source_length = max_source_length\n",
    "        self.max_target_length = max_target_length\n",
    "        self.split = split\n",
    "        self.max_examples = max_examples\n",
    "\n",
    "        # Generate cache key based on configuration\n",
    "        config_params = {\n",
    "            'data_path': data_path,\n",
    "            'max_source_length': max_source_length,\n",
    "            'max_target_length': max_target_length,\n",
    "            'split': split,\n",
    "            'max_examples': max_examples\n",
    "        }\n",
    "\n",
    "        self.cache_key = self.data_manager.generate_cache_key(data_path, config_params)\n",
    "\n",
    "        # Initialize data structures\n",
    "        self.examples = []\n",
    "        self.complexity_scores = []\n",
    "        self.domain_levels = {'morphological': {}, 'syntactic': {}, 'semantic': {}}\n",
    "        self.domain_indices = {'morphological': [], 'syntactic': [], 'semantic': [], 'general': []}\n",
    "\n",
    "        # Load or process data\n",
    "        self._load_or_process_data()\n",
    "\n",
    "        print(f\"Dataset initialized: {len(self.examples)} examples\")\n",
    "        self._print_dataset_stats()\n",
    "\n",
    "\n",
    "    def _load_or_process_data(self):\n",
    "        \"\"\"FIXED: Load from cache with better pattern matching\"\"\"\n",
    "        # Try loading from cache with flexible matching\n",
    "        base_cache_key = f'dataset_{self.split}'\n",
    "        cached_data = self.data_manager.load_preprocessed_data(self.cache_key, base_cache_key)\n",
    "\n",
    "        if cached_data is not None:\n",
    "            try:\n",
    "                self.examples = cached_data['examples']\n",
    "                self.complexity_scores = cached_data['complexity_scores']\n",
    "                self.domain_levels = cached_data.get('domain_levels', self.domain_levels)\n",
    "                self.domain_indices = cached_data.get('domain_indices', self.domain_indices)\n",
    "                print(f\"âœ… Loaded {len(self.examples)} examples from cache\")\n",
    "                return\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Cache data corrupted: {e}, reprocessing...\")\n",
    "\n",
    "        # If no cache found, try alternative cache keys\n",
    "        print(f\"ðŸ” No exact cache match, trying alternative patterns...\")\n",
    "\n",
    "        # Try with just the split name\n",
    "        alt_cached_data = self.data_manager.load_preprocessed_data('', f'dataset_{self.split}')\n",
    "        if alt_cached_data is not None:\n",
    "            try:\n",
    "                self.examples = alt_cached_data['examples']\n",
    "                self.complexity_scores = alt_cached_data['complexity_scores']\n",
    "                self.domain_levels = alt_cached_data.get('domain_levels', self.domain_levels)\n",
    "                self.domain_indices = alt_cached_data.get('domain_indices', self.domain_indices)\n",
    "                print(f\"âœ… Loaded {len(self.examples)} examples from alternative cache\")\n",
    "                return\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Alternative cache also corrupted: {e}\")\n",
    "\n",
    "        # Process from scratch if no cache works\n",
    "        print(f\"ðŸ“Š No usable cache found, processing {self.split} data from scratch...\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        success = self._process_raw_data()\n",
    "\n",
    "        if success and len(self.examples) > 0:\n",
    "            self._create_domain_curricula()\n",
    "\n",
    "            # Save to cache with the current cache key\n",
    "            cached_data = {\n",
    "                'examples': self.examples,\n",
    "                'complexity_scores': self.complexity_scores,\n",
    "                'domain_levels': self.domain_levels,\n",
    "                'domain_indices': self.domain_indices\n",
    "            }\n",
    "\n",
    "            self.data_manager.save_preprocessed_data(cached_data, self.cache_key, base_cache_key)\n",
    "\n",
    "            processing_time = time.time() - start_time\n",
    "            print(f\"âœ… Processing completed in {processing_time:.2f} seconds\")\n",
    "        else:\n",
    "            print(\"âŒ Failed to process data or no examples created\")\n",
    "\n",
    "    def _process_raw_data(self) -> bool:\n",
    "        \"\"\"Process raw data with improved error handling\"\"\"\n",
    "        try:\n",
    "            if not os.path.exists(self.data_path):\n",
    "                print(f\"Data file not found: {self.data_path}\")\n",
    "                return False\n",
    "\n",
    "            # Read and split documents\n",
    "            with open(self.data_path, 'r', encoding='utf-8') as f:\n",
    "                text = f.read()\n",
    "\n",
    "            documents = [doc.strip() for doc in re.split(r'\\n\\s*\\n', text)\n",
    "                        if len(doc.strip()) > 150]\n",
    "\n",
    "            if self.max_examples:\n",
    "                max_docs = min(len(documents), self.max_examples // 3)\n",
    "                documents = documents[:max_docs]\n",
    "\n",
    "            print(f\"Processing {len(documents)} documents...\")\n",
    "\n",
    "            # Process documents with progress tracking\n",
    "            for doc_idx, document in enumerate(tqdm(documents, desc='Processing documents')):\n",
    "                try:\n",
    "                    # Analyze complexity\n",
    "                    complexity_analysis = self.complexity_analyzer.analyze_text_complexity_batch([document])[0]\n",
    "\n",
    "                    # Create tasks\n",
    "                    tasks = self.task_creator.create_domain_specific_tasks(document, complexity_analysis)\n",
    "\n",
    "                    # Add valid tasks\n",
    "                    for task in tasks:\n",
    "                        if task and self._validate_task(task):\n",
    "                            task['complexity_analysis'] = complexity_analysis\n",
    "                            task['document_id'] = f\"{self.split}_{doc_idx}\"\n",
    "\n",
    "                            self.examples.append(task)\n",
    "                            self.complexity_scores.append(task['complexity_score'])\n",
    "\n",
    "                            # Update domain indices\n",
    "                            domain = task.get('domain', 'general')\n",
    "                            if domain in self.domain_indices:\n",
    "                                self.domain_indices[domain].append(len(self.examples) - 1)\n",
    "                            else:\n",
    "                                self.domain_indices['general'].append(len(self.examples) - 1)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing document {doc_idx}: {e}\")\n",
    "                    continue\n",
    "\n",
    "            return len(self.examples) > 0\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in raw data processing: {e}\")\n",
    "            return False\n",
    "\n",
    "    def _validate_task(self, task: Dict) -> bool:\n",
    "        \"\"\"Validate that task has required fields\"\"\"\n",
    "        required_fields = ['input_ids', 'attention_mask', 'labels', 'task_type', 'domain']\n",
    "        return all(field in task for field in required_fields)\n",
    "\n",
    "    def _create_domain_curricula(self):\n",
    "        \"\"\"Create curriculum levels for each domain\"\"\"\n",
    "        print(\"Creating domain curricula...\")\n",
    "\n",
    "        for domain in ['morphological', 'syntactic', 'semantic']:\n",
    "            domain_examples = [(i, ex) for i, ex in enumerate(self.examples)\n",
    "                             if ex.get('domain') == domain]\n",
    "\n",
    "            if not domain_examples:\n",
    "                continue\n",
    "\n",
    "            # Sort by complexity\n",
    "            domain_examples.sort(key=lambda x: x[1].get('complexity_score', 0))\n",
    "\n",
    "            # Create curriculum levels\n",
    "            max_level = {'morphological': 2, 'syntactic': 3, 'semantic': 4}[domain]\n",
    "            level_size = max(1, len(domain_examples) // (max_level + 1))\n",
    "\n",
    "            for level in range(max_level + 1):\n",
    "                start_idx = level * level_size\n",
    "                end_idx = min((level + 1) * level_size, len(domain_examples))\n",
    "\n",
    "                if start_idx < len(domain_examples):\n",
    "                    level_indices = [idx for idx, _ in domain_examples[start_idx:end_idx]]\n",
    "                    self.domain_levels[domain][level] = level_indices\n",
    "\n",
    "        print(\"Domain curricula created\")\n",
    "\n",
    "    def _print_dataset_stats(self):\n",
    "        \"\"\"Print comprehensive dataset statistics\"\"\"\n",
    "        print(f\"Dataset Statistics:\")\n",
    "        for domain in ['morphological', 'syntactic', 'semantic', 'general']:\n",
    "            count = len(self.domain_indices.get(domain, []))\n",
    "            percentage = (count / len(self.examples) * 100) if self.examples else 0\n",
    "            print(f\"  {domain}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "        if self.complexity_scores:\n",
    "            print(f\"Complexity Statistics:\")\n",
    "            print(f\"  Mean: {np.mean(self.complexity_scores):.3f}\")\n",
    "            print(f\"  Std: {np.std(self.complexity_scores):.3f}\")\n",
    "\n",
    "    def get_domain_curriculum_data(self, domain: str, level: int) -> List[int]:\n",
    "        \"\"\"Get indices for domain curriculum level\"\"\"\n",
    "        return self.domain_levels.get(domain, {}).get(level, [])\n",
    "\n",
    "    def get_domain_indices(self, domain: str) -> List[int]:\n",
    "        \"\"\"Get all indices for a specific domain\"\"\"\n",
    "        return self.domain_indices.get(domain, [])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.examples[idx]\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# IMPROVED CURRICULUM SAMPLER\n",
    "# =============================================================================\n",
    "\n",
    "class ImprovedCurriculumSampler(Sampler):\n",
    "    \"\"\"Improved curriculum sampler with better phase management\"\"\"\n",
    "\n",
    "    def __init__(self, dataset: ImprovedDomainAwareDataset, current_epoch: int, total_epochs: int):\n",
    "        self.dataset = dataset\n",
    "        self.current_epoch = current_epoch\n",
    "        self.total_epochs = total_epochs\n",
    "\n",
    "        # Phase configuration\n",
    "        self.foundation_epochs = 3\n",
    "        self.curriculum_epochs = 4\n",
    "        self.integration_epochs = 3\n",
    "\n",
    "        # Determine current phase\n",
    "        self.phase = self._get_current_phase()\n",
    "        self.active_indices = self._get_phase_indices()\n",
    "\n",
    "        print(f\"Curriculum Sampler - Epoch {current_epoch + 1}, Phase: {self.phase}\")\n",
    "        print(f\"Active examples: {len(self.active_indices)}\")\n",
    "\n",
    "    def _get_current_phase(self) -> str:\n",
    "        \"\"\"Determine current training phase\"\"\"\n",
    "        if self.current_epoch < self.foundation_epochs:\n",
    "            return \"foundation\"\n",
    "        elif self.current_epoch < self.foundation_epochs + self.curriculum_epochs:\n",
    "            return \"curriculum\"\n",
    "        else:\n",
    "            return \"integration\"\n",
    "\n",
    "    def _get_phase_indices(self) -> List[int]:\n",
    "        \"\"\"Get indices based on current phase\"\"\"\n",
    "        if self.phase == \"foundation\":\n",
    "            return self._get_foundation_indices()\n",
    "        elif self.phase == \"curriculum\":\n",
    "            return self._get_curriculum_indices()\n",
    "        else:\n",
    "            return self._get_integration_indices()\n",
    "\n",
    "    def _get_foundation_indices(self) -> List[int]:\n",
    "        \"\"\"Foundation phase: Balanced with morphology emphasis\"\"\"\n",
    "        all_indices = []\n",
    "\n",
    "        # Get domain indices\n",
    "        morph_indices = self.dataset.get_domain_indices('morphological')\n",
    "        other_indices = (self.dataset.get_domain_indices('syntactic') +\n",
    "                        self.dataset.get_domain_indices('semantic') +\n",
    "                        self.dataset.get_domain_indices('general'))\n",
    "\n",
    "        # Morphology emphasis (60% morphological, 40% others)\n",
    "        target_morph = min(len(morph_indices), len(self.dataset) * 6 // 10)\n",
    "        all_indices.extend(morph_indices[:target_morph])\n",
    "\n",
    "        remaining = len(self.dataset) - len(all_indices)\n",
    "        if other_indices and remaining > 0:\n",
    "            sample_size = min(len(other_indices), remaining)\n",
    "            all_indices.extend(random.sample(other_indices, sample_size))\n",
    "\n",
    "        # Ensure we have examples\n",
    "        if not all_indices:\n",
    "            all_indices = list(range(len(self.dataset)))\n",
    "\n",
    "        random.shuffle(all_indices)\n",
    "        return all_indices\n",
    "\n",
    "    def _get_curriculum_indices(self) -> List[int]:\n",
    "        \"\"\"Curriculum phase: Progressive difficulty\"\"\"\n",
    "        curriculum_epoch = self.current_epoch - self.foundation_epochs\n",
    "        progress = curriculum_epoch / max(self.curriculum_epochs - 1, 1)\n",
    "\n",
    "        all_indices = []\n",
    "\n",
    "        # Progressive curriculum for each domain\n",
    "        for domain in ['morphological', 'syntactic', 'semantic']:\n",
    "            max_levels = {'morphological': 2, 'syntactic': 3, 'semantic': 4}\n",
    "            max_level = max_levels[domain]\n",
    "            current_level = min(int(progress * (max_level + 1)), max_level)\n",
    "\n",
    "            for level in range(current_level + 1):\n",
    "                level_indices = self.dataset.get_domain_curriculum_data(domain, level)\n",
    "                all_indices.extend(level_indices)\n",
    "\n",
    "        # Add general examples\n",
    "        general_indices = self.dataset.get_domain_indices('general')\n",
    "        if general_indices:\n",
    "            sample_size = min(len(general_indices), len(self.dataset) // 4)\n",
    "            all_indices.extend(random.sample(general_indices, sample_size))\n",
    "\n",
    "        # Fallback to all examples if curriculum is empty\n",
    "        if not all_indices:\n",
    "            all_indices = list(range(len(self.dataset)))\n",
    "\n",
    "        random.shuffle(all_indices)\n",
    "        return list(set(all_indices))\n",
    "\n",
    "    def _get_integration_indices(self) -> List[int]:\n",
    "        \"\"\"Integration phase: Use all examples\"\"\"\n",
    "        all_indices = list(range(len(self.dataset)))\n",
    "        random.shuffle(all_indices)\n",
    "        return all_indices\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.active_indices)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.active_indices)\n",
    "\n",
    "class TrainingConfig:\n",
    "    \"\"\"Training configuration with enhanced regularization parameters\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Existing model configuration\n",
    "        self.model_name = 't5-small'\n",
    "        self.max_source_length = 512\n",
    "        self.max_target_length = 256\n",
    "\n",
    "        # Existing training parameters\n",
    "        self.num_epochs = 10\n",
    "        self.batch_size = 16\n",
    "        self.learning_rate = 5e-5\n",
    "        self.weight_decay = 0.01\n",
    "        self.warmup_ratio = 0.1\n",
    "        self.max_grad_norm = 1.0\n",
    "        self.patience = 4\n",
    "\n",
    "        # Existing curriculum phases\n",
    "        self.foundation_epochs = 3\n",
    "        self.curriculum_epochs = 4\n",
    "        self.integration_epochs = 3\n",
    "\n",
    "        # Existing data limits\n",
    "        self.max_train_examples = None\n",
    "        self.max_val_examples = 20000\n",
    "\n",
    "        # NEW: Enhanced regularization parameters\n",
    "        self.lambda_accel = 1e-4  # Acceleration penalty coefficient\n",
    "        self.regularization_strategy = 'first_middle_last'  # Which layers to penalize\n",
    "        self.temporal_alpha_init = 0.1  # Initial value for temporal embedding weight\n",
    "        self.enable_temporal_embedding = True  # Enable/disable temporal embedding difference\n",
    "        self.temporal_alpha_range = (0.01, 0.5)  # Clamp range for alpha\n",
    "\n",
    "        # Hardware\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "        print(\"Enhanced training configuration initialized with regularization\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# ENHANCED REGULARIZATION LOSSES\n",
    "# =============================================================================\n",
    "\n",
    "class TargetedSmoothnessLoss(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Applies smoothness penalty to select transformer layers in T5 encoder/decoder\n",
    "    \"\"\"\n",
    "    def __init__(self, lambda_coeff: float, strategy: str = 'first_middle_last'):\n",
    "        super().__init__()\n",
    "        self.lambda_coeff = lambda_coeff\n",
    "        self.strategy = strategy\n",
    "        \n",
    "        print(f\"Initialized TargetedSmoothnessLoss with lambda={lambda_coeff}, strategy='{strategy}'\")\n",
    "\n",
    "    def _get_layer_indices(self, total_layers: int) -> list:\n",
    "        \"\"\"Select which layers to penalize based on strategy\"\"\"\n",
    "        if total_layers < 3:\n",
    "            return [total_layers // 2] if total_layers > 0 else []\n",
    "        \n",
    "        if self.strategy == 'first_middle_last':\n",
    "            # Penalize layers 1, middle, and last (avoiding embedding layer at 0)\n",
    "            return [1, (total_layers // 2) + 1, total_layers]\n",
    "        elif self.strategy == 'middle_only':\n",
    "            return [total_layers // 2]\n",
    "        elif self.strategy == 'all_layers':\n",
    "            return list(range(1, total_layers + 1))  # All except embedding\n",
    "        else:\n",
    "            return [1, total_layers]  # first and last transformer layers\n",
    "\n",
    "    def _calculate_penalty(self, hidden_states: tuple) -> torch.Tensor:\n",
    "        \"\"\"Calculate acceleration penalty using second-order finite differences\"\"\"\n",
    "        if not hidden_states or len(hidden_states) < 2:\n",
    "            return torch.tensor(0.0, device=hidden_states[0].device if hidden_states else torch.device('cpu'))\n",
    "            \n",
    "        num_transformer_layers = len(hidden_states) - 1  # Exclude embedding layer\n",
    "        indices_to_penalize = self._get_layer_indices(num_transformer_layers)\n",
    "        \n",
    "        total_penalty = torch.tensor(0.0, device=hidden_states[0].device)\n",
    "        penalty_count = 0\n",
    "        \n",
    "        for layer_idx in set(indices_to_penalize):\n",
    "            # Ensure valid index for hidden_states tuple\n",
    "            if 0 < layer_idx < len(hidden_states):\n",
    "                layer_hidden_states = hidden_states[layer_idx]\n",
    "                \n",
    "                # Need at least 3 positions for second-order difference\n",
    "                if layer_hidden_states.shape[1] > 2:\n",
    "                    # Extract consecutive positions\n",
    "                    h_t = layer_hidden_states[:, :-2, :]          # positions 0 to n-2\n",
    "                    h_t_plus_1 = layer_hidden_states[:, 1:-1, :]  # positions 1 to n-1  \n",
    "                    h_t_plus_2 = layer_hidden_states[:, 2:, :]    # positions 2 to n\n",
    "                    \n",
    "                    # Second-order finite difference (discrete acceleration)\n",
    "                    acceleration = h_t_plus_2 - 2 * h_t_plus_1 + h_t\n",
    "                    \n",
    "                    # L2 penalty on acceleration magnitude\n",
    "                    layer_penalty = torch.mean(torch.norm(acceleration, p=2, dim=2)**2)\n",
    "                    total_penalty += layer_penalty\n",
    "                    penalty_count += 1\n",
    "                    \n",
    "        # Average penalty across penalized layers\n",
    "        if penalty_count > 0:\n",
    "            total_penalty = total_penalty / penalty_count\n",
    "            \n",
    "        return total_penalty\n",
    "\n",
    "    def forward(self, outputs, original_loss: torch.Tensor) -> dict:\n",
    "        \"\"\"Compute regularized loss and return detailed breakdown\"\"\"\n",
    "        if self.lambda_coeff <= 0:\n",
    "            return {\n",
    "                'total_loss': original_loss,\n",
    "                'original_loss': original_loss,\n",
    "                'smoothness_penalty': torch.tensor(0.0, device=original_loss.device),\n",
    "                'encoder_penalty': torch.tensor(0.0, device=original_loss.device),\n",
    "                'decoder_penalty': torch.tensor(0.0, device=original_loss.device)\n",
    "            }\n",
    "\n",
    "        encoder_penalty = torch.tensor(0.0, device=original_loss.device)\n",
    "        decoder_penalty = torch.tensor(0.0, device=original_loss.device)\n",
    "        \n",
    "        # Apply to encoder if available\n",
    "        if hasattr(outputs, 'encoder_hidden_states') and outputs.encoder_hidden_states:\n",
    "            encoder_penalty = self._calculate_penalty(outputs.encoder_hidden_states)\n",
    "            \n",
    "        # Apply to decoder if available  \n",
    "        if hasattr(outputs, 'decoder_hidden_states') and outputs.decoder_hidden_states:\n",
    "            decoder_penalty = self._calculate_penalty(outputs.decoder_hidden_states)\n",
    "            \n",
    "        total_smoothness_penalty = encoder_penalty + decoder_penalty\n",
    "        total_loss = original_loss + (self.lambda_coeff * total_smoothness_penalty)\n",
    "        \n",
    "        return {\n",
    "            'total_loss': total_loss,\n",
    "            'original_loss': original_loss,\n",
    "            'smoothness_penalty': total_smoothness_penalty,\n",
    "            'encoder_penalty': encoder_penalty,\n",
    "            'decoder_penalty': decoder_penalty\n",
    "        }\n",
    "\n",
    "\n",
    "class TemporalEmbeddingEnhancement(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    WARNING: This implementation has significant architectural concerns.\n",
    "    Adds learnable temporal differences to first layer embeddings.\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha_init: float = 0.1, alpha_range: tuple = (0.01, 0.5)):\n",
    "        super().__init__()\n",
    "        self.alpha = torch.nn.Parameter(torch.tensor(alpha_init))\n",
    "        self.alpha_range = alpha_range\n",
    "        \n",
    "        print(f\"WARNING: TemporalEmbeddingEnhancement initialized with alpha={alpha_init}\")\n",
    "        print(f\"This approach has known architectural inconsistencies with T5.\")\n",
    "\n",
    "    def _clamp_alpha(self):\n",
    "        \"\"\"Clamp alpha to valid range during training\"\"\"\n",
    "        with torch.no_grad():\n",
    "            self.alpha.clamp_(self.alpha_range[0], self.alpha_range[1])\n",
    "\n",
    "    def forward(self, encoder_hidden_states: tuple, decoder_hidden_states: tuple = None) -> dict:\n",
    "        \"\"\"\n",
    "        Apply temporal embedding enhancement to first layer embeddings\n",
    "        Returns modified hidden states and regularization info\n",
    "        \"\"\"\n",
    "        self._clamp_alpha()\n",
    "        \n",
    "        enhanced_encoder = None\n",
    "        enhanced_decoder = None\n",
    "        temporal_penalty = torch.tensor(0.0, device=self.alpha.device)\n",
    "        \n",
    "        # Process encoder embeddings (first hidden state)\n",
    "        if encoder_hidden_states and len(encoder_hidden_states) > 0:\n",
    "            enhanced_encoder = self._enhance_embeddings(encoder_hidden_states[0])\n",
    "            \n",
    "        # Process decoder embeddings if available\n",
    "        if decoder_hidden_states and len(decoder_hidden_states) > 0:\n",
    "            enhanced_decoder = self._enhance_embeddings(decoder_hidden_states[0])\n",
    "            \n",
    "        # Calculate temporal penalty (L2 on alpha to prevent explosion)\n",
    "        temporal_penalty = 0.01 * (self.alpha ** 2)\n",
    "        \n",
    "        return {\n",
    "            'enhanced_encoder': enhanced_encoder,\n",
    "            'enhanced_decoder': enhanced_decoder,\n",
    "            'temporal_penalty': temporal_penalty,\n",
    "            'alpha_value': self.alpha.item()\n",
    "        }\n",
    "    \n",
    "    def _enhance_embeddings(self, embeddings: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Apply temporal difference enhancement to embeddings\n",
    "        embeddings: [batch_size, seq_length, hidden_dim]\n",
    "        \"\"\"\n",
    "        if embeddings.shape[1] < 2:\n",
    "            return embeddings\n",
    "            \n",
    "        # Calculate differences between consecutive positions\n",
    "        current_embeds = embeddings[:, 1:, :]     # positions 1 to n\n",
    "        previous_embeds = embeddings[:, :-1, :]   # positions 0 to n-1\n",
    "        temporal_diff = current_embeds - previous_embeds\n",
    "        \n",
    "        # Apply learnable weighting and add back to current embeddings\n",
    "        enhanced_current = current_embeds + (self.alpha * temporal_diff)\n",
    "        \n",
    "        # Concatenate with first embedding (unchanged)\n",
    "        enhanced_embeddings = torch.cat([\n",
    "            embeddings[:, :1, :],  # First embedding unchanged\n",
    "            enhanced_current       # Enhanced subsequent embeddings\n",
    "        ], dim=1)\n",
    "        \n",
    "        return enhanced_embeddings\n",
    "\n",
    "\n",
    "class CombinedRegularizationLoss(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Combines both smoothness penalty and temporal embedding enhancement\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.smoothness_loss = TargetedSmoothnessLoss(\n",
    "            lambda_coeff=getattr(config, 'lambda_accel', 0.0),\n",
    "            strategy=getattr(config, 'regularization_strategy', 'first_middle_last')\n",
    "        )\n",
    "        \n",
    "        self.temporal_enhancement = None\n",
    "        if getattr(config, 'enable_temporal_embedding', False):\n",
    "            self.temporal_enhancement = TemporalEmbeddingEnhancement(\n",
    "                alpha_init=getattr(config, 'temporal_alpha_init', 0.1),\n",
    "                alpha_range=getattr(config, 'temporal_alpha_range', (0.01, 0.5))\n",
    "            )\n",
    "            \n",
    "        print(f\"Combined regularization initialized:\")\n",
    "        print(f\"  Smoothness penalty: {'Enabled' if self.smoothness_loss.lambda_coeff > 0 else 'Disabled'}\")\n",
    "        print(f\"  Temporal enhancement: {'Enabled' if self.temporal_enhancement else 'Disabled'}\")\n",
    "\n",
    "    def forward(self, outputs, original_loss: torch.Tensor) -> dict:\n",
    "        \"\"\"Apply combined regularization and return comprehensive loss breakdown\"\"\"\n",
    "        \n",
    "        # Apply smoothness penalty\n",
    "        smoothness_results = self.smoothness_loss(outputs, original_loss)\n",
    "        current_loss = smoothness_results['total_loss']\n",
    "        \n",
    "        # Apply temporal enhancement if enabled\n",
    "        temporal_results = {'temporal_penalty': torch.tensor(0.0, device=original_loss.device), 'alpha_value': 0.0}\n",
    "        \n",
    "        if self.temporal_enhancement:\n",
    "            temporal_results = self.temporal_enhancement(\n",
    "                encoder_hidden_states=getattr(outputs, 'encoder_hidden_states', None),\n",
    "                decoder_hidden_states=getattr(outputs, 'decoder_hidden_states', None)\n",
    "            )\n",
    "            # Add temporal penalty to loss\n",
    "            current_loss = current_loss + temporal_results['temporal_penalty']\n",
    "        \n",
    "        # Combine all results\n",
    "        return {\n",
    "            'total_loss': current_loss,\n",
    "            'original_loss': original_loss,\n",
    "            'smoothness_penalty': smoothness_results['smoothness_penalty'],\n",
    "            'encoder_penalty': smoothness_results['encoder_penalty'],\n",
    "            'decoder_penalty': smoothness_results['decoder_penalty'],\n",
    "            'temporal_penalty': temporal_results['temporal_penalty'],\n",
    "            'alpha_value': temporal_results['alpha_value']\n",
    "        }\n",
    "\n",
    "# =============================================================================\n",
    "# IMPROVED TRAINER WITH BETTER ERROR HANDLING\n",
    "# =============================================================================\n",
    "\n",
    "class ImprovedT5Trainer:\n",
    "    \"\"\"Improved trainer with robust error handling and better save/load\"\"\"\n",
    "\n",
    "    def __init__(self, model: T5ForConditionalGeneration, tokenizer: T5Tokenizer,\n",
    "                data_manager: RobustDataManager, config):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data_manager = data_manager\n",
    "        self.config = config\n",
    "\n",
    "        # Setup optimizer (existing code)\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters()\n",
    "                        if not any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": config.weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters()\n",
    "                        if any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        self.optimizer = AdamW(optimizer_grouped_parameters, lr=config.learning_rate)\n",
    "        self.scheduler = None\n",
    "        self.global_step = 0\n",
    "\n",
    "        # NEW: Initialize combined regularization\n",
    "        self.regularization_loss = CombinedRegularizationLoss(config).to(config.device)\n",
    "        \n",
    "        # Include regularization parameters in optimizer if temporal enhancement is enabled\n",
    "        if self.regularization_loss.temporal_enhancement:\n",
    "            temporal_params = list(self.regularization_loss.temporal_enhancement.parameters())\n",
    "            if temporal_params:\n",
    "                self.optimizer.add_param_group({\n",
    "                    'params': temporal_params,\n",
    "                    'lr': config.learning_rate * 0.1,  # Lower learning rate for alpha\n",
    "                    'weight_decay': 0.0\n",
    "                })\n",
    "                print(f\"Added {len(temporal_params)} temporal parameters to optimizer\")\n",
    "\n",
    "        # Existing training metrics initialization\n",
    "        self.training_stats = {\n",
    "            'epoch_losses': [],\n",
    "            'val_losses': [],\n",
    "            'learning_rates': [],\n",
    "            'phase_transitions': [],\n",
    "            'regularization_stats': []  # NEW: Track regularization metrics\n",
    "        }\n",
    "\n",
    "        print(\"Enhanced T5 Trainer initialized with combined regularization\")\n",
    "\n",
    "    def train(self, train_dataset: ImprovedDomainAwareDataset,\n",
    "              val_dataset: Optional[ImprovedDomainAwareDataset] = None,\n",
    "              resume_epoch: int = 0):\n",
    "        \"\"\"Main training loop with improved error handling\"\"\"\n",
    "\n",
    "        print(f\"Starting training with {len(train_dataset)} examples\")\n",
    "\n",
    "        # Setup scheduler\n",
    "        steps_per_epoch = max(1, len(train_dataset) // self.config.batch_size)\n",
    "        total_steps = steps_per_epoch * self.config.num_epochs\n",
    "        warmup_steps = int(total_steps * self.config.warmup_ratio)\n",
    "\n",
    "        self.scheduler = get_linear_schedule_with_warmup(\n",
    "            self.optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps\n",
    "        )\n",
    "\n",
    "        # Move model to device\n",
    "        self.model.to(self.config.device)\n",
    "\n",
    "        best_val_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "\n",
    "        for epoch in range(resume_epoch, self.config.num_epochs):\n",
    "            print(f\"\\nEpoch {epoch + 1}/{self.config.num_epochs}\")\n",
    "\n",
    "            # Create curriculum sampler\n",
    "            sampler = ImprovedCurriculumSampler(train_dataset, epoch, self.config.num_epochs)\n",
    "\n",
    "            # Ensure we have examples\n",
    "            if len(sampler) == 0:\n",
    "                print(\"Warning: No examples in sampler, using all dataset\")\n",
    "                sampler.active_indices = list(range(len(train_dataset)))\n",
    "\n",
    "            # Create data loader\n",
    "            train_loader = DataLoader(\n",
    "                train_dataset,\n",
    "                batch_size=self.config.batch_size,\n",
    "                sampler=sampler,\n",
    "                collate_fn=self._collate_fn,\n",
    "                num_workers=0,\n",
    "                pin_memory=True if self.config.device == 'cuda' else False\n",
    "            )\n",
    "\n",
    "            # Train epoch\n",
    "            epoch_loss = self._train_epoch(train_loader)\n",
    "\n",
    "            # Validation\n",
    "            val_loss = None\n",
    "            if val_dataset:\n",
    "                val_loss = self._evaluate(val_dataset)\n",
    "\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    patience_counter = 0\n",
    "                    self._save_checkpoint(epoch, val_loss, is_best=True)\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "\n",
    "                if patience_counter >= self.config.patience:\n",
    "                    print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "                    break\n",
    "\n",
    "            # Save training stats\n",
    "            self.training_stats['epoch_losses'].append(epoch_loss)\n",
    "            self.training_stats['val_losses'].append(val_loss or 0)\n",
    "            self.training_stats['learning_rates'].append(self.scheduler.get_last_lr()[0])\n",
    "\n",
    "            # Print progress\n",
    "            print(f\"Train Loss: {epoch_loss:.4f}\")\n",
    "            if val_loss:\n",
    "                print(f\"Val Loss: {val_loss:.4f}, Best: {best_val_loss:.4f}\")\n",
    "\n",
    "            # Save regular checkpoint\n",
    "            if (epoch + 1) % 2 == 0:\n",
    "                self._save_checkpoint(epoch, val_loss or epoch_loss)\n",
    "\n",
    "            # Memory cleanup\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "        # Save final model\n",
    "        self._save_final_model()\n",
    "        return self.training_stats\n",
    "\n",
    "    def _train_epoch(self, train_loader) -> float:\n",
    "        \"\"\"Train single epoch with enhanced regularization and gradient accumulation\"\"\"\n",
    "        self.model.train()\n",
    "        epoch_loss = 0.0\n",
    "        num_batches = 0\n",
    "        \n",
    "        # Regularization tracking\n",
    "        epoch_reg_stats = {\n",
    "            'smoothness_penalty': 0.0,\n",
    "            'temporal_penalty': 0.0,\n",
    "            'alpha_values': []\n",
    "        }\n",
    "\n",
    "        accumulation_steps = max(1, 32 // self.config.batch_size)\n",
    "\n",
    "        for batch_idx, batch in enumerate(tqdm(train_loader, desc='Enhanced Training')):\n",
    "            try:\n",
    "                # Move to device\n",
    "                batch = {k: v.to(self.config.device) if isinstance(v, torch.Tensor) else v\n",
    "                        for k, v in batch.items()}\n",
    "\n",
    "                # Forward pass with hidden states enabled\n",
    "                outputs = self.model(\n",
    "                    input_ids=batch['input_ids'],\n",
    "                    attention_mask=batch['attention_mask'],\n",
    "                    labels=batch['labels'],\n",
    "                    output_hidden_states=True  # CRITICAL: Enable hidden states\n",
    "                )\n",
    "\n",
    "                # Apply combined regularization\n",
    "                loss_results = self.regularization_loss(outputs, outputs.loss)\n",
    "                total_loss = loss_results['total_loss']\n",
    "                \n",
    "                loss = total_loss / accumulation_steps\n",
    "                loss.backward()\n",
    "\n",
    "                # Track regularization statistics\n",
    "                epoch_reg_stats['smoothness_penalty'] += loss_results['smoothness_penalty'].item()\n",
    "                epoch_reg_stats['temporal_penalty'] += loss_results['temporal_penalty'].item()\n",
    "                if loss_results['alpha_value'] > 0:\n",
    "                    epoch_reg_stats['alpha_values'].append(loss_results['alpha_value'])\n",
    "\n",
    "                # Gradient accumulation and optimization\n",
    "                if (batch_idx + 1) % accumulation_steps == 0:\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.max_grad_norm)\n",
    "                    \n",
    "                    # Also clip regularization parameters\n",
    "                    if self.regularization_loss.temporal_enhancement:\n",
    "                        torch.nn.utils.clip_grad_norm_(\n",
    "                            self.regularization_loss.temporal_enhancement.parameters(), \n",
    "                            self.config.max_grad_norm\n",
    "                        )\n",
    "                    \n",
    "                    self.optimizer.step()\n",
    "                    self.scheduler.step()\n",
    "                    self.optimizer.zero_grad()\n",
    "                    self.global_step += 1\n",
    "\n",
    "                epoch_loss += loss.item() * accumulation_steps\n",
    "                num_batches += 1\n",
    "\n",
    "                # Log regularization stats every 100 batches\n",
    "                if batch_idx % 100 == 0 and batch_idx > 0:\n",
    "                    avg_smooth = epoch_reg_stats['smoothness_penalty'] / num_batches\n",
    "                    avg_temporal = epoch_reg_stats['temporal_penalty'] / num_batches\n",
    "                    current_alpha = epoch_reg_stats['alpha_values'][-1] if epoch_reg_stats['alpha_values'] else 0.0\n",
    "                    \n",
    "                    print(f\"Batch {batch_idx}: Smoothness={avg_smooth:.6f}, Temporal={avg_temporal:.6f}, Alpha={current_alpha:.4f}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error in enhanced training batch {batch_idx}: {e}\")\n",
    "                continue\n",
    "\n",
    "        # Save epoch regularization statistics\n",
    "        final_reg_stats = {\n",
    "            'avg_smoothness_penalty': epoch_reg_stats['smoothness_penalty'] / max(num_batches, 1),\n",
    "            'avg_temporal_penalty': epoch_reg_stats['temporal_penalty'] / max(num_batches, 1),\n",
    "            'final_alpha': epoch_reg_stats['alpha_values'][-1] if epoch_reg_stats['alpha_values'] else 0.0,\n",
    "            'alpha_std': np.std(epoch_reg_stats['alpha_values']) if len(epoch_reg_stats['alpha_values']) > 1 else 0.0\n",
    "        }\n",
    "        self.training_stats['regularization_stats'].append(final_reg_stats)\n",
    "\n",
    "        return epoch_loss / max(num_batches, 1)\n",
    "\n",
    "    def _evaluate(self, val_dataset) -> float:\n",
    "        \"\"\"Evaluate model on validation set\"\"\"\n",
    "        self.model.eval()\n",
    "\n",
    "        # Use subset for faster validation\n",
    "        val_size = min(1000, len(val_dataset))\n",
    "        val_indices = random.sample(range(len(val_dataset)), val_size)\n",
    "\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=self.config.batch_size,\n",
    "            sampler=val_indices,\n",
    "            collate_fn=self._collate_fn,\n",
    "            num_workers=0\n",
    "        )\n",
    "\n",
    "        total_loss = 0.0\n",
    "        num_batches = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc='Validation'):\n",
    "                try:\n",
    "                    batch = {k: v.to(self.config.device) if isinstance(v, torch.Tensor) else v\n",
    "                            for k, v in batch.items()}\n",
    "\n",
    "                    outputs = self.model(\n",
    "                        input_ids=batch['input_ids'],\n",
    "                        attention_mask=batch['attention_mask'],\n",
    "                        labels=batch['labels'],\n",
    "                        output_hidden_states=True  # Add this line\n",
    "                    )\n",
    "\n",
    "                    total_loss += outputs.loss.item()\n",
    "                    num_batches += 1\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error in validation batch: {e}\")\n",
    "                    continue\n",
    "\n",
    "        return total_loss / max(num_batches, 1)\n",
    "\n",
    "    def _collate_fn(self, batch):\n",
    "        \"\"\"Improved collate function with better error handling\"\"\"\n",
    "        try:\n",
    "            # Filter out None examples\n",
    "            batch = [item for item in batch if item is not None]\n",
    "\n",
    "            if not batch:\n",
    "                # Return dummy batch\n",
    "                return self._create_dummy_batch(1)\n",
    "\n",
    "            input_ids = torch.stack([item['input_ids'] for item in batch])\n",
    "            attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
    "            labels = torch.stack([item['labels'] for item in batch])\n",
    "\n",
    "            # Mask padding tokens in labels\n",
    "            labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "\n",
    "            return {\n",
    "                'input_ids': input_ids,\n",
    "                'attention_mask': attention_mask,\n",
    "                'labels': labels\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Collate function error: {e}\")\n",
    "            return self._create_dummy_batch(len(batch))\n",
    "\n",
    "    def _create_dummy_batch(self, batch_size: int):\n",
    "        \"\"\"Create dummy batch for error recovery\"\"\"\n",
    "        return {\n",
    "            'input_ids': torch.zeros(batch_size, self.config.max_source_length, dtype=torch.long),\n",
    "            'attention_mask': torch.ones(batch_size, self.config.max_source_length, dtype=torch.long),\n",
    "            'labels': torch.full((batch_size, self.config.max_target_length), -100, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "    def _save_checkpoint(self, epoch: int, loss: float, is_best: bool = False):\n",
    "        \"\"\"Save checkpoint with regularization state\"\"\"\n",
    "        metrics = {\n",
    "            'epoch': epoch + 1,\n",
    "            'loss': loss,\n",
    "            'global_step': self.global_step,\n",
    "            'regularization_stats': self.training_stats['regularization_stats'][-1] if self.training_stats['regularization_stats'] else {}\n",
    "        }\n",
    "\n",
    "        # Save regularization state\n",
    "        regularization_state = {\n",
    "            'smoothness_lambda': self.regularization_loss.smoothness_loss.lambda_coeff,\n",
    "            'alpha_value': getattr(self.regularization_loss.temporal_enhancement, 'alpha', torch.tensor(0.0)).item() if self.regularization_loss.temporal_enhancement else 0.0\n",
    "        }\n",
    "        \n",
    "        success = self.data_manager.save_model_checkpoint(\n",
    "            self.model, self.tokenizer, self.optimizer, self.scheduler,\n",
    "            epoch, metrics, is_best\n",
    "        )\n",
    "        \n",
    "        # Also save regularization state separately\n",
    "        if success:\n",
    "            checkpoint_dir = self.data_manager.get_path('checkpoints') / ('best_model' if is_best else f'epoch_{epoch+1}')\n",
    "            reg_state_path = checkpoint_dir / 'regularization_state.pt'\n",
    "            torch.save(regularization_state, reg_state_path)\n",
    "\n",
    "        if not success:\n",
    "            print(f\"Failed to save enhanced checkpoint for epoch {epoch + 1}\")\n",
    "\n",
    "    def _save_final_model(self):\n",
    "        \"\"\"Save final model to designated directory\"\"\"\n",
    "        final_dir = self.data_manager.get_path('final_models') / 'final_model'\n",
    "        final_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        try:\n",
    "            self.model.save_pretrained(final_dir)\n",
    "            self.tokenizer.save_pretrained(final_dir)\n",
    "\n",
    "            # Save training stats\n",
    "            with open(final_dir / 'training_stats.json', 'w') as f:\n",
    "                json.dump(self.training_stats, f, indent=2)\n",
    "\n",
    "            print(f\"Final model saved to {final_dir}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to save final model: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CHECKPOINT LOADING FUNCTIONALITY\n",
    "# =============================================================================\n",
    "\n",
    "def load_existing_checkpoint(data_manager: RobustDataManager, checkpoint_name: str = None):\n",
    "    \"\"\"FIXED: Load checkpoint with proper safetensors support and better selection\"\"\"\n",
    "    checkpoints = data_manager.list_checkpoints()\n",
    "\n",
    "    if not checkpoints:\n",
    "        print(\"âŒ No checkpoints found\")\n",
    "        return None\n",
    "\n",
    "    if checkpoint_name:\n",
    "        # Find specific checkpoint\n",
    "        target_checkpoint = None\n",
    "        for cp in checkpoints:\n",
    "            if checkpoint_name in cp['checkpoint_name']:\n",
    "                target_checkpoint = cp\n",
    "                break\n",
    "\n",
    "        if not target_checkpoint:\n",
    "            print(f\"âŒ Checkpoint '{checkpoint_name}' not found\")\n",
    "            print(\"Available checkpoints:\")\n",
    "            for cp in checkpoints:\n",
    "                print(f\"  - {cp['checkpoint_name']} (Epoch {cp['epoch']})\")\n",
    "            return None\n",
    "    else:\n",
    "        # FIXED: Better automatic checkpoint selection\n",
    "        # Priority: 1) Latest epoch checkpoint, 2) Best checkpoint, 3) Any checkpoint\n",
    "\n",
    "        epoch_checkpoints = [cp for cp in checkpoints if cp['checkpoint_name'].startswith('epoch_')]\n",
    "        best_checkpoints = [cp for cp in checkpoints if cp.get('is_best', False)]\n",
    "\n",
    "        if epoch_checkpoints:\n",
    "            # Sort by epoch number and take the latest\n",
    "            target_checkpoint = max(epoch_checkpoints, key=lambda x: x.get('epoch', 0))\n",
    "            print(f\"ðŸƒ Using latest epoch checkpoint: {target_checkpoint['checkpoint_name']} (Epoch {target_checkpoint['epoch']})\")\n",
    "        elif best_checkpoints:\n",
    "            target_checkpoint = best_checkpoints[0]\n",
    "            print(f\"ðŸ† Using best checkpoint: {target_checkpoint['checkpoint_name']} (Epoch {target_checkpoint['epoch']})\")\n",
    "        else:\n",
    "            target_checkpoint = max(checkpoints, key=lambda x: x.get('epoch', 0))\n",
    "            print(f\"ðŸ“¦ Using available checkpoint: {target_checkpoint['checkpoint_name']} (Epoch {target_checkpoint['epoch']})\")\n",
    "\n",
    "    # Try loading the checkpoint\n",
    "    return _load_checkpoint_files(target_checkpoint)\n",
    "def _load_checkpoint_files(checkpoint_info: Dict):\n",
    "    \"\"\"FIXED: Load checkpoint files with safetensors support\"\"\"\n",
    "    try:\n",
    "        checkpoint_path = Path(checkpoint_info['path'])\n",
    "        print(f\"ðŸ“‚ Loading checkpoint from: {checkpoint_path}\")\n",
    "\n",
    "        # Check what model format is available\n",
    "        has_safetensors = checkpoint_info.get('has_safetensors', False)\n",
    "        has_pytorch_model = checkpoint_info.get('has_pytorch_model', False)\n",
    "\n",
    "        print(f\"   Model format available: {'safetensors' if has_safetensors else 'pytorch_bin' if has_pytorch_model else 'none'}\")\n",
    "\n",
    "        # Load model - FIXED to handle safetensors properly\n",
    "        print(\"ðŸ”„ Loading model...\")\n",
    "        try:\n",
    "            # Try direct loading first (works for both formats)\n",
    "            model = T5ForConditionalGeneration.from_pretrained(checkpoint_path)\n",
    "            print(\"âœ… Model loaded successfully with from_pretrained\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ from_pretrained failed: {e}\")\n",
    "            # Try manual loading\n",
    "            try:\n",
    "                config = T5Config.from_pretrained(checkpoint_path)\n",
    "                model = T5ForConditionalGeneration(config)\n",
    "\n",
    "                if has_safetensors:\n",
    "                    # For safetensors, transformers should handle it automatically\n",
    "                    model = T5ForConditionalGeneration.from_pretrained(checkpoint_path, use_safetensors=True)\n",
    "                    print(\"âœ… Model loaded with safetensors\")\n",
    "                elif has_pytorch_model:\n",
    "                    state_dict = torch.load(checkpoint_path / 'pytorch_model.bin', map_location='cpu')\n",
    "                    model.load_state_dict(state_dict)\n",
    "                    print(\"âœ… Model loaded with pytorch state_dict\")\n",
    "                else:\n",
    "                    print(\"âŒ No compatible model file found\")\n",
    "                    return None\n",
    "\n",
    "            except Exception as e2:\n",
    "                print(f\"âŒ Manual loading also failed: {e2}\")\n",
    "                return None\n",
    "\n",
    "        # Load tokenizer - FIXED to be more flexible\n",
    "        print(\"ðŸ”„ Loading tokenizer...\")\n",
    "        try:\n",
    "            tokenizer = T5Tokenizer.from_pretrained(checkpoint_path)\n",
    "            print(\"âœ… Tokenizer loaded successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Local tokenizer loading failed: {e}\")\n",
    "            # Fallback to original model tokenizer\n",
    "            try:\n",
    "                tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "                print(\"âœ… Using fallback tokenizer (t5-small)\")\n",
    "            except Exception as e2:\n",
    "                print(f\"âŒ Even fallback tokenizer failed: {e2}\")\n",
    "                return None\n",
    "\n",
    "        # Load training state if available\n",
    "        training_state = None\n",
    "        if checkpoint_info.get('has_training_state', False):\n",
    "            try:\n",
    "                print(\"ðŸ”„ Loading training state...\")\n",
    "                training_state_path = checkpoint_path / 'training_state.pt'\n",
    "                training_state = torch.load(training_state_path, map_location='cpu')\n",
    "                print(\"âœ… Training state loaded successfully\")\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Failed to load training state: {e}\")\n",
    "                print(\"   Will continue without training state\")\n",
    "\n",
    "        print(f\"âœ… Successfully loaded checkpoint: {checkpoint_info['checkpoint_name']}\")\n",
    "        return {\n",
    "            'model': model,\n",
    "            'tokenizer': tokenizer,\n",
    "            'training_state': training_state,\n",
    "            'checkpoint_info': checkpoint_info\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed to load checkpoint: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# UTILITY FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def validate_training_data(data_dir: str) -> bool:\n",
    "    \"\"\"Validate that training data exists and is accessible\"\"\"\n",
    "    required_files = ['babylm_train.txt', 'babylm_dev.txt']\n",
    "\n",
    "    for filename in required_files:\n",
    "        filepath = os.path.join(data_dir, filename)\n",
    "        if not os.path.exists(filepath):\n",
    "            print(f\"Required file missing: {filepath}\")\n",
    "            return False\n",
    "\n",
    "        try:\n",
    "            with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                content = f.read(1000)  # Read first 1000 chars\n",
    "                if len(content.strip()) < 100:\n",
    "                    print(f\"File appears empty or too small: {filepath}\")\n",
    "                    return False\n",
    "        except Exception as e:\n",
    "            print(f\"Cannot read file {filepath}: {e}\")\n",
    "            return False\n",
    "\n",
    "    print(\"Training data validation passed\")\n",
    "    return True\n",
    "\n",
    "\n",
    "def estimate_training_time(num_examples: int, batch_size: int, num_epochs: int,\n",
    "                         device: str = 'cpu') -> Dict[str, float]:\n",
    "    \"\"\"Estimate training time based on system configuration\"\"\"\n",
    "\n",
    "    # Rough estimates based on typical hardware\n",
    "    if device == 'cuda':\n",
    "        examples_per_minute = 1000  # GPU estimate\n",
    "    else:\n",
    "        examples_per_minute = 100   # CPU estimate\n",
    "\n",
    "    total_examples = num_examples * num_epochs\n",
    "    estimated_minutes = total_examples / examples_per_minute\n",
    "\n",
    "    return {\n",
    "        'total_examples': total_examples,\n",
    "        'estimated_minutes': estimated_minutes,\n",
    "        'estimated_hours': estimated_minutes / 60,\n",
    "        'examples_per_epoch': num_examples,\n",
    "        'batches_per_epoch': max(1, num_examples // batch_size)\n",
    "    }\n",
    "\n",
    "\n",
    "def monitor_system_resources():\n",
    "    \"\"\"Monitor system resources during training\"\"\"\n",
    "    try:\n",
    "        import psutil\n",
    "\n",
    "        cpu_percent = psutil.cpu_percent()\n",
    "        memory = psutil.virtual_memory()\n",
    "        disk = psutil.disk_usage('/')\n",
    "\n",
    "        print(f\"System Resources:\")\n",
    "        print(f\"  CPU: {cpu_percent:.1f}%\")\n",
    "        print(f\"  Memory: {memory.percent:.1f}% ({memory.used // 1024**3:.1f}GB / {memory.total // 1024**3:.1f}GB)\")\n",
    "        print(f\"  Disk: {disk.percent:.1f}% free\")\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            gpu_memory = torch.cuda.get_device_properties(0).total_memory\n",
    "            gpu_allocated = torch.cuda.memory_allocated(0)\n",
    "            gpu_percent = (gpu_allocated / gpu_memory) * 100\n",
    "            print(f\"  GPU Memory: {gpu_percent:.1f}% ({gpu_allocated // 1024**3:.1f}GB / {gpu_memory // 1024**3:.1f}GB)\")\n",
    "\n",
    "    except ImportError:\n",
    "        print(\"psutil not available for resource monitoring\")\n",
    "    except Exception as e:\n",
    "        print(f\"Resource monitoring error: {e}\")\n",
    "\n",
    "\n",
    "def create_training_report(training_stats: Dict, config: TrainingConfig,\n",
    "                         final_metrics: Dict = None) -> str:\n",
    "    \"\"\"Create comprehensive training report\"\"\"\n",
    "\n",
    "    report_lines = [\n",
    "        \"=== T5 DOMAIN-AWARE TRAINING REPORT ===\",\n",
    "        \"\",\n",
    "        \"CONFIGURATION:\",\n",
    "        f\"  Model: {config.model_name}\",\n",
    "        f\"  Epochs: {config.num_epochs}\",\n",
    "        f\"  Batch Size: {config.batch_size}\",\n",
    "        f\"  Learning Rate: {config.learning_rate}\",\n",
    "        f\"  Device: {config.device}\",\n",
    "        \"\",\n",
    "        \"TRAINING STATISTICS:\",\n",
    "    ]\n",
    "\n",
    "    if training_stats.get('epoch_losses'):\n",
    "        best_loss = min(training_stats['epoch_losses'])\n",
    "        final_loss = training_stats['epoch_losses'][-1]\n",
    "        report_lines.extend([\n",
    "            f\"  Best Training Loss: {best_loss:.4f}\",\n",
    "            f\"  Final Training Loss: {final_loss:.4f}\",\n",
    "            f\"  Loss Improvement: {training_stats['epoch_losses'][0] - final_loss:.4f}\",\n",
    "        ])\n",
    "\n",
    "    if training_stats.get('val_losses'):\n",
    "        val_losses = [l for l in training_stats['val_losses'] if l > 0]\n",
    "        if val_losses:\n",
    "            best_val_loss = min(val_losses)\n",
    "            report_lines.append(f\"  Best Validation Loss: {best_val_loss:.4f}\")\n",
    "\n",
    "    if final_metrics:\n",
    "        report_lines.extend([\n",
    "            \"\",\n",
    "            \"FINAL METRICS:\",\n",
    "        ])\n",
    "        for metric, value in final_metrics.items():\n",
    "            report_lines.append(f\"  {metric}: {value}\")\n",
    "\n",
    "    report_lines.extend([\n",
    "        \"\",\n",
    "        f\"Report generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\",\n",
    "        \"=\" * 50\n",
    "    ])\n",
    "\n",
    "    return \"\\n\".join(report_lines)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def debug_cache_files(data_manager: RobustDataManager):\n",
    "    \"\"\"Debug function to show all cache files\"\"\"\n",
    "    print(\"ðŸ” DEBUG: Cache Files Analysis\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    preprocessed_dir = data_manager.get_path('preprocessed')\n",
    "    if preprocessed_dir.exists():\n",
    "        cache_files = list(preprocessed_dir.glob('*.pkl'))\n",
    "        print(f\"Found {len(cache_files)} cache files:\")\n",
    "\n",
    "        for cache_file in sorted(cache_files):\n",
    "            try:\n",
    "                size_mb = cache_file.stat().st_size / (1024 * 1024)\n",
    "                mtime = datetime.fromtimestamp(cache_file.stat().st_mtime)\n",
    "                print(f\"  ðŸ“„ {cache_file.name}\")\n",
    "                print(f\"      Size: {size_mb:.1f}MB\")\n",
    "                print(f\"      Modified: {mtime}\")\n",
    "\n",
    "                # Try to load and get basic info\n",
    "                try:\n",
    "                    with open(cache_file, 'rb') as f:\n",
    "                        data = pickle.load(f)\n",
    "                    if hasattr(data, '__len__'):\n",
    "                        print(f\"      Length: {len(data)}\")\n",
    "                    print(f\"      Type: {type(data).__name__}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"      Error loading: {e}\")\n",
    "\n",
    "                print()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"  âŒ Error analyzing {cache_file.name}: {e}\")\n",
    "    else:\n",
    "        print(\"âŒ Preprocessed directory doesn't exist\")\n",
    "\n",
    "\n",
    "def debug_checkpoint_files(data_manager: RobustDataManager):\n",
    "    \"\"\"Debug function to show all checkpoint files\"\"\"\n",
    "    print(\"ðŸ” DEBUG: Checkpoint Files Analysis\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    checkpoint_dir = data_manager.get_path('checkpoints')\n",
    "    if checkpoint_dir.exists():\n",
    "        for subdir in sorted(checkpoint_dir.iterdir()):\n",
    "            if subdir.is_dir():\n",
    "                print(f\"ðŸ“ {subdir.name}/\")\n",
    "\n",
    "                files = list(subdir.iterdir())\n",
    "                for file_path in sorted(files):\n",
    "                    if file_path.is_file():\n",
    "                        size_mb = file_path.stat().st_size / (1024 * 1024)\n",
    "                        print(f\"  ðŸ“„ {file_path.name} ({size_mb:.1f}MB)\")\n",
    "\n",
    "                print()\n",
    "    else:\n",
    "        print(\"âŒ Checkpoint directory doesn't exist\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN TRAINING FUNCTION WITH RESUME CAPABILITY\n",
    "# =============================================================================\n",
    "\n",
    "def train_improved_t5_with_resume(data_dir: str, save_dir: str, project_name: str = \"t5_improved\",\n",
    "                                resume_from: str = None, validate_data: bool = True):\n",
    "    \"\"\"\n",
    "    Enhanced training function with resume capability and better error handling - FIXED\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"ðŸš€ Starting improved T5 training with resume capability\")\n",
    "    print(f\"ðŸ“ Data directory: {data_dir}\")\n",
    "    print(f\"ðŸ’¾ Save directory: {save_dir}\")\n",
    "\n",
    "    # FIXED: Validate paths exist\n",
    "    if not os.path.exists(data_dir):\n",
    "        raise ValueError(f\"Data directory does not exist: {data_dir}\")\n",
    "\n",
    "    # Create save directory if it doesn't exist\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Validate data if requested\n",
    "    if validate_data and not validate_training_data(data_dir):\n",
    "        raise ValueError(\"Training data validation failed\")\n",
    "\n",
    "    # Initialize configuration\n",
    "    config = TrainingConfig()\n",
    "\n",
    "    # Initialize data manager\n",
    "    data_manager = RobustDataManager(save_dir, project_name)\n",
    "\n",
    "    # FIXED: Always try to resume if checkpoints exist\n",
    "    checkpoint_data = None\n",
    "    resume_epoch = 0\n",
    "\n",
    "    print(\"ðŸ” Checking for existing checkpoints...\")\n",
    "    checkpoints = data_manager.list_checkpoints()\n",
    "\n",
    "    if checkpoints and resume_from != \"fresh\":\n",
    "        if resume_from:\n",
    "            checkpoint_data = load_existing_checkpoint(data_manager, resume_from)\n",
    "        else:\n",
    "            # Auto-resume from best/latest checkpoint\n",
    "            checkpoint_data = load_existing_checkpoint(data_manager, None)\n",
    "\n",
    "        if checkpoint_data:\n",
    "            print(f\"ðŸ“¦ Resuming from checkpoint: {checkpoint_data['checkpoint_info']['checkpoint_name']}\")\n",
    "            resume_epoch = checkpoint_data['checkpoint_info']['epoch']\n",
    "        else:\n",
    "            print(\"âš ï¸ Checkpoint loading failed, starting fresh\")\n",
    "    else:\n",
    "        print(\"ðŸ†• Starting fresh training (no checkpoints found or fresh requested)\")\n",
    "\n",
    "    # Initialize or load components\n",
    "    if checkpoint_data:\n",
    "        model = checkpoint_data['model']\n",
    "        tokenizer = checkpoint_data['tokenizer']\n",
    "        print(\"âœ… Model and tokenizer loaded from checkpoint\")\n",
    "    else:\n",
    "        print(\"ðŸ”„ Initializing new model and tokenizer...\")\n",
    "        tokenizer = T5Tokenizer.from_pretrained(config.model_name)\n",
    "        model_config = T5Config.from_pretrained(config.model_name)\n",
    "        model = T5ForConditionalGeneration(model_config)\n",
    "        print(\"âœ… New model and tokenizer initialized\")\n",
    "\n",
    "    complexity_analyzer = EnhancedComplexityAnalyzer(tokenizer, data_manager)\n",
    "    task_creator = ImprovedTaskCreator(tokenizer, config.max_source_length, config.max_target_length)\n",
    "\n",
    "    # Create datasets\n",
    "    datasets = {}\n",
    "    file_mapping = {'train': 'babylm_train.txt', 'dev': 'babylm_dev.txt'}\n",
    "\n",
    "    for split, filename in file_mapping.items():\n",
    "        filepath = os.path.join(data_dir, filename)\n",
    "\n",
    "        if os.path.exists(filepath):\n",
    "            print(f\"ðŸ“Š Creating {split} dataset...\")\n",
    "            max_examples = config.max_train_examples if split == 'train' else config.max_val_examples\n",
    "\n",
    "            dataset = ImprovedDomainAwareDataset(\n",
    "                data_path=filepath,\n",
    "                tokenizer=tokenizer,\n",
    "                complexity_analyzer=complexity_analyzer,\n",
    "                task_creator=task_creator,\n",
    "                data_manager=data_manager,\n",
    "                max_source_length=config.max_source_length,\n",
    "                max_target_length=config.max_target_length,\n",
    "                split=split,\n",
    "                max_examples=max_examples\n",
    "            )\n",
    "\n",
    "            if len(dataset) > 0:\n",
    "                datasets[split] = dataset\n",
    "                print(f\"âœ… Created {split} dataset: {len(dataset)} examples\")\n",
    "\n",
    "    if 'train' not in datasets:\n",
    "        raise ValueError(\"No training dataset available\")\n",
    "\n",
    "    # Initialize trainer\n",
    "    print(\"ðŸ‹ï¸ Initializing trainer...\")\n",
    "    trainer = ImprovedT5Trainer(model, tokenizer, data_manager, config)\n",
    "\n",
    "    # FIXED: Restore training state if resuming\n",
    "    if checkpoint_data and checkpoint_data['training_state']:\n",
    "        training_state = checkpoint_data['training_state']\n",
    "        try:\n",
    "            print(\"ðŸ”„ Restoring optimizer state...\")\n",
    "            trainer.optimizer.load_state_dict(training_state['optimizer_state'])\n",
    "\n",
    "            # FIXED: Only restore scheduler if it was saved and we have one\n",
    "            if training_state.get('scheduler_state') and trainer.scheduler:\n",
    "                try:\n",
    "                    trainer.scheduler.load_state_dict(training_state['scheduler_state'])\n",
    "                    print(\"âœ… Scheduler state restored\")\n",
    "                except Exception as e:\n",
    "                    print(f\"âš ï¸ Failed to restore scheduler state: {e}\")\n",
    "\n",
    "            trainer.global_step = training_state.get('global_step', 0)\n",
    "            print(f\"âœ… Restored training state from epoch {training_state['epoch']}\")\n",
    "            print(f\"   Global step: {trainer.global_step}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Failed to restore training state: {e}\")\n",
    "            print(\"   Continuing with fresh optimizer state\")\n",
    "\n",
    "    # Print system info and estimates\n",
    "    monitor_system_resources()\n",
    "\n",
    "    if 'train' in datasets:\n",
    "        time_estimate = estimate_training_time(\n",
    "            len(datasets['train']), config.batch_size, config.num_epochs, config.device\n",
    "        )\n",
    "        print(f\"â° Training time estimate: {time_estimate['estimated_hours']:.1f} hours\")\n",
    "\n",
    "    # FIXED: Adjust epochs if resuming\n",
    "    if resume_epoch > 0:\n",
    "        remaining_epochs = config.num_epochs - resume_epoch\n",
    "        print(f\"ðŸ“ˆ Resuming from epoch {resume_epoch + 1}, {remaining_epochs} epochs remaining\")\n",
    "\n",
    "    # Start training\n",
    "    print(\"ðŸŽ¯ Starting training...\")\n",
    "    training_stats = trainer.train(\n",
    "        train_dataset=datasets['train'],\n",
    "        val_dataset=datasets.get('dev'),\n",
    "        resume_epoch=resume_epoch\n",
    "    )\n",
    "\n",
    "    # Generate training report\n",
    "    report = create_training_report(training_stats, config)\n",
    "\n",
    "    # Save report\n",
    "    report_path = data_manager.get_path('logs') / f'training_report_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.txt'\n",
    "    with open(report_path, 'w') as f:\n",
    "        f.write(report)\n",
    "\n",
    "    print(\"ðŸŽ‰ Training completed successfully!\")\n",
    "    print(f\"ðŸ“‹ Training report saved to: {report_path}\")\n",
    "    print(report)\n",
    "\n",
    "    return {\n",
    "        'model': model,\n",
    "        'tokenizer': tokenizer,\n",
    "        'data_manager': data_manager,\n",
    "        'training_stats': training_stats,\n",
    "        'config': config,\n",
    "        'report': report\n",
    "    }\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SIMPLE TRAINING FUNCTION (FOR FRESH START)\n",
    "# =============================================================================\n",
    "\n",
    "def train_improved_t5(data_dir: str, save_dir: str, project_name: str = \"t5_improved\"):\n",
    "    \"\"\"\n",
    "    Simple training function for fresh start without resume\n",
    "    \"\"\"\n",
    "    return train_improved_t5_with_resume(\n",
    "        data_dir=data_dir,\n",
    "        save_dir=save_dir,\n",
    "        project_name=project_name,\n",
    "        resume_from=None,\n",
    "        validate_data=True\n",
    "    )\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SETUP AND INITIALIZATION FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def setup_training_environment(base_dir: str, project_name: str = \"t5_improved\"):\n",
    "    \"\"\"Setup training environment with proper directory structure - FIXED\"\"\"\n",
    "\n",
    "    data_manager = RobustDataManager(base_dir, project_name)\n",
    "    config = TrainingConfig()\n",
    "\n",
    "    print(f\"âœ… Training environment setup complete\")\n",
    "    print(f\"ðŸ“ Project directory: {data_manager.get_path('root')}\")\n",
    "    print(f\"ðŸš€ GPU available: {torch.cuda.is_available()}\")\n",
    "\n",
    "    # FIXED: Show existing files\n",
    "    print(\"ðŸ“‹ Existing files:\")\n",
    "    for path_type, path in data_manager.structure.items():\n",
    "        if path.exists():\n",
    "            file_count = len(list(path.glob('*'))) if path.is_dir() else 0\n",
    "            print(f\"  {path_type}: {file_count} files\")\n",
    "\n",
    "    return data_manager, config\n",
    "\n",
    "def list_available_checkpoints(data_manager: RobustDataManager):\n",
    "    \"\"\"List all available checkpoints - FIXED\"\"\"\n",
    "    checkpoints = data_manager.list_checkpoints()\n",
    "\n",
    "    if not checkpoints:\n",
    "        print(\"âŒ No checkpoints found\")\n",
    "        return\n",
    "\n",
    "    print(\"ðŸ“‹ Available checkpoints:\")\n",
    "    for cp in checkpoints:\n",
    "        best_marker = \" ðŸ† (BEST)\" if cp.get('is_best', False) else \"\"\n",
    "        inferred_marker = \" âš ï¸ (INFERRED)\" if cp.get('inferred', False) else \"\"\n",
    "        loss = cp.get('metrics', {}).get('loss', 'N/A')\n",
    "        loss_str = f\"{loss:.4f}\" if isinstance(loss, (int, float)) else str(loss)\n",
    "\n",
    "        print(f\"  ðŸ“¦ {cp['checkpoint_name']}: Epoch {cp['epoch']}, Loss: {loss_str}{best_marker}{inferred_marker}\")\n",
    "\n",
    "        if cp.get('inferred'):\n",
    "            has_training_state = \"âœ…\" if cp.get('has_training_state') else \"âŒ\"\n",
    "            print(f\"      Training state: {has_training_state}\")\n",
    "\n",
    "\n",
    "def cleanup_old_checkpoints(data_manager: RobustDataManager, keep_last_n: int = 5):\n",
    "    \"\"\"Clean up old checkpoints to save space\"\"\"\n",
    "    print(f\"ðŸ§¹ Cleaning up old checkpoints (keeping last {keep_last_n})\")\n",
    "    data_manager.cleanup_old_checkpoints(keep_last_n)\n",
    "\n",
    "\n",
    "def force_fresh_start(data_manager: RobustDataManager, confirm: bool = False):\n",
    "    \"\"\"FIXED: Force a completely fresh start by clearing caches\"\"\"\n",
    "    if not confirm:\n",
    "        print(\"âš ï¸ This will delete all cached data and checkpoints!\")\n",
    "        print(\"âš ï¸ Call with confirm=True if you're sure\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # Clear preprocessed data\n",
    "        preprocessed_dir = data_manager.get_path('preprocessed')\n",
    "        if preprocessed_dir.exists():\n",
    "            shutil.rmtree(preprocessed_dir)\n",
    "            preprocessed_dir.mkdir(parents=True, exist_ok=True)\n",
    "            print(\"ðŸ—‘ï¸ Cleared preprocessed data cache\")\n",
    "\n",
    "        # Clear metadata\n",
    "        metadata_dir = data_manager.get_path('metadata')\n",
    "        if metadata_dir.exists():\n",
    "            shutil.rmtree(metadata_dir)\n",
    "            metadata_dir.mkdir(parents=True, exist_ok=True)\n",
    "            print(\"ðŸ—‘ï¸ Cleared metadata cache\")\n",
    "\n",
    "        # Clear checkpoints (optional)\n",
    "        response = input(\"â“ Also clear all checkpoints? (y/N): \")\n",
    "        if response.lower().startswith('y'):\n",
    "            checkpoints_dir = data_manager.get_path('checkpoints')\n",
    "            if checkpoints_dir.exists():\n",
    "                shutil.rmtree(checkpoints_dir)\n",
    "                checkpoints_dir.mkdir(parents=True, exist_ok=True)\n",
    "                print(\"ðŸ—‘ï¸ Cleared all checkpoints\")\n",
    "\n",
    "        print(\"âœ… Fresh start prepared\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error during cleanup: {e}\")\n",
    "\n",
    "\n",
    "def train_improved_t5(data_dir: str, save_dir: str, project_name: str = \"t5_improved\"):\n",
    "    \"\"\"\n",
    "    Simple training function for fresh start without resume - FIXED\n",
    "    \"\"\"\n",
    "    return train_improved_t5_with_resume(\n",
    "        data_dir=data_dir,\n",
    "        save_dir=save_dir,\n",
    "        project_name=project_name,\n",
    "        resume_from=\"fresh\",  # FIXED: Use \"fresh\" to force fresh start\n",
    "        validate_data=True\n",
    "    )\n",
    "\n",
    "\n",
    "def resume_training(data_dir: str, save_dir: str, project_name: str = \"t5_improved\",\n",
    "                   checkpoint_name: str = None):\n",
    "    \"\"\"\n",
    "    FIXED: Dedicated function for resuming training\n",
    "    \"\"\"\n",
    "    print(\"ðŸ”„ Resume Training Mode\")\n",
    "\n",
    "    # Setup data manager to check for existing work\n",
    "    data_manager = RobustDataManager(save_dir, project_name)\n",
    "\n",
    "    # List available checkpoints\n",
    "    print(\"ðŸ” Scanning for checkpoints...\")\n",
    "    list_available_checkpoints(data_manager)\n",
    "\n",
    "    if checkpoint_name:\n",
    "        print(f\"ðŸŽ¯ Targeting specific checkpoint: {checkpoint_name}\")\n",
    "    else:\n",
    "        print(\"ðŸŽ¯ Will auto-select best/latest checkpoint\")\n",
    "\n",
    "    return train_improved_t5_with_resume(\n",
    "        data_dir=data_dir,\n",
    "        save_dir=save_dir,\n",
    "        project_name=project_name,\n",
    "        resume_from=checkpoint_name,\n",
    "        validate_data=True\n",
    "    )\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# EXAMPLE USAGE AND MAIN EXECUTION\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ðŸš€ T5 DOMAIN-AWARE TRAINING SYSTEM - FIXED VERSION\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # FIXED: Configuration\n",
    "    DATA_DIR = '/kaggle/working/baseline-pretraining/babylm_data'\n",
    "    SAVE_DIR = '/kaggle/working'\n",
    "    PROJECT_NAME = 't5_domain_aware_v3'\n",
    "\n",
    "    # Example 1: Setup and check environment\n",
    "    print(\"\\n=== ðŸ”§ ENVIRONMENT SETUP ===\")\n",
    "    try:\n",
    "        data_manager, config = setup_training_environment(SAVE_DIR, PROJECT_NAME)\n",
    "        list_available_checkpoints(data_manager)\n",
    "        print(\"âœ… Environment setup completed!\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Environment setup failed: {e}\")\n",
    "\n",
    "    # Example 2: Resume training (recommended)\n",
    "    print(\"\\n=== ðŸ”„ RESUME TRAINING ===\")\n",
    "    try:\n",
    "        results = resume_training(\n",
    "            data_dir=DATA_DIR,\n",
    "            save_dir=SAVE_DIR,\n",
    "            project_name=PROJECT_NAME,\n",
    "            checkpoint_name=None  # Auto-select best checkpoint\n",
    "        )\n",
    "        print(\"âœ… Training completed successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Resume training failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "    # Example 3: Fresh training (if needed)\n",
    "    print(\"\\n=== ðŸ†• FRESH TRAINING (if resume failed) ===\")\n",
    "    try:\n",
    "        # Uncomment the following lines if you want to force fresh start\n",
    "        # force_fresh_start(data_manager, confirm=True)\n",
    "        # results = train_improved_t5(DATA_DIR, SAVE_DIR, PROJECT_NAME)\n",
    "        # print(\"âœ… Fresh training completed successfully!\")\n",
    "        print(\"âš ï¸ Fresh training commented out - uncomment if needed\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Fresh training failed: {e}\")\n",
    "\n",
    "    # Example 4: Cleanup (optional)\n",
    "    print(\"\\n=== ðŸ§¹ CLEANUP (optional) ===\")\n",
    "    try:\n",
    "        # cleanup_old_checkpoints(data_manager, keep_last_n=3)\n",
    "        print(\"âš ï¸ Cleanup commented out - uncomment if needed\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Cleanup failed: {e}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ðŸŽ‰ SCRIPT EXECUTION COMPLETED\")\n",
    "    print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
