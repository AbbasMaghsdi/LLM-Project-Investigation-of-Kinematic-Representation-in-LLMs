{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "!pip uninstall -y numpy transformers datasets\n",
    "!pip install numpy --force-reinstall --no-cache-dir\n",
    "!pip install transformers datasets --force-reinstall --no-cache-dir\n",
    "\n",
    "\n",
    "os.kill(os.getpid(), 9)  # Restart the Colab runtime (REQUIRED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/babylm/baseline-pretraining.git\n",
    "%cd baseline-pretraining\n",
    "!wget -O babylm_data.zip \"https://files.osf.io/v1/resources/ad7qg/providers/osfstorage/661517db943bee3731dfec25/?zip=\"\n",
    "!unzip babylm_data.zip -d babylm_data\n",
    "!unzip babylm_data/train_10M.zip -d babylm_data/train_10M\n",
    "!unzip babylm_data/dev.zip -d babylm_data/dev\n",
    "!unzip babylm_data/test.zip -d babylm_data/test\n",
    "!cat babylm_data/train_10M/train_10M/*.train > babylm_data/babylm_train.txt\n",
    "!cat babylm_data/dev/dev/*.dev > babylm_data/babylm_dev.txt\n",
    "!cat babylm_data/test/test/*.test > babylm_data/babylm_test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning T5small on babylm 10M dataset with random masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments,\n",
    "    DataCollatorForSeq2Seq, get_cosine_schedule_with_warmup\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from typing import List, Dict\n",
    "from datasets import DatasetDict, Dataset\n",
    "\n",
    "# =====================\n",
    "# Settings & Constants\n",
    "# =====================\n",
    "#os.environ['BABYLM_ROOT_DIR'] = '/content/baseline-pretraining/babylm_data'\n",
    "from pathlib import Path\n",
    "\n",
    "BABYLM_ROOT_DIR = Path(\"/content/baseline-pretraining/babylm_data\")\n",
    "assert BABYLM_ROOT_DIR.exists(), \"Dataset directory does not exist!\"\n",
    "\n",
    "os.environ['WANDB_DISABLED'] = 'true'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "\n",
    "# ================\n",
    "# Load tokenizer\n",
    "# ================\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "EXTRA_IDS = 10\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': [f\"<extra_id_{i}>\" for i in range(EXTRA_IDS)]})\n",
    "\n",
    "# ==================\n",
    "# Load model\n",
    "# ==================\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.to(device)\n",
    "\n",
    "# ==================\n",
    "# Span Corruption Function\n",
    "# ==================\n",
    "def random_spans_noise_mask(length, noise_density=0.15, mean_span_length=3):\n",
    "    num_noise_tokens = int(length * noise_density)\n",
    "    num_spans = max(1, num_noise_tokens // mean_span_length)\n",
    "    span_lengths = np.random.poisson(mean_span_length, num_spans)\n",
    "    span_lengths = np.clip(span_lengths, 1, length)\n",
    "    span_starts = sorted(random.sample(range(length), num_spans))\n",
    "    mask = np.zeros(length, dtype=bool)\n",
    "    for start, span_len in zip(span_starts, span_lengths):\n",
    "        mask[start:start+span_len] = True\n",
    "    return mask\n",
    "\n",
    "def t5_denoising_example(example):\n",
    "    text = example[\"text\"]\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    if len(tokens) < 2:\n",
    "        return None  # This cleanly skips the example\n",
    "\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    mask = random_spans_noise_mask(len(token_ids))\n",
    "\n",
    "    input_tokens = []\n",
    "    target_tokens = []\n",
    "    current_extra_id = 0\n",
    "    i = 0\n",
    "    while i < len(token_ids):\n",
    "        if mask[i]:\n",
    "            input_tokens.append(tokenizer.convert_tokens_to_ids(f\"<extra_id_{current_extra_id}>\"))\n",
    "            span = []\n",
    "            while i < len(token_ids) and mask[i]:\n",
    "                span.append(token_ids[i])\n",
    "                i += 1\n",
    "            target_tokens.extend([tokenizer.convert_tokens_to_ids(f\"<extra_id_{current_extra_id}>\")] + span)\n",
    "            current_extra_id += 1\n",
    "        else:\n",
    "            input_tokens.append(token_ids[i])\n",
    "            i += 1\n",
    "\n",
    "    input_ids = tokenizer.prepare_for_model(input_tokens, max_length=64, padding=\"max_length\", truncation=True)[\"input_ids\"]\n",
    "    labels = tokenizer.prepare_for_model(target_tokens, max_length=64, padding=\"max_length\", truncation=True)[\"input_ids\"]\n",
    "    return {\"input_ids\": input_ids, \"labels\": labels}\n",
    "\n",
    "\n",
    "\n",
    "# ================\n",
    "# Load Dataset\n",
    "# ================\n",
    "\n",
    "def load_text_file_as_dataset(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "    lines = [line.strip() for line in lines if line.strip()]  # Remove empty lines\n",
    "    return Dataset.from_list([{\"text\": line} for line in lines])\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": load_text_file_as_dataset(BABYLM_ROOT_DIR / \"babylm_train.txt\"),\n",
    "    \"validation\": load_text_file_as_dataset(BABYLM_ROOT_DIR / \"babylm_dev.txt\"),\n",
    "    \"test\": load_text_file_as_dataset(BABYLM_ROOT_DIR / \"babylm_test.txt\"),\n",
    "})\n",
    "\n",
    "\n",
    "\n",
    "# Remove empty lines\n",
    "dataset = dataset.filter(lambda x: x['text'] and x['text'].strip())\n",
    "\n",
    "# Sort + limit validation/test\n",
    "dataset['train'] = dataset['train'].map(lambda x: {\"length\": len(x['text'].split())}).sort('length').remove_columns(['length'])\n",
    "dataset['validation'] = dataset['validation'].select(range(min(5000, len(dataset['validation']))))\n",
    "dataset['test'] = dataset['test'].select(range(min(5000, len(dataset['test']))))\n",
    "\n",
    "def is_valid(example):\n",
    "    return len(tokenizer.tokenize(example[\"text\"])) >= 5\n",
    "\n",
    "dataset[\"train\"] = dataset[\"train\"].filter(is_valid)\n",
    "dataset[\"validation\"] = dataset[\"validation\"].filter(is_valid)\n",
    "dataset[\"test\"] = dataset[\"test\"].filter(is_valid)\n",
    "\n",
    "# Apply T5-style denoising tokenization\n",
    "#dataset = dataset.map(t5_denoising_example, remove_columns=['text'])\n",
    "dataset = dataset.map(\n",
    "    t5_denoising_example,\n",
    "    remove_columns=['text'],\n",
    "    batched=False,\n",
    "    desc=\"Applying span corruption\"\n",
    ")\n",
    "\n",
    "dataset.set_format(type='torch')\n",
    "\n",
    "# ==========================\n",
    "# Data Collator\n",
    "# ==========================\n",
    "collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, label_pad_token_id=-100)\n",
    "\n",
    "# ==========================\n",
    "# TrainingArguments with label smoothing, weight decay, and scheduler\n",
    "# ==========================\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"/content/drive/MyDrive/llm-project/t5-small-babylm-denoised\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=3e-4,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=500,\n",
    "    logging_dir=str(BABYLM_ROOT_DIR / \"logs\"),\n",
    "    logging_steps=500,\n",
    "    eval_steps=4000,\n",
    "    save_steps=4000,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=[],\n",
    "    label_smoothing_factor=0.1,\n",
    "    remove_unused_columns=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    data_collator=collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate\n",
    "test_results = trainer.evaluate(dataset[\"test\"], metric_key_prefix=\"test\")\n",
    "print(\"Test Results:\", test_results)\n",
    "\n",
    "# Save\n",
    "trainer.save_model()\n",
    "tokenizer.save_pretrained(args.output_dir)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
