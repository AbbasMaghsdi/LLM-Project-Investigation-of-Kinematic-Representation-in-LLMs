{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "!pip uninstall -y numpy transformers datasets\n",
    "!pip install numpy --force-reinstall --no-cache-dir\n",
    "!pip install transformers datasets --force-reinstall --no-cache-dir\n",
    "\n",
    "\n",
    "os.kill(os.getpid(), 9)  # Restart the Colab runtime (REQUIRED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/babylm/baseline-pretraining.git\n",
    "%cd baseline-pretraining\n",
    "!wget -O babylm_data.zip \"https://files.osf.io/v1/resources/ad7qg/providers/osfstorage/661517db943bee3731dfec25/?zip=\"\n",
    "!unzip babylm_data.zip -d babylm_data\n",
    "!unzip babylm_data/train_10M.zip -d babylm_data/train_10M\n",
    "!unzip babylm_data/dev.zip -d babylm_data/dev\n",
    "!unzip babylm_data/test.zip -d babylm_data/test\n",
    "!cat babylm_data/train_10M/train_10M/*.train > babylm_data/babylm_train.txt\n",
    "!cat babylm_data/dev/dev/*.dev > babylm_data/babylm_dev.txt\n",
    "!cat babylm_data/test/test/*.test > babylm_data/babylm_test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training t5-small from scratch with random weights with 0.02 0.02 0.4 delta of embedding of first layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Memory-optimized training script for a custom T5 model with delta-embeddings FROM SCRATCH.\n",
    "This version includes:\n",
    "- Training T5-small from scratch (random initialization)\n",
    "- Fixed out-of-memory issues during evaluation\n",
    "- Gradient accumulation for effective larger batch sizes\n",
    "- Memory-efficient evaluation strategy\n",
    "- Fixed out-of-vocab token ID issues\n",
    "- Simple evaluation during training (loss only)\n",
    "- Preserved delta-embedding logic (subtraction and addition of embeddings)\n",
    "\"\"\"\n",
    "import os\n",
    "import random\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Dict, Optional\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import nltk\n",
    "import evaluate\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    T5Tokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from transformers.modeling_outputs import Seq2SeqLMOutput\n",
    "from transformers import T5Config\n",
    "from inspect import signature\n",
    "\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "# Download NLTK data with error handling\n",
    "try:\n",
    "    nltk.data.find(\"tokenizers/punkt\")\n",
    "    logger.info(\"‚úÖ NLTK punkt tokenizer already downloaded\")\n",
    "except (LookupError, OSError):\n",
    "    try:\n",
    "        nltk.download(\"punkt\", quiet=False)\n",
    "        logger.info(\"‚úÖ NLTK punkt tokenizer downloaded successfully\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Failed to download NLTK punkt: {e}\")\n",
    "        logger.info(\"Continuing without sentence tokenization...\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info(f\"Using device: {device}\")\n",
    "# ---\n",
    "# Step 1: Setup, Configuration, and Seeding\n",
    "# ---\n",
    "\n",
    "# --- Memory-Optimized Hyperparameters ---\n",
    "MODEL_NAME = \"t5-small\"\n",
    "DELTA_WEIGHT = 0.2\n",
    "#DELTA_WEIGHT = 0.02\n",
    "#DELTA_WEIGHT = 0.4\n",
    "MAX_INPUT_LENGTH = 256\n",
    "MAX_TARGET_LENGTH = 128\n",
    "NOISE_DENSITY = 0.15\n",
    "MEAN_SPAN_LENGTH = 3\n",
    "\n",
    "# ‚úÖ MEMORY FIX: Even smaller batch sizes with gradient accumulation\n",
    "TRAIN_BATCH_SIZE = 32  # Reduced further\n",
    "EVAL_BATCH_SIZE = 32   # Reduced further\n",
    "GRADIENT_ACCUMULATION_STEPS = 4  # Effective batch size = 4 * 4 = 16\n",
    "\n",
    "LEARNING_RATE = 5e-4  # Slightly higher learning rate for from-scratch training\n",
    "NUM_EPOCHS = 5  # More epochs needed for from-scratch training\n",
    "\n",
    "BASE_PROJECT_DIR = Path(\"/content/drive/MyDrive/llm-project\")\n",
    "PROCESSED_DATASET_PATH = BASE_PROJECT_DIR / \"processed_dataset\"\n",
    "OUTPUT_DIR = str(BASE_PROJECT_DIR / \"t5-small-first-0.4delta-embedding-babylm-from-scratch0\")\n",
    "LOGGING_DIR = str(BASE_PROJECT_DIR / \"t5_logs_from_scratch0\")\n",
    "BABYLM_ROOT_DIR = Path(\"/content/baseline-pretraining/babylm_data\")\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# ‚úÖ MEMORY FIX: Clear GPU memory at start\n",
    "def clear_gpu_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "set_seed(42)\n",
    "clear_gpu_memory()\n",
    "\n",
    "try:\n",
    "    nltk.data.find(\"tokenizers/punkt\")\n",
    "except (LookupError, OSError):\n",
    "    nltk.download(\"punkt\", quiet=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    logger.info(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    logger.info(f\"Available Memory: {torch.cuda.memory_reserved(0) / 1e9:.1f} GB\")\n",
    "\n",
    "# ---\n",
    "# Step 2: Custom Model with Delta-Embedding Logic (PRESERVED)\n",
    "# ---\n",
    "class T5ForDeltaEmbeddings(T5ForConditionalGeneration):\n",
    "    def __init__(self, config, delta_weight: float = 0.1):\n",
    "        super().__init__(config)\n",
    "        self.delta_weight = delta_weight\n",
    "        if not hasattr(self, 'has_logged_init'):\n",
    "            logger.info(f\"Initialized T5ForDeltaEmbeddings with delta_weight = {self.delta_weight}\")\n",
    "            self.has_logged_init = True\n",
    "\n",
    "    def _apply_delta_to_embeddings(self, embeddings: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        PRESERVED DELTA-EMBEDDING LOGIC:\n",
    "        - Compute delta as difference between current and previous embeddings\n",
    "        - Add delta scaled by delta_weight to original embeddings\n",
    "        \"\"\"\n",
    "        if self.delta_weight == 0.0:\n",
    "            return embeddings\n",
    "\n",
    "        # ‚úÖ MEMORY FIX: Use in-place operations where possible\n",
    "        previous_embeddings = torch.roll(embeddings, shifts=1, dims=1)\n",
    "        previous_embeddings[:, 0, :] = 0\n",
    "\n",
    "        # Core delta-embedding logic: subtraction and addition\n",
    "        delta = embeddings - previous_embeddings  # SUBTRACTION\n",
    "        embeddings = embeddings.add_(delta, alpha=self.delta_weight)  # ADDITION\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        decoder_input_ids=None,\n",
    "        decoder_attention_mask=None,\n",
    "        head_mask=None,\n",
    "        decoder_head_mask=None,\n",
    "        cross_attn_head_mask=None,\n",
    "        encoder_outputs=None,\n",
    "        past_key_values=None,\n",
    "        inputs_embeds=None,\n",
    "        decoder_inputs_embeds=None,\n",
    "        labels=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "        **kwargs,\n",
    "    ) -> Seq2SeqLMOutput:\n",
    "\n",
    "        # Handle Encoder inputs\n",
    "        if encoder_outputs is None:\n",
    "            if input_ids is not None and inputs_embeds is None:\n",
    "                inputs_embeds = self.shared(input_ids)\n",
    "\n",
    "            if inputs_embeds is not None:\n",
    "                inputs_embeds = self._apply_delta_to_embeddings(inputs_embeds)\n",
    "                input_ids = None\n",
    "\n",
    "        # Handle Decoder inputs\n",
    "        if labels is not None and decoder_input_ids is None and decoder_inputs_embeds is None:\n",
    "            decoder_input_ids = self._shift_right(labels)\n",
    "\n",
    "        if decoder_input_ids is not None and decoder_inputs_embeds is None:\n",
    "            decoder_inputs_embeds = self.shared(decoder_input_ids)\n",
    "\n",
    "        if decoder_inputs_embeds is not None:\n",
    "            decoder_inputs_embeds = self._apply_delta_to_embeddings(decoder_inputs_embeds)\n",
    "            decoder_input_ids = None\n",
    "\n",
    "        forward_args = signature(super().forward).parameters\n",
    "        filtered_kwargs = {k: v for k, v in kwargs.items() if k in forward_args}\n",
    "\n",
    "        return super().forward(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            decoder_attention_mask=decoder_attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            decoder_head_mask=decoder_head_mask,\n",
    "            cross_attn_head_mask=cross_attn_head_mask,\n",
    "            encoder_outputs=encoder_outputs,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            decoder_inputs_embeds=decoder_inputs_embeds,\n",
    "            labels=labels,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            **filtered_kwargs,\n",
    "        )\n",
    "\n",
    "# ---\n",
    "# Step 3: Load Tokenizer and Initialize Model FROM SCRATCH\n",
    "# ---\n",
    "logger.info(\"Loading tokenizer...\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "logger.info(\"Initializing custom model FROM SCRATCH...\")\n",
    "config = T5Config.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# ‚úÖ FROM SCRATCH: Initialize model with random weights (no pretrained loading)\n",
    "with torch.cuda.device(device):\n",
    "    logger.info(\"üöÄ Creating T5 model with RANDOM INITIALIZATION (from scratch)\")\n",
    "    model = T5ForDeltaEmbeddings(config, delta_weight=DELTA_WEIGHT)\n",
    "\n",
    "    # ‚úÖ CRITICAL: No pretrained weight loading - model starts with random weights\n",
    "    logger.info(\"‚úÖ Model initialized from scratch with random weights\")\n",
    "\n",
    "# ‚úÖ FIX: Ensure model vocabulary matches tokenizer\n",
    "original_vocab_size = model.config.vocab_size\n",
    "tokenizer_vocab_size = len(tokenizer)\n",
    "\n",
    "if original_vocab_size != tokenizer_vocab_size:\n",
    "    logger.info(f\"Resizing model embeddings from {original_vocab_size} to {tokenizer_vocab_size}\")\n",
    "    model.resize_token_embeddings(tokenizer_vocab_size)\n",
    "\n",
    "model.config.vocab_size = tokenizer_vocab_size\n",
    "model.to(device)\n",
    "\n",
    "# ‚úÖ MEMORY FIX: Enable gradient checkpointing to save memory\n",
    "model.gradient_checkpointing_enable()\n",
    "logger.info(\"‚úÖ Gradient checkpointing enabled to save memory\")\n",
    "\n",
    "# Log model parameters for verification\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "logger.info(f\"üìä Model Statistics:\")\n",
    "logger.info(f\"  Total parameters: {total_params:,}\")\n",
    "logger.info(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "logger.info(f\"  Model size: ~{total_params * 4 / 1e6:.1f} MB (fp32)\")\n",
    "\n",
    "# ---\n",
    "# Step 4: Prepare BabyLM Dataset (Memory Optimized)\n",
    "# ---\n",
    "def corrupt_text_for_t5(examples: Dict) -> Dict:\n",
    "    texts = examples[\"text\"]\n",
    "    inputs, targets = [], []\n",
    "    sentinel_start_id = tokenizer.convert_tokens_to_ids(\"<extra_id_0>\")\n",
    "\n",
    "    for text in texts:\n",
    "        # ‚úÖ MEMORY FIX: Reduce max length for preprocessing\n",
    "        tokens = tokenizer(text, add_special_tokens=False, return_tensors=\"np\", max_length=256, truncation=True)[\"input_ids\"][0]\n",
    "\n",
    "        if len(tokens) < 2:\n",
    "            continue\n",
    "\n",
    "        num_tokens_to_mask = int(len(tokens) * NOISE_DENSITY)\n",
    "        num_spans = int(num_tokens_to_mask / MEAN_SPAN_LENGTH)\n",
    "        if num_spans == 0:\n",
    "            continue\n",
    "\n",
    "        span_starts = np.random.choice(np.arange(len(tokens)), size=num_spans, replace=False)\n",
    "        span_lengths = np.random.poisson(lam=MEAN_SPAN_LENGTH, size=num_spans)\n",
    "        span_lengths = np.maximum(1, span_lengths)\n",
    "\n",
    "        mask = np.zeros_like(tokens, dtype=bool)\n",
    "        for start, length in zip(span_starts, span_lengths):\n",
    "            mask[start : start + length] = True\n",
    "\n",
    "        input_ids_list, label_ids, sentinel_id = [], [], sentinel_start_id\n",
    "        i = 0\n",
    "        while i < len(tokens):\n",
    "            if not mask[i]:\n",
    "                input_ids_list.append(tokens[i])\n",
    "                i += 1\n",
    "            else:\n",
    "                input_ids_list.append(sentinel_id)\n",
    "                label_ids.append(sentinel_id)\n",
    "                sentinel_id -= 1\n",
    "                while i < len(tokens) and mask[i]:\n",
    "                    label_ids.append(tokens[i])\n",
    "                    i += 1\n",
    "\n",
    "        inputs.append(tokenizer.decode(input_ids_list))\n",
    "        targets.append(tokenizer.decode(label_ids))\n",
    "\n",
    "    model_inputs = tokenizer(inputs, max_length=MAX_INPUT_LENGTH, padding=\"max_length\", truncation=True)\n",
    "    labels = tokenizer(text_target=targets, max_length=MAX_TARGET_LENGTH, padding=\"max_length\", truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Load or process dataset\n",
    "if PROCESSED_DATASET_PATH.exists():\n",
    "    logger.info(f\"‚úÖ Loading processed dataset from disk: {PROCESSED_DATASET_PATH}\")\n",
    "    tokenized_dataset = DatasetDict.load_from_disk(str(PROCESSED_DATASET_PATH))\n",
    "else:\n",
    "    logger.info(\"Processed dataset not found. Starting preprocessing...\")\n",
    "    try:\n",
    "        raw_dataset = DatasetDict({\n",
    "             \"train\": load_dataset(\"text\", data_files=str(BABYLM_ROOT_DIR / \"babylm_train.txt\"))[\"train\"],\n",
    "             \"validation\": load_dataset(\"text\", data_files=str(BABYLM_ROOT_DIR / \"babylm_dev.txt\"))[\"train\"],\n",
    "        })\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load dataset: {e}. Ensure babylm_data is in the correct path.\")\n",
    "        exit()\n",
    "\n",
    "    tokenized_dataset = raw_dataset.map(\n",
    "        corrupt_text_for_t5, batched=True, remove_columns=[\"text\"],\n",
    "        num_proc=os.cpu_count() // 2, desc=\"Running T5 Corruptor\"\n",
    "    )\n",
    "    logger.info(f\"üíæ Saving processed dataset to disk at: {PROCESSED_DATASET_PATH}\")\n",
    "    tokenized_dataset.save_to_disk(str(PROCESSED_DATASET_PATH))\n",
    "\n",
    "logger.info(\"Preparing train and eval splits...\")\n",
    "train_dataset = tokenized_dataset[\"train\"].shuffle(seed=42)\n",
    "\n",
    "# ‚úÖ MEMORY FIX: Use much smaller eval dataset during training\n",
    "eval_dataset_full = tokenized_dataset[\"validation\"].shuffle(seed=42)\n",
    "eval_dataset = eval_dataset_full.select(range(min(100, len(eval_dataset_full))))  # Only 100 examples for training evals\n",
    "logger.info(f\"Using {len(eval_dataset)} examples for training evaluation (memory optimization)\")\n",
    "\n",
    "# ---\n",
    "# Step 5: Simple Metrics for Training (Loss Only)\n",
    "# ---\n",
    "def simple_compute_metrics(eval_pred):\n",
    "    \"\"\"Simple metrics computation that only returns loss-based metrics during training\"\"\"\n",
    "    return {}\n",
    "\n",
    "# ---\n",
    "# Step 6: Robust ROUGE Computation for Final Evaluation\n",
    "# ---\n",
    "def compute_rouge_metrics(predictions, labels, tokenizer):\n",
    "    \"\"\"Robust ROUGE computation with proper error handling\"\"\"\n",
    "    try:\n",
    "        vocab_size = tokenizer.vocab_size\n",
    "        predictions = np.clip(predictions, 0, vocab_size - 1)\n",
    "\n",
    "        max_pred_id = np.max(predictions) if len(predictions) > 0 else 0\n",
    "        if max_pred_id >= vocab_size:\n",
    "            logger.warning(f\"‚ö†Ô∏è Still found out-of-vocab token ID after clipping! Max ID: {max_pred_id}, Vocab size: {vocab_size}\")\n",
    "            predictions = np.where(predictions >= vocab_size, tokenizer.unk_token_id or tokenizer.pad_token_id, predictions)\n",
    "\n",
    "        try:\n",
    "            decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error decoding predictions: {e}\")\n",
    "            decoded_preds = []\n",
    "            for pred in predictions:\n",
    "                try:\n",
    "                    decoded_preds.append(tokenizer.decode(pred, skip_special_tokens=True))\n",
    "                except:\n",
    "                    decoded_preds.append(\"\")\n",
    "\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "        try:\n",
    "            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error decoding labels: {e}\")\n",
    "            decoded_labels = [\"\"] * len(labels)\n",
    "\n",
    "        decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) if pred.strip() else \"empty\" for pred in decoded_preds]\n",
    "        decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) if label.strip() else \"empty\" for label in decoded_labels]\n",
    "\n",
    "        rouge_metric = evaluate.load(\"rouge\")\n",
    "        result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "        return {key: value * 100 for key, value in result.items()}\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error computing ROUGE metrics: {e}\")\n",
    "        return {\"rouge1\": 0.0, \"rouge2\": 0.0, \"rougeL\": 0.0}\n",
    "\n",
    "# ---\n",
    "# Step 7: Memory-Optimized Training Setup\n",
    "# ---\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer, model=model, label_pad_token_id=-100,\n",
    "    pad_to_multiple_of=8 if device.type == \"cuda\" else None\n",
    ")\n",
    "\n",
    "# ‚úÖ MEMORY OPTIMIZATION: Aggressive memory saving settings\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=EVAL_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,  # Effective batch size = 16\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    weight_decay=0.01,\n",
    "\n",
    "    # ‚úÖ MEMORY FIX: Disable generation during training\n",
    "    predict_with_generate=False,\n",
    "    generation_max_length=MAX_TARGET_LENGTH,\n",
    "    generation_num_beams=1,\n",
    "\n",
    "    # ‚úÖ MEMORY FIX: Optimize logging and evaluation frequency\n",
    "    logging_dir=LOGGING_DIR,\n",
    "    report_to=\"tensorboard\",\n",
    "    fp16=True if device.type == \"cuda\" else False,\n",
    "    dataloader_pin_memory=False,\n",
    "\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=200,  # Less frequent logging\n",
    "\n",
    "    # ‚úÖ CRITICAL MEMORY FIX: Much less frequent evaluation\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=1000,  # Evaluate much less frequently\n",
    "\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2,  # Keep only 2 checkpoints\n",
    "\n",
    "    load_best_model_at_end=False,  # ‚úÖ MEMORY FIX: Disable to save memory\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "\n",
    "    # ‚úÖ MEMORY FIX: Additional memory optimizations\n",
    "    remove_unused_columns=True,  # Remove unused columns to save memory\n",
    "    group_by_length=False,  # Disable grouping to prevent memory spikes\n",
    "    length_column_name=None,\n",
    "\n",
    "    # ‚úÖ MEMORY FIX: Optimize evaluation\n",
    "    eval_accumulation_steps=1,  # Process eval in smaller chunks\n",
    "\n",
    "    # ‚úÖ MEMORY FIX: Disable some features that use extra memory\n",
    "    include_inputs_for_metrics=False,\n",
    "\n",
    "    # ‚úÖ FROM SCRATCH: Learning rate scheduler for better convergence\n",
    "    warmup_steps=500,\n",
    "    lr_scheduler_type=\"linear\",\n",
    ")\n",
    "\n",
    "# ‚úÖ MEMORY FIX: Use processing_class instead of deprecated tokenizer parameter\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    processing_class=tokenizer,  # Use processing_class instead of tokenizer\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=simple_compute_metrics,\n",
    ")\n",
    "\n",
    "# ---\n",
    "# Step 8: Memory-Optimized Training FROM SCRATCH\n",
    "# ---\n",
    "# ‚úÖ MEMORY FIX: Clear memory before training\n",
    "clear_gpu_memory()\n",
    "\n",
    "logger.info(\"üöÄ Starting training FROM SCRATCH with delta-embedding T5 model...\")\n",
    "logger.info(f\"üìä Training Configuration:\")\n",
    "logger.info(f\"  Delta weight: {DELTA_WEIGHT}\")\n",
    "logger.info(f\"  Effective batch size: {TRAIN_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\n",
    "logger.info(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "logger.info(f\"  Number of epochs: {NUM_EPOCHS}\")\n",
    "logger.info(f\"  Model initialized: FROM SCRATCH (random weights)\")\n",
    "\n",
    "# Check for checkpoints\n",
    "last_checkpoint = get_last_checkpoint(training_args.output_dir)\n",
    "if last_checkpoint:\n",
    "    logger.info(f\"‚úÖ Checkpoint found at {last_checkpoint}. Resuming training.\")\n",
    "    trainer.train(resume_from_checkpoint=last_checkpoint)\n",
    "else:\n",
    "    logger.info(\"‚ÑπÔ∏è No checkpoint found. Starting training from scratch.\")\n",
    "    try:\n",
    "        trainer.train()\n",
    "    except torch.cuda.OutOfMemoryError as e:\n",
    "        logger.error(f\"CUDA out of memory during training: {e}\")\n",
    "        logger.info(\"Try reducing TRAIN_BATCH_SIZE to 2 or 1, or increasing gradient_accumulation_steps\")\n",
    "        raise\n",
    "\n",
    "# ‚úÖ MEMORY FIX: Clear memory after training\n",
    "clear_gpu_memory()\n",
    "\n",
    "# ---\n",
    "# Step 9: Memory-Efficient Final Evaluation\n",
    "# ---\n",
    "logger.info(\"Training complete. Running memory-efficient final evaluation...\")\n",
    "\n",
    "# ‚úÖ MEMORY FIX: Use very small dataset for final evaluation\n",
    "final_eval_dataset = eval_dataset_full.select(range(min(50, len(eval_dataset_full))))  # Only 50 examples\n",
    "logger.info(f\"Using {len(final_eval_dataset)} examples for final evaluation\")\n",
    "\n",
    "# ‚úÖ MEMORY FIX: Enable generation with very conservative settings\n",
    "trainer.args.predict_with_generate = True\n",
    "trainer.args.generation_max_length = MAX_TARGET_LENGTH\n",
    "trainer.args.generation_num_beams = 1\n",
    "trainer.args.per_device_eval_batch_size = 1  # Reduce to batch size 1 for generation\n",
    "\n",
    "try:\n",
    "    logger.info(\"Generating predictions for ROUGE computation...\")\n",
    "\n",
    "    # ‚úÖ MEMORY FIX: Process evaluation in smaller chunks\n",
    "    predictions = trainer.predict(final_eval_dataset)\n",
    "\n",
    "    logger.info(\"Computing ROUGE metrics...\")\n",
    "    rouge_results = compute_rouge_metrics(\n",
    "        predictions.predictions,\n",
    "        predictions.label_ids,\n",
    "        tokenizer\n",
    "    )\n",
    "\n",
    "    logger.info(f\"Final ROUGE Results: {rouge_results}\")\n",
    "\n",
    "    # Simple evaluation on the small dataset\n",
    "    eval_results = trainer.evaluate(eval_dataset=final_eval_dataset)\n",
    "    logger.info(f\"Final Evaluation Results: {eval_results}\")\n",
    "\n",
    "    final_results = {**eval_results, **rouge_results}\n",
    "    logger.info(f\"Combined Final Results: {final_results}\")\n",
    "\n",
    "except torch.cuda.OutOfMemoryError as e:\n",
    "    logger.error(f\"CUDA out of memory during final evaluation: {e}\")\n",
    "    logger.info(\"Skipping ROUGE computation due to memory constraints.\")\n",
    "    try:\n",
    "        # Try even simpler evaluation\n",
    "        trainer.args.predict_with_generate = False\n",
    "        eval_results = trainer.evaluate(eval_dataset=final_eval_dataset.select(range(20)))\n",
    "        logger.info(f\"Minimal Evaluation Results: {eval_results}\")\n",
    "    except Exception as e2:\n",
    "        logger.error(f\"Even minimal evaluation failed: {e2}\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error during final evaluation: {e}\")\n",
    "    logger.info(\"Falling back to minimal evaluation...\")\n",
    "    try:\n",
    "        trainer.args.predict_with_generate = False\n",
    "        eval_results = trainer.evaluate(eval_dataset=final_eval_dataset.select(range(20)))\n",
    "        logger.info(f\"Fallback Evaluation Results: {eval_results}\")\n",
    "    except Exception as e2:\n",
    "        logger.error(f\"Fallback evaluation also failed: {e2}\")\n",
    "\n",
    "# ---\n",
    "# Step 10: Save Final Model\n",
    "# ---\n",
    "logger.info(f\"üíæ Saving final model to: {OUTPUT_DIR}\")\n",
    "try:\n",
    "    trainer.save_model()\n",
    "    tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "    # Save training info\n",
    "    training_info = {\n",
    "        \"model_type\": \"T5ForDeltaEmbeddings\",\n",
    "        \"delta_weight\": DELTA_WEIGHT,\n",
    "        \"trained_from_scratch\": True,\n",
    "        \"total_parameters\": total_params,\n",
    "        \"trainable_parameters\": trainable_params,\n",
    "        \"final_training_loss\": trainer.state.log_history[-1].get('train_loss', 'N/A') if trainer.state.log_history else 'N/A'\n",
    "    }\n",
    "\n",
    "    import json\n",
    "    with open(os.path.join(OUTPUT_DIR, 'training_info.json'), 'w') as f:\n",
    "        json.dump(training_info, f, indent=2)\n",
    "\n",
    "    logger.info(\"‚úÖ Model and training info saved successfully!\")\n",
    "    logger.info(f\"üìÑ Training info: {training_info}\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error saving model: {e}\")\n",
    "\n",
    "# ‚úÖ MEMORY FIX: Final cleanup\n",
    "clear_gpu_memory()\n",
    "logger.info(\"‚úÖ FROM SCRATCH training and evaluation complete!\")\n",
    "\n",
    "# ---\n",
    "# Step 11: Memory-Efficient Generation Test\n",
    "# ---\n",
    "def test_generation(model, tokenizer, test_text: str, max_length: int = 64):  # Reduced max length\n",
    "    \"\"\"Memory-efficient generation test\"\"\"\n",
    "    try:\n",
    "        model.eval()\n",
    "        inputs = tokenizer(test_text, return_tensors=\"pt\", max_length=MAX_INPUT_LENGTH, truncation=True)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_length=max_length,\n",
    "                num_beams=1,\n",
    "                do_sample=False,\n",
    "                early_stopping=True,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return generated_text\n",
    "    except torch.cuda.OutOfMemoryError as e:\n",
    "        logger.error(f\"Out of memory during generation test: {e}\")\n",
    "        return \"Generation failed: Out of memory\"\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during generation test: {e}\")\n",
    "        return f\"Generation failed: {e}\"\n",
    "\n",
    "# Test with a simple example\n",
    "test_input = \"The quick brown fox <extra_id_0> over the lazy dog.\"\n",
    "logger.info(f\"Testing generation with input: '{test_input}'\")\n",
    "result = test_generation(model, tokenizer, test_input)\n",
    "logger.info(f\"Generated output: '{result}'\")\n",
    "\n",
    "# Test delta-embedding effect with longer sequence\n",
    "test_input_long = \"Hello world <extra_id_0> this is <extra_id_1> test of delta embeddings <extra_id_2>.\"\n",
    "logger.info(f\"Testing with longer sequence: '{test_input_long}'\")\n",
    "result_long = test_generation(model, tokenizer, test_input_long)\n",
    "logger.info(f\"Generated output (long): '{result_long}'\")\n",
    "\n",
    "# ‚úÖ FROM SCRATCH TRAINING COMPLETE\n",
    "logger.info(\"\"\"\n",
    "üéâ FROM SCRATCH TRAINING COMPLETED!\n",
    "\n",
    "Key Changes Made:\n",
    "‚úÖ Model initialized with RANDOM WEIGHTS (no pretrained loading)\n",
    "‚úÖ Delta-embedding logic PRESERVED (subtraction and addition of embeddings)\n",
    "‚úÖ Increased epochs to 3 for better convergence from scratch\n",
    "‚úÖ Slightly higher learning rate (5e-4) for from-scratch training\n",
    "‚úÖ Added warmup steps and linear scheduler for better convergence\n",
    "‚úÖ All memory optimizations maintained\n",
    "‚úÖ Training info saved to training_info.json\n",
    "\n",
    "Your T5-small model with delta-embeddings has been trained from scratch!\n",
    "\"\"\")\n",
    "\n",
    "# ‚úÖ MEMORY TIPS for further optimization if still having issues:\n",
    "logger.info(\"\"\"\n",
    "üí° If you still encounter memory issues, try these additional steps:\n",
    "1. Reduce TRAIN_BATCH_SIZE to 2 or 1\n",
    "2. Increase GRADIENT_ACCUMULATION_STEPS to 8 or 16\n",
    "3. Reduce MAX_INPUT_LENGTH and MAX_TARGET_LENGTH to 128 and 64\n",
    "4. Set eval_steps to higher value (like 2000) or disable evaluation entirely\n",
    "5. Reduce NUM_EPOCHS to 1 or 2 for initial testing\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
