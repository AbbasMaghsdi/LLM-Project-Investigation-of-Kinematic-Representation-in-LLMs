{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "!pip uninstall -y numpy transformers datasets\n",
    "!pip install numpy --force-reinstall --no-cache-dir\n",
    "!pip install transformers datasets --force-reinstall --no-cache-dir\n",
    "\n",
    "\n",
    "os.kill(os.getpid(), 9)  # Restart the Colab runtime (REQUIRED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/babylm/baseline-pretraining.git\n",
    "%cd baseline-pretraining\n",
    "!wget -O babylm_data.zip \"https://files.osf.io/v1/resources/ad7qg/providers/osfstorage/661517db943bee3731dfec25/?zip=\"\n",
    "!unzip babylm_data.zip -d babylm_data\n",
    "!unzip babylm_data/train_10M.zip -d babylm_data/train_10M\n",
    "!unzip babylm_data/dev.zip -d babylm_data/dev\n",
    "!unzip babylm_data/test.zip -d babylm_data/test\n",
    "!cat babylm_data/train_10M/train_10M/*.train > babylm_data/babylm_train.txt\n",
    "!cat babylm_data/dev/dev/*.dev > babylm_data/babylm_dev.txt\n",
    "!cat babylm_data/test/test/*.test > babylm_data/babylm_test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training the t5-small base model from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Memory-optimized training script for T5 model FROM SCRATCH (Base Model - No Delta Embedding).\n",
    "This version includes:\n",
    "- Training T5-small from scratch (random initialization)\n",
    "- NO delta embedding logic - standard T5 architecture\n",
    "- Aggressive memory optimization for Colab\n",
    "- Fixed out-of-memory issues during evaluation\n",
    "- Gradient accumulation for effective larger batch sizes\n",
    "- Same dataset and preprocessing as delta embedding models for fair comparison\n",
    "\"\"\"\n",
    "import os\n",
    "import random\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Dict, Optional\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import nltk\n",
    "import evaluate\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    T5Tokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from transformers.modeling_outputs import Seq2SeqLMOutput\n",
    "from transformers import T5Config\n",
    "from inspect import signature\n",
    "# Download NLTK data with error handling\n",
    "try:\n",
    "    nltk.data.find(\"tokenizers/punkt\")\n",
    "    logger.info(\"‚úÖ NLTK punkt tokenizer already downloaded\")\n",
    "except (LookupError, OSError):\n",
    "    try:\n",
    "        nltk.download(\"punkt\", quiet=False)\n",
    "        logger.info(\"‚úÖ NLTK punkt tokenizer downloaded successfully\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Failed to download NLTK punkt: {e}\")\n",
    "        logger.info(\"Continuing without sentence tokenization...\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info(f\"Using device: {device}\")\n",
    "\n",
    "# ---\n",
    "# Step 1: Setup, Configuration, and Seeding\n",
    "# ---\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- Memory-Optimized Hyperparameters ---\n",
    "MODEL_NAME = \"t5-small\"\n",
    "MAX_INPUT_LENGTH = 256\n",
    "MAX_TARGET_LENGTH = 128\n",
    "NOISE_DENSITY = 0.15\n",
    "MEAN_SPAN_LENGTH = 3\n",
    "\n",
    "# ‚úÖ MEMORY FIX: Ultra-small batch sizes with gradient accumulation\n",
    "TRAIN_BATCH_SIZE = 32  # Same as delta models\n",
    "EVAL_BATCH_SIZE = 8   # Same as delta models\n",
    "GRADIENT_ACCUMULATION_STEPS = 4  # Same as delta models\n",
    "\n",
    "LEARNING_RATE = 5e-4  # Higher learning rate for from-scratch training\n",
    "NUM_EPOCHS = 5  # Start with 1 epoch\n",
    "\n",
    "BASE_PROJECT_DIR = Path(\"/content/drive/MyDrive/llm-project\")\n",
    "PROCESSED_DATASET_PATH = BASE_PROJECT_DIR / \"processed_dataset\"\n",
    "OUTPUT_DIR = str(BASE_PROJECT_DIR / \"t5-small-base-babylm-from-scratch\")\n",
    "LOGGING_DIR = str(BASE_PROJECT_DIR / \"t5_logs_base_scratch\")\n",
    "BABYLM_ROOT_DIR = Path(\"/content/baseline-pretraining/babylm_data\")\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# ‚úÖ MEMORY FIX: Enhanced GPU memory management\n",
    "def clear_gpu_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "        gc.collect()\n",
    "\n",
    "set_seed(42)\n",
    "clear_gpu_memory()\n",
    "\n",
    "try:\n",
    "    nltk.data.find(\"tokenizers/punkt\")\n",
    "except (LookupError, OSError):\n",
    "    nltk.download(\"punkt\", quiet=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    logger.info(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    logger.info(f\"Available Memory: {torch.cuda.memory_reserved(0) / 1e9:.1f} GB\")\n",
    "\n",
    "# ---\n",
    "# Step 2: Load Tokenizer and Initialize Standard T5 Model FROM SCRATCH\n",
    "# ---\n",
    "logger.info(\"Loading tokenizer...\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "logger.info(\"Initializing standard T5 model FROM SCRATCH...\")\n",
    "config = T5Config.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# ‚úÖ FROM SCRATCH: Initialize model with random weights (no pretrained loading)\n",
    "with torch.cuda.device(device):\n",
    "    logger.info(\"üöÄ Creating STANDARD T5 model with RANDOM INITIALIZATION (from scratch)\")\n",
    "    model = T5ForConditionalGeneration(config)\n",
    "\n",
    "    # ‚úÖ CRITICAL: No pretrained weight loading - model starts with random weights\n",
    "    logger.info(\"‚úÖ Standard T5 model initialized from scratch with random weights\")\n",
    "\n",
    "# ‚úÖ FIX: Ensure model vocabulary matches tokenizer\n",
    "original_vocab_size = model.config.vocab_size\n",
    "tokenizer_vocab_size = len(tokenizer)\n",
    "\n",
    "if original_vocab_size != tokenizer_vocab_size:\n",
    "    logger.info(f\"Resizing model embeddings from {original_vocab_size} to {tokenizer_vocab_size}\")\n",
    "    model.resize_token_embeddings(tokenizer_vocab_size)\n",
    "\n",
    "model.config.vocab_size = tokenizer_vocab_size\n",
    "model.to(device)\n",
    "\n",
    "# ‚úÖ FIX: Gradient checkpointing with proper error handling\n",
    "try:\n",
    "    model.gradient_checkpointing_enable()\n",
    "    logger.info(\"‚úÖ Gradient checkpointing enabled\")\n",
    "except Exception as e:\n",
    "    logger.warning(f\"Could not enable gradient checkpointing: {e}\")\n",
    "    logger.info(\"Continuing without gradient checkpointing to avoid memory conflicts\")\n",
    "\n",
    "# Log model parameters for verification\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "logger.info(f\"üìä Model Statistics:\")\n",
    "logger.info(f\"  Total parameters: {total_params:,}\")\n",
    "logger.info(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "logger.info(f\"  Model size: ~{total_params * 4 / 1e6:.1f} MB (fp32)\")\n",
    "\n",
    "# ---\n",
    "# Step 3: Prepare BabyLM Dataset (Same as Delta Models for Fair Comparison)\n",
    "# ---\n",
    "def corrupt_text_for_t5(examples: Dict) -> Dict:\n",
    "    texts = examples[\"text\"]\n",
    "    inputs, targets = [], []\n",
    "    sentinel_start_id = tokenizer.convert_tokens_to_ids(\"<extra_id_0>\")\n",
    "\n",
    "    for text in texts:\n",
    "        # ‚úÖ MEMORY FIX: Reduce max length for preprocessing\n",
    "        tokens = tokenizer(text, add_special_tokens=False, return_tensors=\"np\", max_length=256, truncation=True)[\"input_ids\"][0]\n",
    "\n",
    "        if len(tokens) < 2:\n",
    "            continue\n",
    "\n",
    "        num_tokens_to_mask = int(len(tokens) * NOISE_DENSITY)\n",
    "        num_spans = int(num_tokens_to_mask / MEAN_SPAN_LENGTH)\n",
    "        if num_spans == 0:\n",
    "            continue\n",
    "\n",
    "        span_starts = np.random.choice(np.arange(len(tokens)), size=num_spans, replace=False)\n",
    "        span_lengths = np.random.poisson(lam=MEAN_SPAN_LENGTH, size=num_spans)\n",
    "        span_lengths = np.maximum(1, span_lengths)\n",
    "\n",
    "        mask = np.zeros_like(tokens, dtype=bool)\n",
    "        for start, length in zip(span_starts, span_lengths):\n",
    "            mask[start : start + length] = True\n",
    "\n",
    "        input_ids_list, label_ids, sentinel_id = [], [], sentinel_start_id\n",
    "        i = 0\n",
    "        while i < len(tokens):\n",
    "            if not mask[i]:\n",
    "                input_ids_list.append(tokens[i])\n",
    "                i += 1\n",
    "            else:\n",
    "                input_ids_list.append(sentinel_id)\n",
    "                label_ids.append(sentinel_id)\n",
    "                sentinel_id -= 1\n",
    "                while i < len(tokens) and mask[i]:\n",
    "                    label_ids.append(tokens[i])\n",
    "                    i += 1\n",
    "\n",
    "        inputs.append(tokenizer.decode(input_ids_list))\n",
    "        targets.append(tokenizer.decode(label_ids))\n",
    "\n",
    "    model_inputs = tokenizer(inputs, max_length=MAX_INPUT_LENGTH, padding=\"max_length\", truncation=True)\n",
    "    labels = tokenizer(text_target=targets, max_length=MAX_TARGET_LENGTH, padding=\"max_length\", truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Load or process dataset (same logic as delta models)\n",
    "if PROCESSED_DATASET_PATH.exists():\n",
    "    logger.info(f\"‚úÖ Loading processed dataset from disk: {PROCESSED_DATASET_PATH}\")\n",
    "    tokenized_dataset = DatasetDict.load_from_disk(str(PROCESSED_DATASET_PATH))\n",
    "else:\n",
    "    logger.info(\"Processed dataset not found. Starting preprocessing...\")\n",
    "    try:\n",
    "        raw_dataset = DatasetDict({\n",
    "             \"train\": load_dataset(\"text\", data_files=str(BABYLM_ROOT_DIR / \"babylm_train.txt\"))[\"train\"],\n",
    "             \"validation\": load_dataset(\"text\", data_files=str(BABYLM_ROOT_DIR / \"babylm_dev.txt\"))[\"train\"],\n",
    "        })\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load dataset: {e}. Ensure babylm_data is in the correct path.\")\n",
    "        exit()\n",
    "\n",
    "    tokenized_dataset = raw_dataset.map(\n",
    "        corrupt_text_for_t5, batched=True, remove_columns=[\"text\"],\n",
    "        num_proc=os.cpu_count() // 2, desc=\"Running T5 Corruptor\"\n",
    "    )\n",
    "    logger.info(f\"üíæ Saving processed dataset to disk at: {PROCESSED_DATASET_PATH}\")\n",
    "    tokenized_dataset.save_to_disk(str(PROCESSED_DATASET_PATH))\n",
    "\n",
    "logger.info(\"Preparing train and eval splits...\")\n",
    "train_dataset = tokenized_dataset[\"train\"].shuffle(seed=42)\n",
    "\n",
    "# ‚úÖ MEMORY FIX: Use much smaller eval dataset during training\n",
    "eval_dataset_full = tokenized_dataset[\"validation\"].shuffle(seed=42)\n",
    "eval_dataset = eval_dataset_full.select(range(min(50, len(eval_dataset_full))))  # Only 50 examples for training evals\n",
    "logger.info(f\"Using {len(eval_dataset)} examples for training evaluation (memory optimization)\")\n",
    "\n",
    "# ---\n",
    "# Step 4: Simple Metrics for Training (Loss Only)\n",
    "# ---\n",
    "def simple_compute_metrics(eval_pred):\n",
    "    \"\"\"Simple metrics computation that only returns loss-based metrics during training\"\"\"\n",
    "    return {}\n",
    "\n",
    "# ---\n",
    "# Step 5: Robust ROUGE Computation for Final Evaluation\n",
    "# ---\n",
    "def compute_rouge_metrics(predictions, labels, tokenizer):\n",
    "    \"\"\"Robust ROUGE computation with proper error handling\"\"\"\n",
    "    try:\n",
    "        vocab_size = tokenizer.vocab_size\n",
    "        predictions = np.clip(predictions, 0, vocab_size - 1)\n",
    "\n",
    "        max_pred_id = np.max(predictions) if len(predictions) > 0 else 0\n",
    "        if max_pred_id >= vocab_size:\n",
    "            logger.warning(f\"‚ö†Ô∏è Still found out-of-vocab token ID after clipping! Max ID: {max_pred_id}, Vocab size: {vocab_size}\")\n",
    "            predictions = np.where(predictions >= vocab_size, tokenizer.unk_token_id or tokenizer.pad_token_id, predictions)\n",
    "\n",
    "        try:\n",
    "            decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error decoding predictions: {e}\")\n",
    "            decoded_preds = []\n",
    "            for pred in predictions:\n",
    "                try:\n",
    "                    decoded_preds.append(tokenizer.decode(pred, skip_special_tokens=True))\n",
    "                except:\n",
    "                    decoded_preds.append(\"\")\n",
    "\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "        try:\n",
    "            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error decoding labels: {e}\")\n",
    "            decoded_labels = [\"\"] * len(labels)\n",
    "\n",
    "        decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) if pred.strip() else \"empty\" for pred in decoded_preds]\n",
    "        decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) if label.strip() else \"empty\" for label in decoded_labels]\n",
    "\n",
    "        rouge_metric = evaluate.load(\"rouge\")\n",
    "        result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "        return {key: value * 100 for key, value in result.items()}\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error computing ROUGE metrics: {e}\")\n",
    "        return {\"rouge1\": 0.0, \"rouge2\": 0.0, \"rougeL\": 0.0}\n",
    "\n",
    "# ---\n",
    "# Step 6: Ultra Memory-Optimized Training Setup\n",
    "# ---\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer, model=model, label_pad_token_id=-100,\n",
    "    pad_to_multiple_of=8 if device.type == \"cuda\" else None\n",
    ")\n",
    "\n",
    "# ‚úÖ MEMORY OPTIMIZATION: Ultra-aggressive memory saving settings (same as delta models)\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=EVAL_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,  # Effective batch size = 16\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    weight_decay=0.01,\n",
    "\n",
    "    # ‚úÖ MEMORY FIX: Disable generation during training\n",
    "    predict_with_generate=False,\n",
    "    generation_max_length=MAX_TARGET_LENGTH,\n",
    "    generation_num_beams=1,\n",
    "\n",
    "    # ‚úÖ MEMORY FIX: Optimize logging and evaluation frequency\n",
    "    logging_dir=LOGGING_DIR,\n",
    "    report_to=\"tensorboard\",\n",
    "    fp16=True if device.type == \"cuda\" else False,\n",
    "    dataloader_pin_memory=False,\n",
    "\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=500,  # Less frequent logging\n",
    "\n",
    "    # ‚úÖ CRITICAL MEMORY FIX: Very infrequent evaluation\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=1000,  # Evaluate very infrequently\n",
    "\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1000,\n",
    "    save_total_limit=1,  # Keep only 1 checkpoint\n",
    "\n",
    "    load_best_model_at_end=False,  # ‚úÖ MEMORY FIX: Disable to save memory\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "\n",
    "    # ‚úÖ MEMORY FIX: Additional memory optimizations\n",
    "    remove_unused_columns=True,\n",
    "    group_by_length=False,\n",
    "    length_column_name=None,\n",
    "    eval_accumulation_steps=1,\n",
    "    include_inputs_for_metrics=False,\n",
    "\n",
    "    # ‚úÖ FROM SCRATCH: Learning rate scheduler for better convergence\n",
    "    warmup_steps=200,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "\n",
    "    # ‚úÖ MEMORY FIX: Additional optimizations\n",
    "    max_grad_norm=1.0,  # Gradient clipping\n",
    "    adam_epsilon=1e-6,  # Smaller epsilon for memory\n",
    ")\n",
    "\n",
    "# ‚úÖ MEMORY FIX: Use processing_class instead of deprecated tokenizer parameter\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    processing_class=tokenizer,  # Use processing_class instead of tokenizer\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=simple_compute_metrics,\n",
    ")\n",
    "\n",
    "# ---\n",
    "# Step 7: Ultra Memory-Optimized Training FROM SCRATCH\n",
    "# ---\n",
    "# ‚úÖ MEMORY FIX: Clear memory before training\n",
    "clear_gpu_memory()\n",
    "\n",
    "logger.info(\"üöÄ Starting FROM SCRATCH training with STANDARD T5 model (NO DELTA EMBEDDING)...\")\n",
    "logger.info(f\"üìä Training Configuration:\")\n",
    "logger.info(f\"  Model type: STANDARD T5 (No Delta Embedding)\")\n",
    "logger.info(f\"  Effective batch size: {TRAIN_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\n",
    "logger.info(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "logger.info(f\"  Number of epochs: {NUM_EPOCHS}\")\n",
    "logger.info(f\"  Model initialized: FROM SCRATCH (random weights)\")\n",
    "logger.info(f\"  Memory optimizations: Ultra-aggressive\")\n",
    "logger.info(f\"  Architecture: Standard T5-small (baseline for comparison)\")\n",
    "\n",
    "# Check for checkpoints\n",
    "last_checkpoint = get_last_checkpoint(training_args.output_dir)\n",
    "if last_checkpoint:\n",
    "    logger.info(f\"‚úÖ Checkpoint found at {last_checkpoint}. Resuming training.\")\n",
    "    trainer.train(resume_from_checkpoint=last_checkpoint)\n",
    "else:\n",
    "    logger.info(\"‚ÑπÔ∏è No checkpoint found. Starting training from scratch.\")\n",
    "    try:\n",
    "        trainer.train()\n",
    "    except torch.cuda.OutOfMemoryError as e:\n",
    "        logger.error(f\"CUDA out of memory during training: {e}\")\n",
    "        logger.info(\"Try reducing TRAIN_BATCH_SIZE to 1, or reducing MAX_INPUT_LENGTH to 128\")\n",
    "        raise\n",
    "\n",
    "# ‚úÖ MEMORY FIX: Clear memory after training\n",
    "clear_gpu_memory()\n",
    "\n",
    "# ---\n",
    "# Step 8: Minimal Final Evaluation\n",
    "# ---\n",
    "logger.info(\"Training complete. Running minimal final evaluation...\")\n",
    "\n",
    "# ‚úÖ MEMORY FIX: Use very small dataset for final evaluation\n",
    "final_eval_dataset = eval_dataset_full.select(range(min(20, len(eval_dataset_full))))  # Only 20 examples\n",
    "logger.info(f\"Using {len(final_eval_dataset)} examples for final evaluation\")\n",
    "\n",
    "try:\n",
    "    logger.info(\"Running simple evaluation...\")\n",
    "    eval_results = trainer.evaluate(eval_dataset=final_eval_dataset)\n",
    "    logger.info(f\"Final Evaluation Results: {eval_results}\")\n",
    "\n",
    "except torch.cuda.OutOfMemoryError as e:\n",
    "    logger.error(f\"CUDA out of memory during final evaluation: {e}\")\n",
    "    logger.info(\"Skipping final evaluation due to memory constraints.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error during final evaluation: {e}\")\n",
    "\n",
    "# ---\n",
    "# Step 9: Save Final Model\n",
    "# ---\n",
    "logger.info(f\"üíæ Saving final model to: {OUTPUT_DIR}\")\n",
    "try:\n",
    "    trainer.save_model()\n",
    "    tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "    # Save training info\n",
    "    training_info = {\n",
    "        \"model_type\": \"T5ForConditionalGeneration\",\n",
    "        \"architecture\": \"Standard T5-small (baseline)\",\n",
    "        \"delta_embedding\": False,\n",
    "        \"trained_from_scratch\": True,\n",
    "        \"total_parameters\": total_params,\n",
    "        \"trainable_parameters\": trainable_params,\n",
    "        \"final_training_loss\": trainer.state.log_history[-1].get('train_loss', 'N/A') if trainer.state.log_history else 'N/A'\n",
    "    }\n",
    "\n",
    "    import json\n",
    "    with open(os.path.join(OUTPUT_DIR, 'training_info.json'), 'w') as f:\n",
    "        json.dump(training_info, f, indent=2)\n",
    "\n",
    "    logger.info(\"‚úÖ Model and training info saved successfully!\")\n",
    "    logger.info(f\"üìÑ Training info: {training_info}\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error saving model: {e}\")\n",
    "\n",
    "# ‚úÖ MEMORY FIX: Final cleanup\n",
    "clear_gpu_memory()\n",
    "\n",
    "# ---\n",
    "# Step 10: Memory-Efficient Generation Test\n",
    "# ---\n",
    "def test_generation(model, tokenizer, test_text: str, max_length: int = 32):  # Very short generation\n",
    "    \"\"\"Ultra memory-efficient generation test\"\"\"\n",
    "    try:\n",
    "        model.eval()\n",
    "        inputs = tokenizer(test_text, return_tensors=\"pt\", max_length=128, truncation=True)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_length=max_length,\n",
    "                num_beams=1,\n",
    "                do_sample=False,\n",
    "                early_stopping=True,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return generated_text\n",
    "    except torch.cuda.OutOfMemoryError as e:\n",
    "        logger.error(f\"Out of memory during generation test: {e}\")\n",
    "        return \"Generation failed: Out of memory\"\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during generation test: {e}\")\n",
    "        return f\"Generation failed: {e}\"\n",
    "\n",
    "# Test with a simple example\n",
    "test_input = \"The quick brown fox <extra_id_0> over the lazy dog.\"\n",
    "logger.info(f\"Testing generation with input: '{test_input}'\")\n",
    "result = test_generation(model, tokenizer, test_input)\n",
    "logger.info(f\"Generated output: '{result}'\")\n",
    "\n",
    "# ‚úÖ FROM SCRATCH STANDARD T5 TRAINING COMPLETE\n",
    "logger.info(\"\"\"\n",
    "üéâ FROM SCRATCH STANDARD T5 BASELINE TRAINING COMPLETED!\n",
    "\n",
    "Key Features:\n",
    "‚úÖ Model initialized with RANDOM WEIGHTS (no pretrained loading)\n",
    "‚úÖ STANDARD T5 architecture (no modifications)\n",
    "‚úÖ Ultra-aggressive memory optimizations for Colab\n",
    "‚úÖ Same dataset and preprocessing as delta embedding models\n",
    "‚úÖ Fair comparison baseline for delta embedding experiments\n",
    "\n",
    "Standard T5 Architecture:\n",
    "- No delta embedding modifications\n",
    "- Standard encoder-decoder structure\n",
    "- Standard attention mechanisms\n",
    "- Direct hidden states to language modeling head\n",
    "- Pure baseline for comparison with delta embedding variants\n",
    "\n",
    "Your standard T5-small baseline model has been trained from scratch!\n",
    "This model will serve as the comparison baseline for delta embedding experiments.\n",
    "\"\"\")\n",
    "\n",
    "# ‚úÖ COMPARISON TIPS:\n",
    "logger.info(\"\"\"\n",
    "üìä COMPARISON ANALYSIS TIPS:\n",
    "\n",
    "To fairly compare delta embedding models with this baseline:\n",
    "\n",
    "1. **Training Loss Comparison**: Compare final training losses across models\n",
    "2. **Evaluation Metrics**: Compare eval_loss on same validation set\n",
    "3. **Generation Quality**: Test with same prompts and compare outputs\n",
    "4. **Parameter Count**: All models should have similar parameter counts\n",
    "5. **Training Conditions**: All use same:\n",
    "   - Dataset and preprocessing\n",
    "   - Batch sizes and learning rates\n",
    "   - Number of epochs\n",
    "   - Memory optimizations\n",
    "   - Random seed (42)\n",
    "\n",
    "Expected Results:\n",
    "- This baseline should establish the \"standard\" performance\n",
    "- Delta embedding models should be compared against this baseline\n",
    "- Look for improvements in generation quality and training efficiency\n",
    "- Delta embedding may show benefits in specific tasks or longer sequences\n",
    "\"\"\")\n",
    "\n",
    "# ‚úÖ MEMORY TIPS for extreme optimization if still having issues:\n",
    "logger.info(\"\"\"\n",
    "üí° If you still encounter memory issues, try these EXTREME steps:\n",
    "1. Reduce TRAIN_BATCH_SIZE to 1\n",
    "2. Increase GRADIENT_ACCUMULATION_STEPS to 16 or 32\n",
    "3. Reduce MAX_INPUT_LENGTH to 128 and MAX_TARGET_LENGTH to 64\n",
    "4. Set eval_steps to 5000 or disable evaluation entirely (eval_strategy=\"no\")\n",
    "5. Use only CPU if GPU memory is insufficient\n",
    "6. This baseline model should use LESS memory than delta embedding variants\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
