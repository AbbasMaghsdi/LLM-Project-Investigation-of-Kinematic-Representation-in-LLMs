{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "!pip uninstall -y numpy transformers datasets\n",
    "!pip install numpy --force-reinstall --no-cache-dir\n",
    "!pip install transformers datasets --force-reinstall --no-cache-dir\n",
    "\n",
    "\n",
    "os.kill(os.getpid(), 9)  # Restart the Colab runtime (REQUIRED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/babylm/baseline-pretraining.git\n",
    "%cd baseline-pretraining\n",
    "!wget -O babylm_data.zip \"https://files.osf.io/v1/resources/ad7qg/providers/osfstorage/661517db943bee3731dfec25/?zip=\"\n",
    "!unzip babylm_data.zip -d babylm_data\n",
    "!unzip babylm_data/train_10M.zip -d babylm_data/train_10M\n",
    "!unzip babylm_data/dev.zip -d babylm_data/dev\n",
    "!unzip babylm_data/test.zip -d babylm_data/test\n",
    "!cat babylm_data/train_10M/train_10M/*.train > babylm_data/babylm_train.txt\n",
    "!cat babylm_data/dev/dev/*.dev > babylm_data/babylm_dev.txt\n",
    "!cat babylm_data/test/test/*.test > babylm_data/babylm_test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training t5 from scratch with subtraction and adding with 0.2 alpha percent of middle layers embeddings of words or tokens emmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Memory-optimized training script for T5 model FROM SCRATCH with middle-layer delta-embeddings.\n",
    "This version includes:\n",
    "- Training T5-small from scratch (random initialization)\n",
    "- Middle-layer delta embedding logic (subtraction and addition)\n",
    "- Aggressive memory optimization for Colab\n",
    "- Fixed out-of-memory issues during evaluation\n",
    "- Gradient accumulation for effective larger batch sizes\n",
    "\"\"\"\n",
    "import os\n",
    "import random\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Dict, Optional\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import nltk\n",
    "import evaluate\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    T5Tokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from transformers.modeling_outputs import Seq2SeqLMOutput\n",
    "from transformers import T5Config\n",
    "from inspect import signature\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# ---\n",
    "# Step 1: Setup, Configuration, and Seeding\n",
    "# ---\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# --- Memory-Optimized Hyperparameters ---\n",
    "MODEL_NAME = \"t5-small\"\n",
    "DELTA_WEIGHT = 0.2\n",
    "MIDDLE_LAYER_INDEX = 2  # Which layer to apply delta embedding\n",
    "MAX_INPUT_LENGTH = 256\n",
    "MAX_TARGET_LENGTH = 128\n",
    "NOISE_DENSITY = 0.15\n",
    "MEAN_SPAN_LENGTH = 3\n",
    "\n",
    "# ✅ MEMORY FIX: Ultra-small batch sizes with gradient accumulation\n",
    "TRAIN_BATCH_SIZE = 4  # Your setting\n",
    "EVAL_BATCH_SIZE = 4   # Your setting\n",
    "GRADIENT_ACCUMULATION_STEPS = 4  # Your setting\n",
    "\n",
    "LEARNING_RATE = 5e-4  # Higher learning rate for from-scratch training\n",
    "NUM_EPOCHS = 1  # Start with 1 epoch\n",
    "\n",
    "BASE_PROJECT_DIR = Path(\"/content/drive/MyDrive/llm-project\")\n",
    "PROCESSED_DATASET_PATH = BASE_PROJECT_DIR / \"processed_dataset\"\n",
    "OUTPUT_DIR = str(BASE_PROJECT_DIR / \"t5-small-middle-delta-embedding-babylm-from-scratch-middlelayer\")\n",
    "LOGGING_DIR = str(BASE_PROJECT_DIR / \"t5_logs_middle_delta_scratch\")\n",
    "BABYLM_ROOT_DIR = Path(\"/content/baseline-pretraining/babylm_data\")\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# ✅ MEMORY FIX: Enhanced GPU memory management\n",
    "def clear_gpu_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "        gc.collect()\n",
    "\n",
    "set_seed(42)\n",
    "clear_gpu_memory()\n",
    "\n",
    "try:\n",
    "    nltk.data.find(\"tokenizers/punkt\")\n",
    "except (LookupError, OSError):\n",
    "    nltk.download(\"punkt\", quiet=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    logger.info(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    logger.info(f\"Available Memory: {torch.cuda.memory_reserved(0) / 1e9:.1f} GB\")\n",
    "\n",
    "# ---\n",
    "# Step 2: Custom Model with Middle-Layer Delta-Embedding Logic (FROM SCRATCH)\n",
    "# ---\n",
    "class T5ForMiddleLayerDeltaEmbeddings(T5ForConditionalGeneration):\n",
    "    def __init__(self, config, delta_weight: float = 0.1, middle_layer_index: int = 2):\n",
    "        super().__init__(config)\n",
    "        self.delta_weight = delta_weight\n",
    "        # ✅ FIX: Ensure middle layer index is valid for t5-small (6 encoder + 6 decoder layers)\n",
    "        self.middle_layer_index = min(middle_layer_index, config.num_layers - 1)\n",
    "\n",
    "        # ✅ MEMORY FIX: Add hooks for middle layer processing\n",
    "        self._encoder_middle_hidden = None\n",
    "        self._decoder_middle_hidden = None\n",
    "        self._hooks = []\n",
    "\n",
    "        if not hasattr(self, 'has_logged_init'):\n",
    "            logger.info(f\"Initialized T5ForMiddleLayerDeltaEmbeddings with delta_weight = {self.delta_weight}\")\n",
    "            logger.info(f\"Using middle layer index: {self.middle_layer_index}\")\n",
    "            self.has_logged_init = True\n",
    "\n",
    "    def _apply_delta_to_hidden(self, hidden: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Apply delta embedding logic to middle-layer hidden states.\n",
    "        Core logic: Subtract previous token embedding, add scaled difference.\n",
    "        \"\"\"\n",
    "        if self.delta_weight == 0.0:\n",
    "            return hidden\n",
    "\n",
    "        if hidden.dim() != 3:\n",
    "            logger.warning(f\"Expected 3D tensor, got {hidden.dim()}D. Skipping delta application.\")\n",
    "            return hidden\n",
    "\n",
    "        # Get previous token embeddings (shifted by 1 position)\n",
    "        previous_hidden = torch.roll(hidden, shifts=1, dims=1)\n",
    "        previous_hidden[:, 0, :] = 0  # No \"previous token\" for first token\n",
    "\n",
    "        # ✅ CORE DELTA-EMBEDDING LOGIC: SUBTRACTION and ADDITION\n",
    "        delta = hidden - previous_hidden  # SUBTRACTION of previous embedding\n",
    "        # ✅ FIX: Use non-in-place operation to avoid gradient checkpointing conflict\n",
    "        modified_hidden = hidden + self.delta_weight * delta  # ADDITION of scaled delta\n",
    "\n",
    "        return modified_hidden\n",
    "\n",
    "    def _register_hooks(self):\n",
    "        \"\"\"Register forward hooks to capture middle layer outputs\"\"\"\n",
    "        def make_encoder_hook(layer_idx):\n",
    "            def hook(module, input, output):\n",
    "                if layer_idx == self.middle_layer_index:\n",
    "                    self._encoder_middle_hidden = output[0]  # hidden_states is first element\n",
    "                return output\n",
    "            return hook\n",
    "\n",
    "        def make_decoder_hook(layer_idx):\n",
    "            def hook(module, input, output):\n",
    "                if layer_idx == self.middle_layer_index:\n",
    "                    self._decoder_middle_hidden = output[0]  # hidden_states is first element\n",
    "                return output\n",
    "            return hook\n",
    "\n",
    "        # Register hooks for encoder layers\n",
    "        for i, layer in enumerate(self.encoder.block):\n",
    "            hook = layer.register_forward_hook(make_encoder_hook(i))\n",
    "            self._hooks.append(hook)\n",
    "\n",
    "        # Register hooks for decoder layers\n",
    "        for i, layer in enumerate(self.decoder.block):\n",
    "            hook = layer.register_forward_hook(make_decoder_hook(i))\n",
    "            self._hooks.append(hook)\n",
    "\n",
    "    def _remove_hooks(self):\n",
    "        \"\"\"Remove all registered hooks\"\"\"\n",
    "        for hook in self._hooks:\n",
    "            hook.remove()\n",
    "        self._hooks = []\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        decoder_input_ids=None,\n",
    "        decoder_attention_mask=None,\n",
    "        head_mask=None,\n",
    "        decoder_head_mask=None,\n",
    "        cross_attn_head_mask=None,\n",
    "        encoder_outputs=None,\n",
    "        past_key_values=None,\n",
    "        inputs_embeds=None,\n",
    "        decoder_inputs_embeds=None,\n",
    "        labels=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "        **kwargs,\n",
    "    ) -> Seq2SeqLMOutput:\n",
    "\n",
    "        # ✅ MEMORY FIX: Clear previous middle layer captures\n",
    "        self._encoder_middle_hidden = None\n",
    "        self._decoder_middle_hidden = None\n",
    "\n",
    "        # ✅ Register hooks if not already registered\n",
    "        if not self._hooks:\n",
    "            self._register_hooks()\n",
    "\n",
    "        # ✅ FIX: Filter unknown kwargs to avoid errors\n",
    "        forward_args = signature(super().forward).parameters\n",
    "        filtered_kwargs = {k: v for k, v in kwargs.items() if k in forward_args}\n",
    "\n",
    "        # ✅ Step 1: Get base model outputs (hooks will capture middle layer)\n",
    "        try:\n",
    "            outputs = super().forward(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                decoder_input_ids=decoder_input_ids,\n",
    "                decoder_attention_mask=decoder_attention_mask,\n",
    "                head_mask=head_mask,\n",
    "                decoder_head_mask=decoder_head_mask,\n",
    "                cross_attn_head_mask=cross_attn_head_mask,\n",
    "                encoder_outputs=encoder_outputs,\n",
    "                past_key_values=past_key_values,\n",
    "                inputs_embeds=inputs_embeds,\n",
    "                decoder_inputs_embeds=decoder_inputs_embeds,\n",
    "                labels=labels,\n",
    "                use_cache=use_cache,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "                return_dict=return_dict,\n",
    "                **filtered_kwargs,\n",
    "            )\n",
    "\n",
    "            # ✅ Step 2: Apply delta to captured middle-layer hidden states\n",
    "            if self._decoder_middle_hidden is not None:\n",
    "                # Apply delta embedding logic to middle layer\n",
    "                # ✅ FIX: Clone the tensor to avoid in-place modification issues\n",
    "                decoder_hidden_copy = self._decoder_middle_hidden.clone()\n",
    "                modified_decoder_hidden = self._apply_delta_to_hidden(decoder_hidden_copy)\n",
    "\n",
    "                # ✅ Use the modified hidden state to compute new logits\n",
    "                logits = self.lm_head(modified_decoder_hidden)\n",
    "\n",
    "                # Update outputs with modified logits\n",
    "                outputs.logits = logits\n",
    "\n",
    "            return outputs\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in forward pass: {e}\")\n",
    "            # ✅ MEMORY FIX: Fallback to standard forward pass\n",
    "            return super().forward(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                decoder_input_ids=decoder_input_ids,\n",
    "                decoder_attention_mask=decoder_attention_mask,\n",
    "                labels=labels,\n",
    "                **filtered_kwargs,\n",
    "            )\n",
    "\n",
    "    def __del__(self):\n",
    "        \"\"\"Cleanup hooks when model is deleted\"\"\"\n",
    "        self._remove_hooks()\n",
    "\n",
    "# ---\n",
    "# Step 3: Load Tokenizer and Initialize Model FROM SCRATCH\n",
    "# ---\n",
    "logger.info(\"Loading tokenizer...\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "logger.info(\"Initializing custom model FROM SCRATCH...\")\n",
    "config = T5Config.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# ✅ FROM SCRATCH: Initialize model with random weights (no pretrained loading)\n",
    "with torch.cuda.device(device):\n",
    "    logger.info(\"🚀 Creating T5 model with RANDOM INITIALIZATION (from scratch)\")\n",
    "    model = T5ForMiddleLayerDeltaEmbeddings(\n",
    "        config,\n",
    "        delta_weight=DELTA_WEIGHT,\n",
    "        middle_layer_index=MIDDLE_LAYER_INDEX\n",
    "    )\n",
    "\n",
    "    # ✅ CRITICAL: No pretrained weight loading - model starts with random weights\n",
    "    logger.info(\"✅ Model initialized from scratch with random weights\")\n",
    "\n",
    "# ✅ FIX: Ensure model vocabulary matches tokenizer\n",
    "original_vocab_size = model.config.vocab_size\n",
    "tokenizer_vocab_size = len(tokenizer)\n",
    "\n",
    "if original_vocab_size != tokenizer_vocab_size:\n",
    "    logger.info(f\"Resizing model embeddings from {original_vocab_size} to {tokenizer_vocab_size}\")\n",
    "    model.resize_token_embeddings(tokenizer_vocab_size)\n",
    "\n",
    "model.config.vocab_size = tokenizer_vocab_size\n",
    "model.to(device)\n",
    "\n",
    "# ✅ FIX: Gradient checkpointing with proper error handling\n",
    "try:\n",
    "    model.gradient_checkpointing_enable()\n",
    "    logger.info(\"✅ Gradient checkpointing enabled\")\n",
    "except Exception as e:\n",
    "    logger.warning(f\"Could not enable gradient checkpointing: {e}\")\n",
    "    logger.info(\"Continuing without gradient checkpointing to avoid memory conflicts\")\n",
    "\n",
    "# Log model parameters for verification\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "logger.info(f\"📊 Model Statistics:\")\n",
    "logger.info(f\"  Total parameters: {total_params:,}\")\n",
    "logger.info(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "logger.info(f\"  Model size: ~{total_params * 4 / 1e6:.1f} MB (fp32)\")\n",
    "\n",
    "# ---\n",
    "# Step 4: Prepare BabyLM Dataset (Memory Optimized)\n",
    "# ---\n",
    "def corrupt_text_for_t5(examples: Dict) -> Dict:\n",
    "    texts = examples[\"text\"]\n",
    "    inputs, targets = [], []\n",
    "    sentinel_start_id = tokenizer.convert_tokens_to_ids(\"<extra_id_0>\")\n",
    "\n",
    "    for text in texts:\n",
    "        # ✅ MEMORY FIX: Reduce max length for preprocessing\n",
    "        tokens = tokenizer(text, add_special_tokens=False, return_tensors=\"np\", max_length=256, truncation=True)[\"input_ids\"][0]\n",
    "\n",
    "        if len(tokens) < 2:\n",
    "            continue\n",
    "\n",
    "        num_tokens_to_mask = int(len(tokens) * NOISE_DENSITY)\n",
    "        num_spans = int(num_tokens_to_mask / MEAN_SPAN_LENGTH)\n",
    "        if num_spans == 0:\n",
    "            continue\n",
    "\n",
    "        span_starts = np.random.choice(np.arange(len(tokens)), size=num_spans, replace=False)\n",
    "        span_lengths = np.random.poisson(lam=MEAN_SPAN_LENGTH, size=num_spans)\n",
    "        span_lengths = np.maximum(1, span_lengths)\n",
    "\n",
    "        mask = np.zeros_like(tokens, dtype=bool)\n",
    "        for start, length in zip(span_starts, span_lengths):\n",
    "            mask[start : start + length] = True\n",
    "\n",
    "        input_ids_list, label_ids, sentinel_id = [], [], sentinel_start_id\n",
    "        i = 0\n",
    "        while i < len(tokens):\n",
    "            if not mask[i]:\n",
    "                input_ids_list.append(tokens[i])\n",
    "                i += 1\n",
    "            else:\n",
    "                input_ids_list.append(sentinel_id)\n",
    "                label_ids.append(sentinel_id)\n",
    "                sentinel_id -= 1\n",
    "                while i < len(tokens) and mask[i]:\n",
    "                    label_ids.append(tokens[i])\n",
    "                    i += 1\n",
    "\n",
    "        inputs.append(tokenizer.decode(input_ids_list))\n",
    "        targets.append(tokenizer.decode(label_ids))\n",
    "\n",
    "    model_inputs = tokenizer(inputs, max_length=MAX_INPUT_LENGTH, padding=\"max_length\", truncation=True)\n",
    "    labels = tokenizer(text_target=targets, max_length=MAX_TARGET_LENGTH, padding=\"max_length\", truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Load or process dataset\n",
    "if PROCESSED_DATASET_PATH.exists():\n",
    "    logger.info(f\"✅ Loading processed dataset from disk: {PROCESSED_DATASET_PATH}\")\n",
    "    tokenized_dataset = DatasetDict.load_from_disk(str(PROCESSED_DATASET_PATH))\n",
    "else:\n",
    "    logger.info(\"Processed dataset not found. Starting preprocessing...\")\n",
    "    try:\n",
    "        raw_dataset = DatasetDict({\n",
    "             \"train\": load_dataset(\"text\", data_files=str(BABYLM_ROOT_DIR / \"babylm_train.txt\"))[\"train\"],\n",
    "             \"validation\": load_dataset(\"text\", data_files=str(BABYLM_ROOT_DIR / \"babylm_dev.txt\"))[\"train\"],\n",
    "        })\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to load dataset: {e}. Ensure babylm_data is in the correct path.\")\n",
    "        exit()\n",
    "\n",
    "    tokenized_dataset = raw_dataset.map(\n",
    "        corrupt_text_for_t5, batched=True, remove_columns=[\"text\"],\n",
    "        num_proc=os.cpu_count() // 2, desc=\"Running T5 Corruptor\"\n",
    "    )\n",
    "    logger.info(f\"💾 Saving processed dataset to disk at: {PROCESSED_DATASET_PATH}\")\n",
    "    tokenized_dataset.save_to_disk(str(PROCESSED_DATASET_PATH))\n",
    "\n",
    "logger.info(\"Preparing train and eval splits...\")\n",
    "train_dataset = tokenized_dataset[\"train\"].shuffle(seed=42)\n",
    "\n",
    "# ✅ MEMORY FIX: Use much smaller eval dataset during training\n",
    "eval_dataset_full = tokenized_dataset[\"validation\"].shuffle(seed=42)\n",
    "eval_dataset = eval_dataset_full.select(range(min(50, len(eval_dataset_full))))  # Only 50 examples for training evals\n",
    "logger.info(f\"Using {len(eval_dataset)} examples for training evaluation (memory optimization)\")\n",
    "\n",
    "# ---\n",
    "# Step 5: Simple Metrics for Training (Loss Only)\n",
    "# ---\n",
    "def simple_compute_metrics(eval_pred):\n",
    "    \"\"\"Simple metrics computation that only returns loss-based metrics during training\"\"\"\n",
    "    return {}\n",
    "\n",
    "# ---\n",
    "# Step 6: Robust ROUGE Computation for Final Evaluation\n",
    "# ---\n",
    "def compute_rouge_metrics(predictions, labels, tokenizer):\n",
    "    \"\"\"Robust ROUGE computation with proper error handling\"\"\"\n",
    "    try:\n",
    "        vocab_size = tokenizer.vocab_size\n",
    "        predictions = np.clip(predictions, 0, vocab_size - 1)\n",
    "\n",
    "        max_pred_id = np.max(predictions) if len(predictions) > 0 else 0\n",
    "        if max_pred_id >= vocab_size:\n",
    "            logger.warning(f\"⚠️ Still found out-of-vocab token ID after clipping! Max ID: {max_pred_id}, Vocab size: {vocab_size}\")\n",
    "            predictions = np.where(predictions >= vocab_size, tokenizer.unk_token_id or tokenizer.pad_token_id, predictions)\n",
    "\n",
    "        try:\n",
    "            decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error decoding predictions: {e}\")\n",
    "            decoded_preds = []\n",
    "            for pred in predictions:\n",
    "                try:\n",
    "                    decoded_preds.append(tokenizer.decode(pred, skip_special_tokens=True))\n",
    "                except:\n",
    "                    decoded_preds.append(\"\")\n",
    "\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "        try:\n",
    "            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error decoding labels: {e}\")\n",
    "            decoded_labels = [\"\"] * len(labels)\n",
    "\n",
    "        decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) if pred.strip() else \"empty\" for pred in decoded_preds]\n",
    "        decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) if label.strip() else \"empty\" for label in decoded_labels]\n",
    "\n",
    "        rouge_metric = evaluate.load(\"rouge\")\n",
    "        result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "        return {key: value * 100 for key, value in result.items()}\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error computing ROUGE metrics: {e}\")\n",
    "        return {\"rouge1\": 0.0, \"rouge2\": 0.0, \"rougeL\": 0.0}\n",
    "\n",
    "# ---\n",
    "# Step 7: Ultra Memory-Optimized Training Setup\n",
    "# ---\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer, model=model, label_pad_token_id=-100,\n",
    "    pad_to_multiple_of=8 if device.type == \"cuda\" else None\n",
    ")\n",
    "\n",
    "# ✅ MEMORY OPTIMIZATION: Ultra-aggressive memory saving settings\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=EVAL_BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,  # Effective batch size = 16\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    weight_decay=0.01,\n",
    "\n",
    "    # ✅ MEMORY FIX: Disable generation during training\n",
    "    predict_with_generate=False,\n",
    "    generation_max_length=MAX_TARGET_LENGTH,\n",
    "    generation_num_beams=1,\n",
    "\n",
    "    # ✅ MEMORY FIX: Optimize logging and evaluation frequency\n",
    "    logging_dir=LOGGING_DIR,\n",
    "    report_to=\"tensorboard\",\n",
    "    fp16=True if device.type == \"cuda\" else False,\n",
    "    dataloader_pin_memory=False,\n",
    "\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=500,  # Less frequent logging\n",
    "\n",
    "    # ✅ CRITICAL MEMORY FIX: Very infrequent evaluation\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=2000,  # Evaluate very infrequently\n",
    "\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=2000,\n",
    "    save_total_limit=1,  # Keep only 1 checkpoint\n",
    "\n",
    "    load_best_model_at_end=False,  # ✅ MEMORY FIX: Disable to save memory\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "\n",
    "    # ✅ MEMORY FIX: Additional memory optimizations\n",
    "    remove_unused_columns=True,\n",
    "    group_by_length=False,\n",
    "    length_column_name=None,\n",
    "    eval_accumulation_steps=1,\n",
    "    include_inputs_for_metrics=False,\n",
    "\n",
    "    # ✅ FROM SCRATCH: Learning rate scheduler for better convergence\n",
    "    warmup_steps=200,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "\n",
    "    # ✅ MEMORY FIX: Additional optimizations\n",
    "    max_grad_norm=1.0,  # Gradient clipping\n",
    "    adam_epsilon=1e-6,  # Smaller epsilon for memory\n",
    ")\n",
    "\n",
    "# ✅ MEMORY FIX: Use processing_class instead of deprecated tokenizer parameter\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    processing_class=tokenizer,  # Use processing_class instead of tokenizer\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=simple_compute_metrics,\n",
    ")\n",
    "\n",
    "# ---\n",
    "# Step 8: Ultra Memory-Optimized Training FROM SCRATCH\n",
    "# ---\n",
    "# ✅ MEMORY FIX: Clear memory before training\n",
    "clear_gpu_memory()\n",
    "\n",
    "logger.info(\"🚀 Starting FROM SCRATCH training with middle-layer delta-embedding T5 model...\")\n",
    "logger.info(f\"📊 Training Configuration:\")\n",
    "logger.info(f\"  Delta weight: {DELTA_WEIGHT}\")\n",
    "logger.info(f\"  Middle layer index: {MIDDLE_LAYER_INDEX}\")\n",
    "logger.info(f\"  Effective batch size: {TRAIN_BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\n",
    "logger.info(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "logger.info(f\"  Number of epochs: {NUM_EPOCHS}\")\n",
    "logger.info(f\"  Model initialized: FROM SCRATCH (random weights)\")\n",
    "logger.info(f\"  Memory optimizations: Ultra-aggressive\")\n",
    "\n",
    "# Check for checkpoints\n",
    "last_checkpoint = get_last_checkpoint(training_args.output_dir)\n",
    "if last_checkpoint:\n",
    "    logger.info(f\"✅ Checkpoint found at {last_checkpoint}. Resuming training.\")\n",
    "    trainer.train(resume_from_checkpoint=last_checkpoint)\n",
    "else:\n",
    "    logger.info(\"ℹ️ No checkpoint found. Starting training from scratch.\")\n",
    "    try:\n",
    "        trainer.train()\n",
    "    except torch.cuda.OutOfMemoryError as e:\n",
    "        logger.error(f\"CUDA out of memory during training: {e}\")\n",
    "        logger.info(\"Try reducing TRAIN_BATCH_SIZE to 1, or reducing MAX_INPUT_LENGTH to 128\")\n",
    "        raise\n",
    "\n",
    "# ✅ MEMORY FIX: Clear memory after training\n",
    "clear_gpu_memory()\n",
    "\n",
    "# ---\n",
    "# Step 9: Minimal Final Evaluation\n",
    "# ---\n",
    "logger.info(\"Training complete. Running minimal final evaluation...\")\n",
    "\n",
    "# ✅ MEMORY FIX: Use very small dataset for final evaluation\n",
    "final_eval_dataset = eval_dataset_full.select(range(min(20, len(eval_dataset_full))))  # Only 20 examples\n",
    "logger.info(f\"Using {len(final_eval_dataset)} examples for final evaluation\")\n",
    "\n",
    "try:\n",
    "    logger.info(\"Running simple evaluation...\")\n",
    "    eval_results = trainer.evaluate(eval_dataset=final_eval_dataset)\n",
    "    logger.info(f\"Final Evaluation Results: {eval_results}\")\n",
    "\n",
    "except torch.cuda.OutOfMemoryError as e:\n",
    "    logger.error(f\"CUDA out of memory during final evaluation: {e}\")\n",
    "    logger.info(\"Skipping final evaluation due to memory constraints.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error during final evaluation: {e}\")\n",
    "\n",
    "# ---\n",
    "# Step 10: Save Final Model\n",
    "# ---\n",
    "logger.info(f\"💾 Saving final model to: {OUTPUT_DIR}\")\n",
    "try:\n",
    "    trainer.save_model()\n",
    "    tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "    # Save training info\n",
    "    training_info = {\n",
    "        \"model_type\": \"T5ForMiddleLayerDeltaEmbeddings\",\n",
    "        \"delta_weight\": DELTA_WEIGHT,\n",
    "        \"middle_layer_index\": MIDDLE_LAYER_INDEX,\n",
    "        \"trained_from_scratch\": True,\n",
    "        \"total_parameters\": total_params,\n",
    "        \"trainable_parameters\": trainable_params,\n",
    "        \"final_training_loss\": trainer.state.log_history[-1].get('train_loss', 'N/A') if trainer.state.log_history else 'N/A'\n",
    "    }\n",
    "\n",
    "    import json\n",
    "    with open(os.path.join(OUTPUT_DIR, 'training_info.json'), 'w') as f:\n",
    "        json.dump(training_info, f, indent=2)\n",
    "\n",
    "    logger.info(\"✅ Model and training info saved successfully!\")\n",
    "    logger.info(f\"📄 Training info: {training_info}\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error saving model: {e}\")\n",
    "\n",
    "# ✅ MEMORY FIX: Final cleanup\n",
    "clear_gpu_memory()\n",
    "\n",
    "# ---\n",
    "# Step 11: Memory-Efficient Generation Test\n",
    "# ---\n",
    "def test_generation(model, tokenizer, test_text: str, max_length: int = 32):  # Very short generation\n",
    "    \"\"\"Ultra memory-efficient generation test\"\"\"\n",
    "    try:\n",
    "        model.eval()\n",
    "        inputs = tokenizer(test_text, return_tensors=\"pt\", max_length=128, truncation=True)\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_length=max_length,\n",
    "                num_beams=1,\n",
    "                do_sample=False,\n",
    "                early_stopping=True,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "\n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return generated_text\n",
    "    except torch.cuda.OutOfMemoryError as e:\n",
    "        logger.error(f\"Out of memory during generation test: {e}\")\n",
    "        return \"Generation failed: Out of memory\"\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during generation test: {e}\")\n",
    "        return f\"Generation failed: {e}\"\n",
    "\n",
    "# Test with a simple example\n",
    "test_input = \"The quick brown fox <extra_id_0> over the lazy dog.\"\n",
    "logger.info(f\"Testing generation with input: '{test_input}'\")\n",
    "result = test_generation(model, tokenizer, test_input)\n",
    "logger.info(f\"Generated output: '{result}'\")\n",
    "\n",
    "# ✅ FROM SCRATCH MIDDLE-LAYER DELTA TRAINING COMPLETE\n",
    "logger.info(\"\"\"\n",
    "🎉 FROM SCRATCH MIDDLE-LAYER DELTA TRAINING COMPLETED!\n",
    "\n",
    "Key Features:\n",
    "✅ Model initialized with RANDOM WEIGHTS (no pretrained loading)\n",
    "✅ Middle-layer delta-embedding logic IMPLEMENTED (subtraction and addition at layer 2)\n",
    "✅ Ultra-aggressive memory optimizations for Colab\n",
    "✅ Hooks system for capturing middle-layer embeddings\n",
    "✅ Forward pass modified to apply delta logic to middle-layer hidden states\n",
    "✅ All memory optimizations maintained and enhanced\n",
    "\n",
    "Middle-Layer Delta Logic:\n",
    "- Captures hidden states from layer 2 (configurable)\n",
    "- Applies delta embedding: current - previous + alpha * delta\n",
    "- Uses modified hidden states to compute final logits\n",
    "- Preserves temporal relationships in middle representations\n",
    "\n",
    "Your T5-small model with middle-layer delta-embeddings has been trained from scratch!\n",
    "\"\"\")\n",
    "\n",
    "# ✅ MEMORY TIPS for extreme optimization if still having issues:\n",
    "logger.info(\"\"\"\n",
    "💡 If you still encounter memory issues, try these EXTREME steps:\n",
    "1. Reduce TRAIN_BATCH_SIZE to 1\n",
    "2. Increase GRADIENT_ACCUMULATION_STEPS to 16 or 32\n",
    "3. Reduce MAX_INPUT_LENGTH to 128 and MAX_TARGET_LENGTH to 64\n",
    "4. Set eval_steps to 5000 or disable evaluation entirely (eval_strategy=\"no\")\n",
    "5. Use only CPU if GPU memory is insufficient\n",
    "6. Reduce MIDDLE_LAYER_INDEX to 1 or 0 for less complex processing\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
