{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# Completely remove and reinstall NumPy, Transformers, and Datasets\n",
    "!pip uninstall -y numpy transformers datasets\n",
    "!pip install numpy --force-reinstall --no-cache-dir\n",
    "!pip install transformers datasets --force-reinstall --no-cache-dir\n",
    "!pip install rouge-score\n",
    "!pip install evaluate\n",
    "\n",
    "os.kill(os.getpid(), 9)  # Restart the Colab runtime (REQUIRED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/babylm/baseline-pretraining.git\n",
    "%cd baseline-pretraining\n",
    "!wget -O babylm_data.zip \"https://files.osf.io/v1/resources/ad7qg/providers/osfstorage/661517db943bee3731dfec25/?zip=\"\n",
    "!unzip babylm_data.zip -d babylm_data\n",
    "!unzip babylm_data/train_10M.zip -d babylm_data/train_10M\n",
    "!unzip babylm_data/dev.zip -d babylm_data/dev\n",
    "!unzip babylm_data/test.zip -d babylm_data/test\n",
    "!cat babylm_data/train_10M/train_10M/*.train > babylm_data/babylm_train.txt\n",
    "!cat babylm_data/dev/dev/*.dev > babylm_data/babylm_dev.txt\n",
    "!cat babylm_data/test/test/*.test > babylm_data/babylm_test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# finetuning T5 small with Curriculum Learning with Hybrid Complexity Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# new more advanced t5-small base model with data preprocessing and preparation\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from typing import Dict, List, Tuple, Optional, Union\n",
    "from collections import Counter, defaultdict\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, T5Config\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import math\n",
    "from scipy.stats import entropy\n",
    "from scipy.spatial.distance import cosine\n",
    "import networkx as nx\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import KMeans\n",
    "import torch.nn as nn\n",
    "import time\n",
    "# =============================================================================\n",
    "# HYBRID COMPLEXITY ANALYZER COMBINING BOTH APPROACHES\n",
    "# =============================================================================\n",
    "\n",
    "class HybridComplexityAnalyzer:\n",
    "    \"\"\"\n",
    "    Hybrid complexity analyzer that combines rule-based and data-driven approaches\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer: T5Tokenizer, save_dir: str, learning_mode: bool = True):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.save_dir = save_dir\n",
    "        self.learning_mode = learning_mode\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        # Rule-based patterns (from EnhancedT5ComplexityAnalyzer)\n",
    "        self.linguistic_patterns = {\n",
    "            'subordinate_clauses': r'\\b(although|because|since|while|whereas|if|unless|when|after|before|until|once|provided|given|assuming)\\b',\n",
    "            'relative_clauses': r'\\b(which|that|who|whom|whose|where|when|why)\\s+\\w+',\n",
    "            'passive_constructions': r'\\b(was|were|is|are|been|being)\\s+\\w*ed\\b|\\b\\w+\\s+(was|were|is|are)\\s+\\w*ed\\b',\n",
    "            'complex_verb_forms': r'\\b(have|has|had|will|would|could|should|might|must)\\s+(been|have|had)\\b',\n",
    "            'discourse_markers': r'\\b(furthermore|moreover|additionally|however|nevertheless|nonetheless|consequently|therefore|thus|hence)\\b',\n",
    "            'abstract_concepts': r'\\b\\w+(tion|sion|ness|ment|ity|ism|ance|ence|ship|hood|dom|age)\\b',\n",
    "            'modal_expressions': r'\\b(possibly|probably|certainly|definitely|presumably|apparently|obviously)\\b',\n",
    "        }\n",
    "\n",
    "        # Academic word list\n",
    "        self.academic_words = {\n",
    "            'analysis', 'approach', 'area', 'assessment', 'assume', 'authority',\n",
    "            'available', 'benefit', 'concept', 'consistent', 'constitutional',\n",
    "            'context', 'contract', 'create', 'data', 'definition', 'derived',\n",
    "            'distribution', 'economic', 'environment', 'established', 'estimate',\n",
    "            'evidence', 'export', 'factors', 'financial', 'formula', 'function',\n",
    "            'identified', 'income', 'indicate', 'individual', 'interpretation',\n",
    "            'involved', 'issues', 'labor', 'legal', 'legislation', 'major',\n",
    "            'method', 'occur', 'percent', 'period', 'policy', 'principle',\n",
    "            'procedure', 'process', 'required', 'research', 'response', 'role',\n",
    "            'section', 'significant', 'similar', 'source', 'specific', 'structure',\n",
    "            'theory', 'variables'\n",
    "        }\n",
    "\n",
    "        # Data-driven components\n",
    "        self.word_complexity_scores = {}\n",
    "        self.semantic_features = {}\n",
    "        self.syntactic_patterns = {}\n",
    "        self.cooccurrence_networks = {}\n",
    "        self.complexity_clusters = {}\n",
    "\n",
    "        # Models\n",
    "        self.tfidf_vectorizer = None\n",
    "        self.complexity_classifier = None\n",
    "        self.is_trained = False\n",
    "\n",
    "        # Load or initialize\n",
    "        self._initialize_components()\n",
    "\n",
    "    def _initialize_components(self):\n",
    "        \"\"\"Initialize or load pre-trained components\"\"\"\n",
    "        models_file = os.path.join(self.save_dir, 'hybrid_complexity_models.pkl')\n",
    "\n",
    "        if os.path.exists(models_file) and not self.learning_mode:\n",
    "            print(\"üì¶ Loading pre-trained hybrid complexity models...\")\n",
    "            try:\n",
    "                with open(models_file, 'rb') as f:\n",
    "                    saved_models = pickle.load(f)\n",
    "                    self.word_complexity_scores = saved_models.get('word_complexity_scores', {})\n",
    "                    self.semantic_features = saved_models.get('semantic_features', {})\n",
    "                    self.syntactic_patterns = saved_models.get('syntactic_patterns', {})\n",
    "                    self.tfidf_vectorizer = saved_models.get('tfidf_vectorizer', None)\n",
    "                    self.complexity_clusters = saved_models.get('complexity_clusters', {})\n",
    "                    self.is_trained = True\n",
    "                print(\"‚úÖ Loaded pre-trained models\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Failed to load models: {e}\")\n",
    "\n",
    "        # Load basic frequency data\n",
    "        self.word_frequencies = self._load_frequency_dict()\n",
    "\n",
    "    def _load_frequency_dict(self) -> Dict[str, int]:\n",
    "        \"\"\"Load word frequency dictionary\"\"\"\n",
    "        freq_file = os.path.join(self.save_dir, 'word_frequencies.pkl')\n",
    "\n",
    "        if os.path.exists(freq_file):\n",
    "            try:\n",
    "                with open(freq_file, 'rb') as f:\n",
    "                    return pickle.load(f)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        # Create basic frequency mapping\n",
    "        frequencies = {}\n",
    "\n",
    "        # High frequency words (low complexity)\n",
    "        high_freq = ['the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to',\n",
    "                    'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be',\n",
    "                    'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did',\n",
    "                    'will', 'would', 'can', 'could', 'should', 'may', 'might',\n",
    "                    'I', 'you', 'he', 'she', 'it', 'we', 'they', 'this', 'that',\n",
    "                    'time', 'person', 'year', 'way', 'day', 'thing', 'man', 'world',\n",
    "                    'life', 'hand', 'part', 'child', 'eye', 'woman', 'place', 'work']\n",
    "\n",
    "        for i, word in enumerate(high_freq):\n",
    "            frequencies[word.lower()] = i\n",
    "\n",
    "        # Medium frequency words\n",
    "        medium_freq = ['analysis', 'approach', 'structure', 'process', 'development',\n",
    "                      'environment', 'significant', 'individual', 'particular',\n",
    "                      'available', 'information', 'community', 'economic', 'political']\n",
    "\n",
    "        for i, word in enumerate(medium_freq):\n",
    "            frequencies[word.lower()] = 1000 + i * 50\n",
    "\n",
    "        # Save frequency data\n",
    "        try:\n",
    "            with open(freq_file, 'wb') as f:\n",
    "                pickle.dump(frequencies, f)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        return frequencies\n",
    "\n",
    "    def learn_from_data(self, texts: List[str], complexity_scores: List[float] = None):\n",
    "        \"\"\"\n",
    "        Learn complexity patterns from training data\n",
    "        \"\"\"\n",
    "        if not texts:\n",
    "            print(\"‚ö†Ô∏è No training texts provided\")\n",
    "            return\n",
    "\n",
    "        print(\"üß† Learning complexity patterns from data...\")\n",
    "\n",
    "        # If no complexity scores provided, calculate basic ones\n",
    "        if complexity_scores is None:\n",
    "            complexity_scores = [self._calculate_basic_complexity(text) for text in texts]\n",
    "\n",
    "        # 1. Learn word-level complexity\n",
    "        self._learn_word_complexity(texts, complexity_scores)\n",
    "\n",
    "        # 2. Learn semantic patterns using TF-IDF\n",
    "        self._learn_semantic_patterns(texts, complexity_scores)\n",
    "\n",
    "        # 3. Learn syntactic pattern correlations\n",
    "        self._learn_syntactic_correlations(texts, complexity_scores)\n",
    "\n",
    "        # 4. Create complexity clusters\n",
    "        self._create_complexity_clusters(texts, complexity_scores)\n",
    "\n",
    "        # Save learned models\n",
    "        self._save_models()\n",
    "\n",
    "        self.is_trained = True\n",
    "        print(\"‚úÖ Learning completed!\")\n",
    "\n",
    "    def _learn_word_complexity(self, texts: List[str], complexity_scores: List[float]):\n",
    "        \"\"\"Learn word-level complexity associations\"\"\"\n",
    "        print(\"üìä Learning word-level complexity...\")\n",
    "\n",
    "        word_stats = defaultdict(list)\n",
    "\n",
    "        for text, complexity in zip(texts, complexity_scores):\n",
    "            words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "            for word in words:\n",
    "                word_stats[word].append(complexity)\n",
    "\n",
    "        # Calculate complexity scores for words\n",
    "        for word, scores in word_stats.items():\n",
    "            if len(scores) >= 3:  # Minimum occurrences\n",
    "                avg_complexity = np.mean(scores)\n",
    "                frequency = self.word_frequencies.get(word, 10000)  # Default high frequency\n",
    "\n",
    "                # Combine average complexity with rarity\n",
    "                complexity_score = (\n",
    "                    avg_complexity * 0.7 +\n",
    "                    min(frequency / 10000.0, 1.0) * 0.3  # Rarity factor\n",
    "                )\n",
    "\n",
    "                self.word_complexity_scores[word] = {\n",
    "                    'score': complexity_score,\n",
    "                    'frequency': len(scores),\n",
    "                    'avg_complexity': avg_complexity\n",
    "                }\n",
    "\n",
    "    def _learn_semantic_patterns(self, texts: List[str], complexity_scores: List[float]):\n",
    "        \"\"\"Learn semantic complexity patterns using TF-IDF\"\"\"\n",
    "        print(\"üéØ Learning semantic patterns...\")\n",
    "\n",
    "        # Create TF-IDF vectorizer\n",
    "        self.tfidf_vectorizer = TfidfVectorizer(\n",
    "            max_features=5000,\n",
    "            ngram_range=(1, 2),\n",
    "            stop_words='english',\n",
    "            min_df=2,\n",
    "            max_df=0.95\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            tfidf_matrix = self.tfidf_vectorizer.fit_transform(texts)\n",
    "            feature_names = self.tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "            # Calculate correlation between features and complexity\n",
    "            feature_complexities = {}\n",
    "\n",
    "            for i, feature in enumerate(feature_names):\n",
    "                feature_scores = tfidf_matrix[:, i].toarray().flatten()\n",
    "                if np.std(feature_scores) > 0:\n",
    "                    correlation = np.corrcoef(feature_scores, complexity_scores)[0, 1]\n",
    "                    if not np.isnan(correlation):\n",
    "                        feature_complexities[feature] = correlation\n",
    "\n",
    "            # Store top complexity-correlated features\n",
    "            sorted_features = sorted(feature_complexities.items(),\n",
    "                                   key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "            self.semantic_features = {\n",
    "                'high_complexity': [f for f, c in sorted_features[:200] if c > 0],\n",
    "                'low_complexity': [f for f, c in sorted_features[-200:] if c < 0],\n",
    "                'correlations': feature_complexities\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è TF-IDF learning failed: {e}\")\n",
    "            self.semantic_features = {'high_complexity': [], 'low_complexity': [], 'correlations': {}}\n",
    "\n",
    "    def _learn_syntactic_correlations(self, texts: List[str], complexity_scores: List[float]):\n",
    "        \"\"\"Learn correlations between syntactic patterns and complexity\"\"\"\n",
    "        print(\"üîç Learning syntactic correlations...\")\n",
    "\n",
    "        pattern_correlations = {}\n",
    "\n",
    "        for pattern_name, pattern_regex in self.linguistic_patterns.items():\n",
    "            pattern_densities = []\n",
    "\n",
    "            for text in texts:\n",
    "                words = re.findall(r'\\b\\w+\\b', text)\n",
    "                matches = len(re.findall(pattern_regex, text, re.IGNORECASE))\n",
    "                density = matches / max(len(words), 1)\n",
    "                pattern_densities.append(density)\n",
    "\n",
    "            # Calculate correlation with complexity\n",
    "            if np.std(pattern_densities) > 0:\n",
    "                correlation = np.corrcoef(pattern_densities, complexity_scores)[0, 1]\n",
    "                if not np.isnan(correlation):\n",
    "                    pattern_correlations[pattern_name] = {\n",
    "                        'correlation': correlation,\n",
    "                        'avg_density': np.mean(pattern_densities)\n",
    "                    }\n",
    "\n",
    "        self.syntactic_patterns = pattern_correlations\n",
    "\n",
    "    def _create_complexity_clusters(self, texts: List[str], complexity_scores: List[float]):\n",
    "        \"\"\"Create complexity-based text clusters\"\"\"\n",
    "        print(\"üé® Creating complexity clusters...\")\n",
    "\n",
    "        if not self.tfidf_vectorizer:\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            # Transform texts to TF-IDF vectors\n",
    "            tfidf_matrix = self.tfidf_vectorizer.transform(texts)\n",
    "\n",
    "            # Dimensionality reduction\n",
    "            svd = TruncatedSVD(n_components=50)\n",
    "            reduced_features = svd.fit_transform(tfidf_matrix)\n",
    "\n",
    "            # K-means clustering\n",
    "            kmeans = KMeans(n_clusters=5, random_state=42, n_init=10)\n",
    "            cluster_labels = kmeans.fit_predict(reduced_features)\n",
    "\n",
    "            # Map clusters to average complexity\n",
    "            cluster_complexities = {}\n",
    "            for cluster_id in range(5):\n",
    "                mask = cluster_labels == cluster_id\n",
    "                if np.any(mask):\n",
    "                    cluster_complexities[cluster_id] = np.mean(np.array(complexity_scores)[mask])\n",
    "\n",
    "            self.complexity_clusters = {\n",
    "                'svd': svd,\n",
    "                'kmeans': kmeans,\n",
    "                'cluster_complexities': cluster_complexities\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Clustering failed: {e}\")\n",
    "            self.complexity_clusters = {}\n",
    "\n",
    "    def analyze_text_complexity(self, text: str) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Main method to analyze text complexity using hybrid approach\n",
    "        \"\"\"\n",
    "        if not text or len(text.strip()) < 10:\n",
    "            return {'overall_complexity': 0.0}\n",
    "\n",
    "        words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "        sentences = re.split(r'[.!?]+', text)\n",
    "        sentences = [s.strip() for s in sentences if s.strip()]\n",
    "\n",
    "        if not words or not sentences:\n",
    "            return {'overall_complexity': 0.0}\n",
    "\n",
    "        features = {}\n",
    "\n",
    "        # 1. RULE-BASED FEATURES (from original EnhancedT5ComplexityAnalyzer)\n",
    "        features.update(self._calculate_rule_based_features(text, words, sentences))\n",
    "\n",
    "        # 2. DATA-DRIVEN FEATURES\n",
    "        features.update(self._calculate_data_driven_features(text, words))\n",
    "\n",
    "        # 3. SEMANTIC FEATURES\n",
    "        features.update(self._calculate_semantic_features(text))\n",
    "\n",
    "        # 4. OVERALL COMPLEXITY CALCULATION\n",
    "        features['overall_complexity'] = self._calculate_overall_complexity(features)\n",
    "\n",
    "        return features\n",
    "\n",
    "    def _calculate_rule_based_features(self, text: str, words: List[str], sentences: List[str]) -> Dict[str, float]:\n",
    "        \"\"\"Calculate rule-based linguistic features\"\"\"\n",
    "        features = {}\n",
    "\n",
    "        # Lexical diversity\n",
    "        unique_words = len(set(words))\n",
    "        ttr = unique_words / len(words)\n",
    "        corrected_ttr = unique_words / math.sqrt(2 * len(words))\n",
    "        features['lexical_diversity'] = min(corrected_ttr, 1.0)\n",
    "\n",
    "        # Morphological complexity\n",
    "        avg_word_length = sum(len(word) for word in words) / len(words)\n",
    "        features['morphological_complexity'] = min((avg_word_length - 3) / 6, 1.0)\n",
    "\n",
    "        # Sentence complexity\n",
    "        sent_lengths = [len(re.findall(r'\\b\\w+\\b', s)) for s in sentences]\n",
    "        avg_sent_length = sum(sent_lengths) / len(sent_lengths)\n",
    "        features['sentence_length'] = min((avg_sent_length - 8) / 25, 1.0)\n",
    "\n",
    "        # Syntactic pattern density\n",
    "        syntactic_complexity = 0.0\n",
    "        for pattern_name, pattern_regex in self.linguistic_patterns.items():\n",
    "            matches = len(re.findall(pattern_regex, text, re.IGNORECASE))\n",
    "            density = matches / len(words)\n",
    "\n",
    "            # Weight by learned correlation if available\n",
    "            if self.is_trained and pattern_name in self.syntactic_patterns:\n",
    "                weight = abs(self.syntactic_patterns[pattern_name]['correlation'])\n",
    "            else:\n",
    "                weight = 1.0\n",
    "\n",
    "            syntactic_complexity += density * weight\n",
    "\n",
    "        features['syntactic_complexity'] = min(syntactic_complexity, 1.0)\n",
    "\n",
    "        # Academic word density\n",
    "        academic_count = sum(1 for word in words if word in self.academic_words)\n",
    "        features['academic_density'] = min(academic_count / len(words) * 10, 1.0)\n",
    "\n",
    "        return features\n",
    "\n",
    "    def _calculate_data_driven_features(self, text: str, words: List[str]) -> Dict[str, float]:\n",
    "        \"\"\"Calculate data-driven features\"\"\"\n",
    "        features = {}\n",
    "\n",
    "        if not self.is_trained:\n",
    "            features['learned_word_complexity'] = 0.5\n",
    "            features['rare_word_density'] = 0.5\n",
    "            return features\n",
    "\n",
    "        # Learned word complexity\n",
    "        word_complexities = []\n",
    "        rare_word_count = 0\n",
    "\n",
    "        for word in words:\n",
    "            if word in self.word_complexity_scores:\n",
    "                word_complexities.append(self.word_complexity_scores[word]['score'])\n",
    "            elif word not in self.word_frequencies or self.word_frequencies[word] > 5000:\n",
    "                rare_word_count += 1\n",
    "\n",
    "        if word_complexities:\n",
    "            features['learned_word_complexity'] = min(np.mean(word_complexities), 1.0)\n",
    "        else:\n",
    "            features['learned_word_complexity'] = 0.5\n",
    "\n",
    "        features['rare_word_density'] = min(rare_word_count / len(words) * 5, 1.0)\n",
    "\n",
    "        return features\n",
    "\n",
    "    def _calculate_semantic_features(self, text: str) -> Dict[str, float]:\n",
    "        \"\"\"Calculate semantic complexity features\"\"\"\n",
    "        features = {}\n",
    "\n",
    "        if not self.is_trained or not self.tfidf_vectorizer:\n",
    "            features['semantic_complexity'] = 0.5\n",
    "            features['cluster_complexity'] = 0.5\n",
    "            return features\n",
    "\n",
    "        try:\n",
    "            # TF-IDF based semantic complexity\n",
    "            tfidf_vector = self.tfidf_vectorizer.transform([text])\n",
    "            feature_names = self.tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "            semantic_score = 0.0\n",
    "            total_weight = 0.0\n",
    "\n",
    "            for i, feature in enumerate(feature_names):\n",
    "                weight = tfidf_vector[0, i]\n",
    "                if weight > 0 and feature in self.semantic_features['correlations']:\n",
    "                    correlation = self.semantic_features['correlations'][feature]\n",
    "                    semantic_score += weight * max(correlation, 0)  # Only positive correlations\n",
    "                    total_weight += weight\n",
    "\n",
    "            if total_weight > 0:\n",
    "                features['semantic_complexity'] = min(semantic_score / total_weight, 1.0)\n",
    "            else:\n",
    "                features['semantic_complexity'] = 0.5\n",
    "\n",
    "            # Cluster-based complexity\n",
    "            if self.complexity_clusters:\n",
    "                try:\n",
    "                    reduced_vector = self.complexity_clusters['svd'].transform(tfidf_vector)\n",
    "                    cluster_id = self.complexity_clusters['kmeans'].predict(reduced_vector)[0]\n",
    "                    cluster_complexity = self.complexity_clusters['cluster_complexities'].get(cluster_id, 0.5)\n",
    "                    features['cluster_complexity'] = cluster_complexity\n",
    "                except:\n",
    "                    features['cluster_complexity'] = 0.5\n",
    "            else:\n",
    "                features['cluster_complexity'] = 0.5\n",
    "\n",
    "        except Exception as e:\n",
    "            features['semantic_complexity'] = 0.5\n",
    "            features['cluster_complexity'] = 0.5\n",
    "\n",
    "        return features\n",
    "\n",
    "    def _calculate_overall_complexity(self, features: Dict[str, float]) -> float:\n",
    "        \"\"\"Calculate overall complexity from all features\"\"\"\n",
    "\n",
    "        # Adaptive weights based on whether system is trained\n",
    "        if self.is_trained:\n",
    "            weights = {\n",
    "                'lexical_diversity': 0.12,\n",
    "                'morphological_complexity': 0.08,\n",
    "                'sentence_length': 0.10,\n",
    "                'syntactic_complexity': 0.15,\n",
    "                'academic_density': 0.08,\n",
    "                'learned_word_complexity': 0.20,\n",
    "                'rare_word_density': 0.10,\n",
    "                'semantic_complexity': 0.12,\n",
    "                'cluster_complexity': 0.05\n",
    "            }\n",
    "        else:\n",
    "            # Rule-based weights when not trained\n",
    "            weights = {\n",
    "                'lexical_diversity': 0.20,\n",
    "                'morphological_complexity': 0.15,\n",
    "                'sentence_length': 0.15,\n",
    "                'syntactic_complexity': 0.25,\n",
    "                'academic_density': 0.15,\n",
    "                'learned_word_complexity': 0.05,\n",
    "                'rare_word_density': 0.05,\n",
    "                'semantic_complexity': 0.0,\n",
    "                'cluster_complexity': 0.0\n",
    "            }\n",
    "\n",
    "        overall_score = sum(\n",
    "            features.get(feature, 0.0) * weight\n",
    "            for feature, weight in weights.items()\n",
    "        )\n",
    "\n",
    "        return min(max(overall_score, 0.01), 1.0)\n",
    "\n",
    "    def _calculate_basic_complexity(self, text: str) -> float:\n",
    "        \"\"\"Calculate basic complexity for initial learning\"\"\"\n",
    "        words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "        sentences = re.split(r'[.!?]+', text)\n",
    "        sentences = [s.strip() for s in sentences if s.strip()]\n",
    "\n",
    "        if not words or not sentences:\n",
    "            return 0.1\n",
    "\n",
    "        # Simple heuristics\n",
    "        avg_word_len = sum(len(w) for w in words) / len(words)\n",
    "        avg_sent_len = len(words) / len(sentences)\n",
    "        academic_ratio = sum(1 for w in words if w in self.academic_words) / len(words)\n",
    "\n",
    "        complexity = (\n",
    "            min((avg_word_len - 4) / 6, 1.0) * 0.3 +\n",
    "            min((avg_sent_len - 8) / 20, 1.0) * 0.4 +\n",
    "            min(academic_ratio * 10, 1.0) * 0.3\n",
    "        )\n",
    "\n",
    "        return min(max(complexity, 0.05), 0.95)\n",
    "\n",
    "    def _save_models(self):\n",
    "        \"\"\"Save all learned models\"\"\"\n",
    "        models_to_save = {\n",
    "            'word_complexity_scores': self.word_complexity_scores,\n",
    "            'semantic_features': self.semantic_features,\n",
    "            'syntactic_patterns': self.syntactic_patterns,\n",
    "            'tfidf_vectorizer': self.tfidf_vectorizer,\n",
    "            'complexity_clusters': self.complexity_clusters\n",
    "        }\n",
    "\n",
    "        models_file = os.path.join(self.save_dir, 'hybrid_complexity_models.pkl')\n",
    "        try:\n",
    "            with open(models_file, 'wb') as f:\n",
    "                pickle.dump(models_to_save, f)\n",
    "            print(f\"üíæ Models saved to {models_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Failed to save models: {e}\")\n",
    "\n",
    "# =============================================================================\n",
    "# ENHANCED T5 DATASET WITH HYBRID COMPLEXITY ANALYSIS\n",
    "# =============================================================================\n",
    "\n",
    "class T5CurriculumDataset(Dataset):\n",
    "    \"\"\"T5 dataset with hybrid complexity analysis and curriculum learning\"\"\"\n",
    "\n",
    "    def __init__(self, data_path: str, tokenizer: T5Tokenizer,\n",
    "                 complexity_analyzer: HybridComplexityAnalyzer,\n",
    "                 max_source_length: int = 512, max_target_length: int = 256,\n",
    "                 cache_dir: str = None, split: str = \"train\",\n",
    "                 max_examples: int = None, corruption_probability: float = 0.15):\n",
    "\n",
    "        self.data_path = data_path\n",
    "        self.tokenizer = tokenizer\n",
    "        self.complexity_analyzer = complexity_analyzer\n",
    "        self.max_source_length = max_source_length\n",
    "        self.max_target_length = max_target_length\n",
    "        self.split = split\n",
    "        self.max_examples = max_examples\n",
    "        self.corruption_probability = corruption_probability\n",
    "\n",
    "        # Cache setup\n",
    "        self.cache_dir = cache_dir\n",
    "        if cache_dir:\n",
    "            os.makedirs(cache_dir, exist_ok=True)\n",
    "            self.cache_file = os.path.join(cache_dir, f\"t5_curriculum_{split}_data.pkl\")\n",
    "        else:\n",
    "            self.cache_file = None\n",
    "\n",
    "        self.examples = []\n",
    "        self.complexity_scores = []\n",
    "        self.curriculum_levels = {}\n",
    "\n",
    "        self._load_or_process_data()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.examples[idx]\n",
    "\n",
    "    def _load_or_process_data(self):\n",
    "        \"\"\"Load cached data or process from scratch\"\"\"\n",
    "\n",
    "        # Try loading from cache\n",
    "        if self.cache_file and os.path.exists(self.cache_file):\n",
    "            try:\n",
    "                print(f\"üì¶ Loading cached data from {self.cache_file}\")\n",
    "                with open(self.cache_file, 'rb') as f:\n",
    "                    cached_data = pickle.load(f)\n",
    "                    self.examples = cached_data['examples']\n",
    "                    self.complexity_scores = cached_data['complexity_scores']\n",
    "                    self.curriculum_levels = cached_data.get('curriculum_levels', {})\n",
    "                print(f\"‚úÖ Loaded {len(self.examples)} cached examples\")\n",
    "                return\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Failed to load cache: {e}\")\n",
    "\n",
    "        # Process from scratch\n",
    "        self._process_raw_data()\n",
    "\n",
    "        # Create curriculum if training data\n",
    "        if self.split == 'train' and len(self.examples) > 100:\n",
    "            self._create_curriculum()\n",
    "\n",
    "        # Save to cache\n",
    "        if self.cache_file:\n",
    "            try:\n",
    "                cached_data = {\n",
    "                    'examples': self.examples,\n",
    "                    'complexity_scores': self.complexity_scores,\n",
    "                    'curriculum_levels': self.curriculum_levels\n",
    "                }\n",
    "                with open(self.cache_file, 'wb') as f:\n",
    "                    pickle.dump(cached_data, f)\n",
    "                print(f\"üíæ Cached {len(self.examples)} examples\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Failed to save cache: {e}\")\n",
    "\n",
    "    def _process_raw_data(self):\n",
    "        \"\"\"Process raw data into T5 tasks\"\"\"\n",
    "\n",
    "        if not os.path.exists(self.data_path):\n",
    "            print(f\"‚ùå Data file not found: {self.data_path}\")\n",
    "            return\n",
    "\n",
    "        # Read data\n",
    "        try:\n",
    "            with open(self.data_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                text = f.read()\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to read data: {e}\")\n",
    "            return\n",
    "\n",
    "        # Clean and split into documents\n",
    "        documents = self._clean_and_split_text(text)\n",
    "\n",
    "        print(f\"üìù Processing {len(documents)} documents...\")\n",
    "\n",
    "        processed_count = 0\n",
    "        all_texts_for_learning = []\n",
    "        all_complexities_for_learning = []\n",
    "\n",
    "        # First pass: collect all texts for complexity learning\n",
    "        if self.complexity_analyzer.learning_mode and self.split == 'train':\n",
    "            print(\"üîç First pass: collecting texts for complexity learning...\")\n",
    "            for doc in tqdm(documents[:min(2000, len(documents))], desc=\"Collecting texts\"):\n",
    "                if len(doc.split()) < 20 or len(doc.split()) > 500:\n",
    "                    continue\n",
    "                all_texts_for_learning.append(doc)\n",
    "                # Calculate basic complexity for initial learning\n",
    "                basic_complexity = self.complexity_analyzer._calculate_basic_complexity(doc)\n",
    "                all_complexities_for_learning.append(basic_complexity)\n",
    "\n",
    "            # Train the complexity analyzer\n",
    "            if all_texts_for_learning:\n",
    "                self.complexity_analyzer.learn_from_data(\n",
    "                    all_texts_for_learning,\n",
    "                    all_complexities_for_learning\n",
    "                )\n",
    "\n",
    "        # Second pass: process into T5 tasks with learned complexity analysis\n",
    "        for doc in tqdm(documents, desc=\"Processing documents\"):\n",
    "            if self.max_examples and processed_count >= self.max_examples:\n",
    "                break\n",
    "\n",
    "            try:\n",
    "                word_count = len(doc.split())\n",
    "                if word_count < 20 or word_count > 500:\n",
    "                    continue\n",
    "\n",
    "                # Analyze complexity with hybrid analyzer\n",
    "                complexity_analysis = self.complexity_analyzer.analyze_text_complexity(doc)\n",
    "                complexity_score = complexity_analysis['overall_complexity']\n",
    "\n",
    "                # Skip very low complexity texts in training\n",
    "                if self.split == 'train' and complexity_score < 0.02:\n",
    "                    continue\n",
    "\n",
    "                # Create T5 tasks\n",
    "                tasks = self._create_t5_tasks(doc)\n",
    "\n",
    "                for task in tasks:\n",
    "                    if task and processed_count < (self.max_examples or float('inf')):\n",
    "                        self.examples.append(task)\n",
    "                        self.complexity_scores.append(complexity_score)\n",
    "                        processed_count += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                continue\n",
    "\n",
    "        print(f\"‚úÖ Processed {len(self.examples)} examples\")\n",
    "\n",
    "    def _clean_and_split_text(self, text: str) -> List[str]:\n",
    "        \"\"\"Clean text and split into documents\"\"\"\n",
    "\n",
    "        # Basic cleaning\n",
    "        text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)\n",
    "        text = re.sub(r'[ \\t]+', ' ', text)\n",
    "        text = re.sub(r'\\s+([,.!?;:])', r'\\1', text)\n",
    "\n",
    "        # Split into paragraphs\n",
    "        paragraphs = text.split('\\n\\n')\n",
    "        documents = []\n",
    "\n",
    "        for para in paragraphs:\n",
    "            para = para.strip()\n",
    "            if len(para.split()) < 20:\n",
    "                continue\n",
    "\n",
    "            # If paragraph is too long, split by sentences\n",
    "            if len(para.split()) > 400:\n",
    "                sentences = re.split(r'(?<=[.!?])\\s+', para)\n",
    "                current_doc = []\n",
    "                current_length = 0\n",
    "\n",
    "                for sent in sentences:\n",
    "                    sent_length = len(sent.split())\n",
    "                    if current_length + sent_length > 300 and current_doc:\n",
    "                        documents.append(' '.join(current_doc))\n",
    "                        current_doc = [sent]\n",
    "                        current_length = sent_length\n",
    "                    else:\n",
    "                        current_doc.append(sent)\n",
    "                        current_length += sent_length\n",
    "\n",
    "                if current_doc and current_length >= 20:\n",
    "                    documents.append(' '.join(current_doc))\n",
    "            else:\n",
    "                documents.append(para)\n",
    "\n",
    "        return documents\n",
    "\n",
    "    def _create_t5_tasks(self, text: str) -> List[Dict]:\n",
    "        \"\"\"Create various T5 tasks from text\"\"\"\n",
    "\n",
    "        tasks = []\n",
    "        sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "        sentences = [s.strip() for s in sentences if len(s.split()) >= 3]\n",
    "\n",
    "        if len(sentences) < 2:\n",
    "            return tasks\n",
    "\n",
    "        # 1. Span Corruption (T5's main pretraining task)\n",
    "        corrupted_task = self._create_span_corruption_task(text)\n",
    "        if corrupted_task:\n",
    "            tasks.append(corrupted_task)\n",
    "\n",
    "        # 2. Summarization\n",
    "        if len(sentences) >= 3:\n",
    "            if len(sentences) <= 5:\n",
    "                summary = sentences[0]\n",
    "            else:\n",
    "                num_summary = max(1, len(sentences) // 3)\n",
    "                summary = ' '.join(sentences[:num_summary])\n",
    "\n",
    "            source_text = f\"summarize: {text}\"\n",
    "            task = self._tokenize_example(source_text, summary, 'summarization')\n",
    "            if task:\n",
    "                tasks.append(task)\n",
    "\n",
    "        # 3. Text Completion\n",
    "        if len(sentences) >= 3:\n",
    "            split_point = len(sentences) // 2\n",
    "            prefix = ' '.join(sentences[:split_point])\n",
    "            completion = ' '.join(sentences[split_point:])\n",
    "\n",
    "            source_text = f\"complete: {prefix}\"\n",
    "            task = self._tokenize_example(source_text, completion, 'completion')\n",
    "            if task:\n",
    "                tasks.append(task)\n",
    "\n",
    "        return tasks\n",
    "\n",
    "    def _create_span_corruption_task(self, text: str) -> Optional[Dict]:\n",
    "        \"\"\"Create T5-style span corruption task\"\"\"\n",
    "\n",
    "        try:\n",
    "            tokens = self.tokenizer.tokenize(text)\n",
    "\n",
    "            if len(tokens) < 10:\n",
    "                return None\n",
    "\n",
    "            # Calculate spans to corrupt\n",
    "            num_tokens_to_corrupt = max(1, int(len(tokens) * self.corruption_probability))\n",
    "            corrupted_tokens = tokens.copy()\n",
    "            target_spans = []\n",
    "            sentinel_count = 0\n",
    "\n",
    "            i = 0\n",
    "            tokens_corrupted = 0\n",
    "\n",
    "            while i < len(corrupted_tokens) and tokens_corrupted < num_tokens_to_corrupt:\n",
    "                if random.random() < 0.3 and tokens_corrupted < num_tokens_to_corrupt:\n",
    "                    # Determine span length (exponential distribution)\n",
    "                    span_length = max(1, min(5, int(random.expovariate(1.0/3.0))))\n",
    "                    span_end = min(i + span_length, len(corrupted_tokens))\n",
    "\n",
    "                    # Extract span\n",
    "                    span_tokens = corrupted_tokens[i:span_end]\n",
    "                    sentinel_token = f'<extra_id_{sentinel_count}>'\n",
    "                    target_spans.append(f'{sentinel_token} {\" \".join(span_tokens)}')\n",
    "\n",
    "                    # Replace in source\n",
    "                    corrupted_tokens[i:span_end] = [sentinel_token]\n",
    "\n",
    "                    tokens_corrupted += len(span_tokens)\n",
    "                    sentinel_count += 1\n",
    "                    i += 1\n",
    "                else:\n",
    "                    i += 1\n",
    "\n",
    "            # Create source and target\n",
    "            source_text = self.tokenizer.convert_tokens_to_string(corrupted_tokens)\n",
    "            target_text = ' '.join(target_spans) + f' <extra_id_{sentinel_count}>'\n",
    "\n",
    "            return self._tokenize_example(source_text, target_text, 'span_corruption')\n",
    "\n",
    "        except Exception as e:\n",
    "            return None\n",
    "\n",
    "    def _tokenize_example(self, source_text: str, target_text: str, task_type: str) -> Optional[Dict]:\n",
    "        \"\"\"Tokenize example with proper error handling\"\"\"\n",
    "\n",
    "        try:\n",
    "            source_text = source_text.strip()\n",
    "            target_text = target_text.strip()\n",
    "\n",
    "            if not source_text or not target_text:\n",
    "                return None\n",
    "\n",
    "            source_encoding = self.tokenizer(\n",
    "                source_text,\n",
    "                max_length=self.max_source_length,\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "\n",
    "            target_encoding = self.tokenizer(\n",
    "                target_text,\n",
    "                max_length=self.max_target_length,\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "\n",
    "            return {\n",
    "                'input_ids': source_encoding['input_ids'].squeeze(0),\n",
    "                'attention_mask': source_encoding['attention_mask'].squeeze(0),\n",
    "                'labels': target_encoding['input_ids'].squeeze(0),\n",
    "                'source_text': source_text,\n",
    "                'target_text': target_text,\n",
    "                'task_type': task_type\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            return None\n",
    "\n",
    "    def _create_curriculum(self):\n",
    "        \"\"\"Create 5-level curriculum based on complexity\"\"\"\n",
    "\n",
    "        print(\"üìö Creating curriculum...\")\n",
    "\n",
    "        # Sort by complexity\n",
    "        complexity_indices = sorted(\n",
    "            range(len(self.examples)),\n",
    "            key=lambda i: self.complexity_scores[i]\n",
    "        )\n",
    "\n",
    "        # Create 5 levels with overlap for smooth transitions\n",
    "        num_levels = 5\n",
    "        level_size = len(complexity_indices) // num_levels\n",
    "        overlap_size = level_size // 4  # 25% overlap\n",
    "\n",
    "        for level in range(num_levels):\n",
    "            start_idx = max(0, level * level_size - overlap_size)\n",
    "            if level == 0:\n",
    "                start_idx = 0\n",
    "\n",
    "            end_idx = min(len(complexity_indices), (level + 1) * level_size + overlap_size)\n",
    "            if level == num_levels - 1:\n",
    "                end_idx = len(complexity_indices)\n",
    "\n",
    "            level_indices = complexity_indices[start_idx:end_idx]\n",
    "            self.curriculum_levels[level] = level_indices\n",
    "\n",
    "            # Statistics\n",
    "            level_complexities = [self.complexity_scores[i] for i in level_indices]\n",
    "            if level_complexities:\n",
    "                print(f\"Level {level}: {len(level_indices)} examples, \"\n",
    "                      f\"complexity: {min(level_complexities):.3f}-{max(level_complexities):.3f}\")\n",
    "\n",
    "    def get_curriculum_level_data(self, level: int) -> List[int]:\n",
    "        \"\"\"Get indices for curriculum level\"\"\"\n",
    "        return self.curriculum_levels.get(level, list(range(len(self.examples))))\n",
    "\n",
    "# =============================================================================\n",
    "# ADAPTIVE CURRICULUM SAMPLER\n",
    "# =============================================================================\n",
    "\n",
    "class AdaptiveCurriculumSampler(Sampler):\n",
    "    \"\"\"Adaptive curriculum sampler that adjusts based on training progress\"\"\"\n",
    "\n",
    "    def __init__(self, dataset: T5CurriculumDataset, current_epoch: int, max_epochs: int,\n",
    "                 strategy: str = 'progressive'):\n",
    "\n",
    "        self.dataset = dataset\n",
    "        self.current_epoch = current_epoch\n",
    "        self.max_epochs = max_epochs\n",
    "        self.strategy = strategy\n",
    "\n",
    "        # Calculate progress\n",
    "        self.progress = min(current_epoch / max(max_epochs - 1, 1), 1.0)\n",
    "\n",
    "        # Determine active levels\n",
    "        self.active_levels = self._get_active_levels()\n",
    "\n",
    "        # Get active indices\n",
    "        self.active_indices = self._get_active_indices()\n",
    "\n",
    "        print(f\"üìö Curriculum Epoch {current_epoch}: Levels {self.active_levels}, \"\n",
    "              f\"{len(self.active_indices)} examples\")\n",
    "\n",
    "    def _get_active_levels(self) -> List[int]:\n",
    "        \"\"\"Determine which curriculum levels to include\"\"\"\n",
    "\n",
    "        if self.strategy == 'progressive':\n",
    "            if self.progress < 0.2:\n",
    "                return [0]\n",
    "            elif self.progress < 0.4:\n",
    "                return [0, 1]\n",
    "            elif self.progress < 0.6:\n",
    "                return [0, 1, 2]\n",
    "            elif self.progress < 0.8:\n",
    "                return [0, 1, 2, 3]\n",
    "            else:\n",
    "                return [0, 1, 2, 3, 4]\n",
    "\n",
    "        elif self.strategy == 'mixed':\n",
    "            # Always include easier levels, gradually add harder ones\n",
    "            levels = [0, 1]\n",
    "            if self.progress > 0.3:\n",
    "                levels.append(2)\n",
    "            if self.progress > 0.6:\n",
    "                levels.append(3)\n",
    "            if self.progress > 0.8:\n",
    "                levels.append(4)\n",
    "            return levels\n",
    "\n",
    "        else:  # uniform\n",
    "            return list(range(5))\n",
    "\n",
    "    def _get_active_indices(self) -> List[int]:\n",
    "        \"\"\"Get indices for active curriculum levels\"\"\"\n",
    "\n",
    "        all_indices = []\n",
    "\n",
    "        # Sample from each active level\n",
    "        for level in self.active_levels:\n",
    "            level_indices = self.dataset.get_curriculum_level_data(level)\n",
    "\n",
    "            # Weight simpler levels more heavily early in training\n",
    "            if len(self.active_levels) > 1:\n",
    "                weight = 1.0 + (len(self.active_levels) - level - 1) * 0.2\n",
    "                sample_size = int(len(level_indices) * weight / sum(\n",
    "                    1.0 + (len(self.active_levels) - l - 1) * 0.2\n",
    "                    for l in self.active_levels\n",
    "                ))\n",
    "                sample_size = min(sample_size, len(level_indices))\n",
    "            else:\n",
    "                sample_size = len(level_indices)\n",
    "\n",
    "            if sample_size > 0:\n",
    "                sampled = random.sample(level_indices, sample_size)\n",
    "                all_indices.extend(sampled)\n",
    "\n",
    "        random.shuffle(all_indices)\n",
    "        return all_indices\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.active_indices)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.active_indices)\n",
    "\n",
    "# =============================================================================\n",
    "# T5 TRAINER WITH CURRICULUM LEARNING\n",
    "# =============================================================================\n",
    "\n",
    "class T5CurriculumTrainer:\n",
    "    \"\"\"T5 trainer with curriculum learning capabilities\"\"\"\n",
    "\n",
    "    def __init__(self, model: T5ForConditionalGeneration, tokenizer: T5Tokenizer,\n",
    "                 save_dir: str, learning_rate: float = 1e-4, weight_decay: float = 0.01):\n",
    "\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.save_dir = save_dir\n",
    "\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        # Optimizer setup\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters()\n",
    "                          if not any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters()\n",
    "                          if any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        self.optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "        self.scheduler = None\n",
    "        self.global_step = 0\n",
    "\n",
    "        # Training metrics\n",
    "        self.training_stats = {\n",
    "            'epoch_losses': [],\n",
    "            'val_losses': [],\n",
    "            'curriculum_levels': []\n",
    "        }\n",
    "\n",
    "        print(\"‚úÖ T5 Curriculum Trainer initialized\")\n",
    "\n",
    "    def train(self, train_dataset: T5CurriculumDataset, val_dataset: Optional[T5CurriculumDataset] = None,\n",
    "              num_epochs: int = 3, batch_size: int = 8, curriculum_strategy: str = 'progressive',\n",
    "              device: str = 'cuda', patience: int = 3):\n",
    "\n",
    "        print(f\"üöÄ Starting T5 curriculum training\")\n",
    "        print(f\"   Epochs: {num_epochs}, Batch size: {batch_size}\")\n",
    "        print(f\"   Strategy: {curriculum_strategy}\")\n",
    "\n",
    "        # Setup scheduler\n",
    "        steps_per_epoch = max(1, len(train_dataset) // batch_size)\n",
    "        total_steps = steps_per_epoch * num_epochs\n",
    "        self.scheduler = get_linear_schedule_with_warmup(\n",
    "            self.optimizer, num_warmup_steps=total_steps // 10, num_training_steps=total_steps\n",
    "        )\n",
    "\n",
    "        # Move model to device\n",
    "        self.model.to(device)\n",
    "\n",
    "        best_val_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(f\"EPOCH {epoch + 1}/{num_epochs}\")\n",
    "            print(f\"{'='*50}\")\n",
    "\n",
    "            # Create curriculum sampler\n",
    "            curriculum_sampler = AdaptiveCurriculumSampler(\n",
    "                train_dataset, epoch, num_epochs, curriculum_strategy\n",
    "            )\n",
    "\n",
    "            # Data loader\n",
    "            train_loader = DataLoader(\n",
    "                train_dataset,\n",
    "                batch_size=batch_size,\n",
    "                sampler=curriculum_sampler,\n",
    "                collate_fn=self._collate_fn,\n",
    "                num_workers=0\n",
    "            )\n",
    "\n",
    "            # Train epoch\n",
    "            epoch_loss = self._train_epoch(train_loader, device)\n",
    "\n",
    "            # Validation\n",
    "            val_loss = None\n",
    "            if val_dataset:\n",
    "                val_loss = self._evaluate(val_dataset, device, batch_size)\n",
    "\n",
    "                # Early stopping\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    patience_counter = 0\n",
    "                    self._save_checkpoint(epoch, is_best=True)\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "\n",
    "                if patience_counter >= patience:\n",
    "                    print(f\"üõë Early stopping at epoch {epoch + 1}\")\n",
    "                    break\n",
    "\n",
    "            # Save stats\n",
    "            self.training_stats['epoch_losses'].append(epoch_loss)\n",
    "            self.training_stats['val_losses'].append(val_loss or 0)\n",
    "            self.training_stats['curriculum_levels'].append(curriculum_sampler.active_levels)\n",
    "\n",
    "            print(f\"üìä Epoch {epoch + 1} - Train Loss: {epoch_loss:.4f}\")\n",
    "            if val_loss:\n",
    "                print(f\"üìä Epoch {epoch + 1} - Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Save final model\n",
    "        self._save_final_model()\n",
    "        return self.training_stats\n",
    "\n",
    "    def _train_epoch(self, train_loader: DataLoader, device: str) -> float:\n",
    "        \"\"\"Train one epoch\"\"\"\n",
    "\n",
    "        self.model.train()\n",
    "        epoch_loss = 0.0\n",
    "        num_batches = 0\n",
    "\n",
    "        progress_bar = tqdm(train_loader, desc='Training')\n",
    "\n",
    "        for batch in progress_bar:\n",
    "            # Move to device\n",
    "            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v\n",
    "                    for k, v in batch.items()}\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = self.model(\n",
    "                input_ids=batch['input_ids'],\n",
    "                attention_mask=batch['attention_mask'],\n",
    "                labels=batch['labels']\n",
    "            )\n",
    "\n",
    "            loss = outputs.loss\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "\n",
    "            self.optimizer.step()\n",
    "            self.scheduler.step()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            # Update metrics\n",
    "            epoch_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            self.global_step += 1\n",
    "\n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({\n",
    "                'Loss': f'{loss.item():.4f}',\n",
    "                'Avg': f'{epoch_loss/num_batches:.4f}'\n",
    "            })\n",
    "\n",
    "        return epoch_loss / max(num_batches, 1)\n",
    "\n",
    "    def _evaluate(self, val_dataset: T5CurriculumDataset, device: str, batch_size: int = 8) -> float:\n",
    "        \"\"\"Evaluate model on validation set\"\"\"\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "        # Sample validation examples\n",
    "        val_indices = list(range(min(500, len(val_dataset))))\n",
    "        random.shuffle(val_indices)\n",
    "\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=batch_size,\n",
    "            sampler=val_indices,\n",
    "            collate_fn=self._collate_fn\n",
    "        )\n",
    "\n",
    "        total_loss = 0.0\n",
    "        num_batches = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc='Validation', leave=False):\n",
    "                batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v\n",
    "                        for k, v in batch.items()}\n",
    "\n",
    "                outputs = self.model(\n",
    "                    input_ids=batch['input_ids'],\n",
    "                    attention_mask=batch['attention_mask'],\n",
    "                    labels=batch['labels']\n",
    "                )\n",
    "\n",
    "                total_loss += outputs.loss.item()\n",
    "                num_batches += 1\n",
    "\n",
    "        return total_loss / max(num_batches, 1)\n",
    "\n",
    "    def _collate_fn(self, batch):\n",
    "        \"\"\"Collate function for T5 batch processing\"\"\"\n",
    "\n",
    "        input_ids = torch.stack([item['input_ids'] for item in batch])\n",
    "        attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
    "        labels = torch.stack([item['labels'] for item in batch])\n",
    "\n",
    "        # Mask padding tokens in labels\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels,\n",
    "            'task_types': [item['task_type'] for item in batch]\n",
    "        }\n",
    "\n",
    "    def _save_checkpoint(self, epoch: int, is_best: bool = False):\n",
    "        \"\"\"Save model checkpoint\"\"\"\n",
    "\n",
    "        checkpoint_dir = os.path.join(self.save_dir, f'checkpoint-epoch-{epoch}')\n",
    "        if is_best:\n",
    "            checkpoint_dir = os.path.join(self.save_dir, 'best_model')\n",
    "\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "        self.model.save_pretrained(checkpoint_dir)\n",
    "        self.tokenizer.save_pretrained(checkpoint_dir)\n",
    "\n",
    "        # Save training state\n",
    "        training_state = {\n",
    "            'epoch': epoch,\n",
    "            'global_step': self.global_step,\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'scheduler_state_dict': self.scheduler.state_dict() if self.scheduler else None,\n",
    "            'training_stats': self.training_stats\n",
    "        }\n",
    "\n",
    "        torch.save(training_state, os.path.join(checkpoint_dir, 'training_state.pt'))\n",
    "\n",
    "    def _save_final_model(self):\n",
    "        \"\"\"Save final model\"\"\"\n",
    "\n",
    "        final_dir = os.path.join(self.save_dir, 'final_model')\n",
    "        os.makedirs(final_dir, exist_ok=True)\n",
    "\n",
    "        self.model.save_pretrained(final_dir)\n",
    "        self.tokenizer.save_pretrained(final_dir)\n",
    "\n",
    "        # Save training stats\n",
    "        with open(os.path.join(final_dir, 'training_stats.json'), 'w') as f:\n",
    "            json.dump(self.training_stats, f, indent=2)\n",
    "\n",
    "        print(f\"üèÜ Final model saved to {final_dir}\")\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION AND MAIN TRAINING FUNCTION\n",
    "# =============================================================================\n",
    "\n",
    "class TrainingConfig:\n",
    "    \"\"\"Training configuration\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Model config\n",
    "        self.model_name = 't5-small'\n",
    "        self.max_source_length = 512\n",
    "        self.max_target_length = 256\n",
    "\n",
    "        # Training config\n",
    "        self.num_epochs = 5\n",
    "        self.batch_size = 4\n",
    "        self.learning_rate = 1e-4\n",
    "        self.weight_decay = 0.01\n",
    "\n",
    "        # Curriculum config\n",
    "        self.curriculum_strategy = 'progressive'  # 'progressive', 'mixed', 'uniform'\n",
    "        self.corruption_probability = 0.15\n",
    "\n",
    "        # Data config\n",
    "        self.max_train_examples = None  # No limit\n",
    "        self.max_val_examples = None\n",
    "        self.patience = 3\n",
    "\n",
    "        # Paths - UPDATE THESE FOR YOUR SETUP\n",
    "        self.data_dir = '/content/baseline-pretraining/babylm_data'\n",
    "        self.save_dir = '/content/drive/MyDrive/llm-project/t5-small-new-base_datapreparation/t5_curriculum_training'\n",
    "        self.cache_dir = '/content/drive/MyDrive/llm-project/t5-small-new-base_datapreparation/full_t5_curriculum_cache'\n",
    "\n",
    "        # Hardware\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "def create_datasets(config: TrainingConfig):\n",
    "    \"\"\"Create training and validation datasets\"\"\"\n",
    "\n",
    "    print(\"üîß Creating datasets...\")\n",
    "\n",
    "    # Initialize components\n",
    "    tokenizer = T5Tokenizer.from_pretrained(config.model_name)\n",
    "\n",
    "    # Initialize hybrid complexity analyzer\n",
    "    complexity_analyzer = HybridComplexityAnalyzer(\n",
    "        tokenizer=tokenizer,\n",
    "        save_dir=os.path.join(config.cache_dir, 'complexity_models'),\n",
    "        learning_mode=True  # Enable learning from data\n",
    "    )\n",
    "\n",
    "    datasets = {}\n",
    "\n",
    "    # Dataset files\n",
    "    file_mapping = {\n",
    "        'train': 'babylm_train.txt',\n",
    "        'dev': 'babylm_dev.txt'\n",
    "    }\n",
    "\n",
    "    for split, filename in file_mapping.items():\n",
    "        filepath = os.path.join(config.data_dir, filename)\n",
    "\n",
    "        if not os.path.exists(filepath):\n",
    "            print(f\"‚ö†Ô∏è File not found: {filepath}\")\n",
    "            continue\n",
    "\n",
    "        max_examples = (config.max_train_examples if split == 'train'\n",
    "                       else config.max_val_examples)\n",
    "\n",
    "        dataset = T5CurriculumDataset(\n",
    "            data_path=filepath,\n",
    "            tokenizer=tokenizer,\n",
    "            complexity_analyzer=complexity_analyzer,\n",
    "            max_source_length=config.max_source_length,\n",
    "            max_target_length=config.max_target_length,\n",
    "            cache_dir=os.path.join(config.cache_dir, 'dataset_cache'),\n",
    "            split=split,\n",
    "            max_examples=max_examples,\n",
    "            corruption_probability=config.corruption_probability\n",
    "        )\n",
    "\n",
    "        if len(dataset) > 0:\n",
    "            datasets[split] = dataset\n",
    "            print(f\"‚úÖ Created {split} dataset: {len(dataset)} examples\")\n",
    "\n",
    "    return datasets, tokenizer, complexity_analyzer\n",
    "\n",
    "def train_t5_with_curriculum(config: TrainingConfig = None):\n",
    "    \"\"\"Main training function\"\"\"\n",
    "\n",
    "    if config is None:\n",
    "        config = TrainingConfig()\n",
    "\n",
    "    print(f\"üöÄ T5 Curriculum Training\")\n",
    "    print(f\"   Model: {config.model_name}\")\n",
    "    print(f\"   Device: {config.device}\")\n",
    "    print(f\"   Strategy: {config.curriculum_strategy}\")\n",
    "\n",
    "    # Create directories\n",
    "    os.makedirs(config.save_dir, exist_ok=True)\n",
    "    os.makedirs(config.cache_dir, exist_ok=True)\n",
    "\n",
    "    # Load model and create datasets BEFORE creating the model\n",
    "    print(\"üìä Creating datasets...\")\n",
    "    datasets, tokenizer, complexity_analyzer = create_datasets(config)\n",
    "\n",
    "    # Now create the model with random weights\n",
    "    print(\"üî§ Loading model...\")\n",
    "    model_config = T5Config.from_pretrained(\"t5-small\")  # Get T5-small architecture\n",
    "    model = T5ForConditionalGeneration(model_config)  # Initialize with random weights\n",
    "\n",
    "    if 'train' not in datasets:\n",
    "        raise ValueError(\"‚ùå No training dataset available!\")\n",
    "\n",
    "    print(f\"üìä Dataset sizes:\")\n",
    "    for split, dataset in datasets.items():\n",
    "        print(f\"   {split}: {len(dataset)} examples\")\n",
    "\n",
    "    # Initialize trainer\n",
    "    trainer = T5CurriculumTrainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        save_dir=config.save_dir,\n",
    "        learning_rate=config.learning_rate,\n",
    "        weight_decay=config.weight_decay\n",
    "    )\n",
    "\n",
    "    # Train\n",
    "    training_stats = trainer.train(\n",
    "        train_dataset=datasets['train'],\n",
    "        val_dataset=datasets.get('dev'),\n",
    "        num_epochs=config.num_epochs,\n",
    "        batch_size=config.batch_size,\n",
    "        curriculum_strategy=config.curriculum_strategy,\n",
    "        device=config.device,\n",
    "        patience=config.patience\n",
    "    )\n",
    "\n",
    "    print(\"üèÅ Training completed!\")\n",
    "\n",
    "    return {\n",
    "        'model': model,\n",
    "        'tokenizer': tokenizer,\n",
    "        'complexity_analyzer': complexity_analyzer,\n",
    "        'training_stats': training_stats,\n",
    "        'config': config\n",
    "    }\n",
    "\n",
    "# =============================================================================\n",
    "# EXAMPLE USAGE\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üéØ T5 Curriculum Learning with Hybrid Complexity Analysis\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Create custom config\n",
    "    config = TrainingConfig()\n",
    "\n",
    "    # Customize paths for your environment\n",
    "    config.data_dir = '/content/baseline-pretraining/babylm_data'\n",
    "    config.save_dir = '/content/drive/MyDrive/llm-project/t5-small-new-base_datapreparation/t5_curriculum_training'\n",
    "    config.cache_dir = '/content/drive/MyDrive/llm-project/t5-small-new-base_datapreparation/full_t5_curriculum_cache'\n",
    "\n",
    "    # Training parameters\n",
    "    config.num_epochs = 5  # Reduced for testing\n",
    "    config.batch_size = 4  # Adjust based on your GPU memory\n",
    "    config.max_train_examples = None  # Limit for testing\n",
    "    config.max_val_examples = 5000\n",
    "\n",
    "    try:\n",
    "        # Run training\n",
    "        results = train_t5_with_curriculum(config)\n",
    "\n",
    "        print(\"\\n‚úÖ Training completed successfully!\")\n",
    "        print(f\"üìÅ Model saved to: {config.save_dir}\")\n",
    "\n",
    "        # Print some statistics\n",
    "        stats = results['training_stats']\n",
    "        print(f\"\\nüìä Training Statistics:\")\n",
    "        print(f\"   Final train loss: {stats['epoch_losses'][-1]:.4f}\")\n",
    "        if stats['val_losses'] and stats['val_losses'][-1] > 0:\n",
    "            print(f\"   Final val loss: {stats['val_losses'][-1]:.4f}\")\n",
    "        print(f\"   Curriculum progression: {stats['curriculum_levels']}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Training failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
