{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "!pip uninstall -y numpy transformers datasets\n",
    "!pip install numpy --force-reinstall --no-cache-dir\n",
    "!pip install transformers datasets --force-reinstall --no-cache-dir\n",
    "\n",
    "\n",
    "os.kill(os.getpid(), 9)  # Restart the Colab runtime (REQUIRED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/babylm/baseline-pretraining.git\n",
    "%cd baseline-pretraining\n",
    "!wget -O babylm_data.zip \"https://files.osf.io/v1/resources/ad7qg/providers/osfstorage/661517db943bee3731dfec25/?zip=\"\n",
    "!unzip babylm_data.zip -d babylm_data\n",
    "!unzip babylm_data/train_10M.zip -d babylm_data/train_10M\n",
    "!unzip babylm_data/dev.zip -d babylm_data/dev\n",
    "!unzip babylm_data/test.zip -d babylm_data/test\n",
    "!cat babylm_data/train_10M/train_10M/*.train > babylm_data/babylm_train.txt\n",
    "!cat babylm_data/dev/dev/*.dev > babylm_data/babylm_dev.txt\n",
    "!cat babylm_data/test/test/*.test > babylm_data/babylm_test.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# t5-small with modification of first-layer-embedding of words and token with Key Improvements in the Enhanced Implementation:\n",
    "\n",
    "### Advanced VT Embedding Architecture:\n",
    "\n",
    "Adaptive Delta Weights: Instead of fixed delta weights, the model now learns optimal weights based on token types, positions, and context\n",
    "\n",
    "Improved Boundary Handling: Uses average embeddings instead of zeros for the first token, reducing artificial discontinuities\n",
    "\n",
    "Layer-Specific Application: Can selectively apply VT to encoder/decoder\n",
    "\n",
    "Warmup Mechanism: Gradually increases delta influence during training\n",
    "\n",
    "I've provided you with a complete, well-organized VT T5 curriculum training code. Here are the key improvements and features:\n",
    "\n",
    "## üöÄ **Key Features**\n",
    "\n",
    "### **1. Complete Self-Contained Code**\n",
    "- No external imports from other files\n",
    "- All classes and functions included in one file\n",
    "- Ready to run directly\n",
    "\n",
    "### **2. VT (Vector Transition) Embeddings**\n",
    "- Applied only to the **first layer embeddings** (word/token level)\n",
    "- Adaptive delta weights based on token properties\n",
    "- Improved boundary handling\n",
    "- Memory-optimized operations\n",
    "\n",
    "### **3. Curriculum Learning Pipeline**\n",
    "- **5 complexity levels** (0=easiest, 4=hardest)\n",
    "- Progressive strategy: gradually introduces harder examples\n",
    "- Hybrid complexity analyzer with rule-based + data-driven features\n",
    "\n",
    "### **4. Training Tasks**\n",
    "- **Span Corruption** (T5's main pretraining task)\n",
    "- **Summarization**\n",
    "- **Text Completion**\n",
    "\n",
    "### **5. Performance Optimizations**\n",
    "- Colab T4 GPU optimizations\n",
    "- Efficient data loading and caching\n",
    "- Memory management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Complete VT T5 with Vector Transition Embeddings + Curriculum Learning\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from typing import Dict, List, Tuple, Optional, Union\n",
    "from collections import Counter, defaultdict\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, T5Config\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import math\n",
    "from scipy.stats import entropy\n",
    "from scipy.spatial.distance import cosine\n",
    "import networkx as nx\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import KMeans\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from inspect import signature\n",
    "\n",
    "\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_random_seeds(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_random_seeds(42)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION CLASS\n",
    "# =============================================================================\n",
    "class VTCurriculumConfig:\n",
    "    \"\"\"Training configuration for VT models with curriculum learning\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Model config\n",
    "        self.model_name = 't5-small'\n",
    "        self.max_source_length = 512\n",
    "        self.max_target_length = 256\n",
    "\n",
    "        # VT-specific config\n",
    "        self.delta_weight = 0.4\n",
    "        self.apply_to_encoder = True\n",
    "        self.apply_to_decoder = True\n",
    "        self.adaptive_delta = True\n",
    "        self.warmup_steps = 1000\n",
    "\n",
    "        # Training config\n",
    "        self.num_epochs = 5\n",
    "        self.batch_size = 4\n",
    "        self.learning_rate = 1e-4\n",
    "        self.weight_decay = 0.01\n",
    "\n",
    "        # Curriculum config\n",
    "        self.curriculum_strategy = 'progressive'  # 'progressive', 'mixed', 'uniform'\n",
    "        self.corruption_probability = 0.15\n",
    "\n",
    "        # Data config\n",
    "        self.max_train_examples = 30000\n",
    "        self.max_val_examples = 2000\n",
    "        self.patience = 3\n",
    "\n",
    "        # FIXED: Use self. instead of config.\n",
    "        self.data_dir = '/content/baseline-pretraining/babylm_data'\n",
    "        self.save_dir = '/content/drive/MyDrive/llm-project/t5-small-new-base_datapreparation/vt_t5_curriculum_training_first_layer_modification'\n",
    "        self.cache_dir = '/content/drive/MyDrive/llm-project/t5-small-new-base_datapreparation/vt_t5_curriculum_cache'\n",
    "\n",
    "        # Hardware\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "        # Speed optimizations for Colab T4\n",
    "        self.use_fp16 = False  # Set to True if you want mixed precision\n",
    "        self.gradient_accumulation_steps = 2\n",
    "        self.pin_memory = True\n",
    "        self.num_workers = 0\n",
    "# =============================================================================\n",
    "# HYBRID COMPLEXITY ANALYZER\n",
    "# =============================================================================\n",
    "\n",
    "class HybridComplexityAnalyzer:\n",
    "    \"\"\"Hybrid complexity analyzer that combines rule-based and data-driven approaches\"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer: T5Tokenizer, save_dir: str, learning_mode: bool = True):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.save_dir = save_dir\n",
    "        self.learning_mode = learning_mode\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        # Rule-based patterns\n",
    "        self.linguistic_patterns = {\n",
    "            'subordinate_clauses': r'\\b(although|because|since|while|whereas|if|unless|when|after|before|until|once|provided|given|assuming)\\b',\n",
    "            'relative_clauses': r'\\b(which|that|who|whom|whose|where|when|why)\\s+\\w+',\n",
    "            'passive_constructions': r'\\b(was|were|is|are|been|being)\\s+\\w*ed\\b|\\b\\w+\\s+(was|were|is|are)\\s+\\w*ed\\b',\n",
    "            'complex_verb_forms': r'\\b(have|has|had|will|would|could|should|might|must)\\s+(been|have|had)\\b',\n",
    "            'discourse_markers': r'\\b(furthermore|moreover|additionally|however|nevertheless|nonetheless|consequently|therefore|thus|hence)\\b',\n",
    "            'abstract_concepts': r'\\b\\w+(tion|sion|ness|ment|ity|ism|ance|ence|ship|hood|dom|age)\\b',\n",
    "            'modal_expressions': r'\\b(possibly|probably|certainly|definitely|presumably|apparently|obviously)\\b',\n",
    "        }\n",
    "\n",
    "        # Academic word list\n",
    "        self.academic_words = {\n",
    "            'analysis', 'approach', 'area', 'assessment', 'assume', 'authority',\n",
    "            'available', 'benefit', 'concept', 'consistent', 'constitutional',\n",
    "            'context', 'contract', 'create', 'data', 'definition', 'derived',\n",
    "            'distribution', 'economic', 'environment', 'established', 'estimate',\n",
    "            'evidence', 'export', 'factors', 'financial', 'formula', 'function',\n",
    "            'identified', 'income', 'indicate', 'individual', 'interpretation',\n",
    "            'involved', 'issues', 'labor', 'legal', 'legislation', 'major',\n",
    "            'method', 'occur', 'percent', 'period', 'policy', 'principle',\n",
    "            'procedure', 'process', 'required', 'research', 'response', 'role',\n",
    "            'section', 'significant', 'similar', 'source', 'specific', 'structure',\n",
    "            'theory', 'variables'\n",
    "        }\n",
    "\n",
    "        # Data-driven components\n",
    "        self.word_complexity_scores = {}\n",
    "        self.semantic_features = {}\n",
    "        self.syntactic_patterns = {}\n",
    "        self.tfidf_vectorizer = None\n",
    "        self.complexity_clusters = {}\n",
    "        self.is_trained = False\n",
    "\n",
    "        # Load or initialize\n",
    "        self._initialize_components()\n",
    "\n",
    "    def _initialize_components(self):\n",
    "        \"\"\"Initialize or load pre-trained components\"\"\"\n",
    "        models_file = os.path.join(self.save_dir, 'hybrid_complexity_models.pkl')\n",
    "\n",
    "        if os.path.exists(models_file) and not self.learning_mode:\n",
    "            print(\"üì¶ Loading pre-trained hybrid complexity models...\")\n",
    "            try:\n",
    "                with open(models_file, 'rb') as f:\n",
    "                    saved_models = pickle.load(f)\n",
    "                    self.word_complexity_scores = saved_models.get('word_complexity_scores', {})\n",
    "                    self.semantic_features = saved_models.get('semantic_features', {})\n",
    "                    self.syntactic_patterns = saved_models.get('syntactic_patterns', {})\n",
    "                    self.tfidf_vectorizer = saved_models.get('tfidf_vectorizer', None)\n",
    "                    self.complexity_clusters = saved_models.get('complexity_clusters', {})\n",
    "                    self.is_trained = True\n",
    "                print(\"‚úÖ Loaded pre-trained models\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Failed to load models: {e}\")\n",
    "\n",
    "        # Load basic frequency data\n",
    "        self.word_frequencies = self._load_frequency_dict()\n",
    "\n",
    "    def _load_frequency_dict(self) -> Dict[str, int]:\n",
    "        \"\"\"Load word frequency dictionary\"\"\"\n",
    "        freq_file = os.path.join(self.save_dir, 'word_frequencies.pkl')\n",
    "\n",
    "        if os.path.exists(freq_file):\n",
    "            try:\n",
    "                with open(freq_file, 'rb') as f:\n",
    "                    return pickle.load(f)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        # Create basic frequency mapping\n",
    "        frequencies = {}\n",
    "\n",
    "        # High frequency words (low complexity)\n",
    "        high_freq = ['the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to',\n",
    "                    'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be',\n",
    "                    'been', 'being', 'have', 'has', 'had', 'do', 'does', 'did',\n",
    "                    'will', 'would', 'can', 'could', 'should', 'may', 'might',\n",
    "                    'I', 'you', 'he', 'she', 'it', 'we', 'they', 'this', 'that',\n",
    "                    'time', 'person', 'year', 'way', 'day', 'thing', 'man', 'world',\n",
    "                    'life', 'hand', 'part', 'child', 'eye', 'woman', 'place', 'work']\n",
    "\n",
    "        for i, word in enumerate(high_freq):\n",
    "            frequencies[word.lower()] = i + 1\n",
    "\n",
    "        # Medium frequency words\n",
    "        medium_freq = ['analysis', 'approach', 'structure', 'process', 'development',\n",
    "                      'environment', 'significant', 'individual', 'particular',\n",
    "                      'available', 'information', 'community', 'economic', 'political']\n",
    "\n",
    "        for i, word in enumerate(medium_freq):\n",
    "            frequencies[word.lower()] = 1000 + i * 50\n",
    "\n",
    "        # Save frequency data\n",
    "        try:\n",
    "            with open(freq_file, 'wb') as f:\n",
    "                pickle.dump(frequencies, f)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        return frequencies\n",
    "\n",
    "    def learn_from_data(self, texts: List[str], complexity_scores: List[float] = None):\n",
    "        \"\"\"Learn complexity patterns from training data\"\"\"\n",
    "        if not texts:\n",
    "            print(\"‚ö†Ô∏è No training texts provided\")\n",
    "            return\n",
    "\n",
    "        print(\"üß† Learning complexity patterns from data...\")\n",
    "\n",
    "        # If no complexity scores provided, calculate basic ones\n",
    "        if complexity_scores is None:\n",
    "            complexity_scores = [self._calculate_basic_complexity(text) for text in texts]\n",
    "\n",
    "        # 1. Learn word-level complexity\n",
    "        self._learn_word_complexity(texts, complexity_scores)\n",
    "\n",
    "        # 2. Learn semantic patterns using TF-IDF\n",
    "        self._learn_semantic_patterns(texts, complexity_scores)\n",
    "\n",
    "        # 3. Learn syntactic pattern correlations\n",
    "        self._learn_syntactic_correlations(texts, complexity_scores)\n",
    "\n",
    "        # Save learned models\n",
    "        self._save_models()\n",
    "        self.is_trained = True\n",
    "        print(\"‚úÖ Learning completed!\")\n",
    "\n",
    "    def _learn_word_complexity(self, texts: List[str], complexity_scores: List[float]):\n",
    "        \"\"\"Learn word-level complexity associations\"\"\"\n",
    "        print(\"üìä Learning word-level complexity...\")\n",
    "        word_stats = defaultdict(list)\n",
    "\n",
    "        for text, complexity in zip(texts, complexity_scores):\n",
    "            words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "            for word in words:\n",
    "                word_stats[word].append(complexity)\n",
    "\n",
    "        # Calculate complexity scores for words\n",
    "        for word, scores in word_stats.items():\n",
    "            if len(scores) >= 3:  # Minimum occurrences\n",
    "                avg_complexity = np.mean(scores)\n",
    "                frequency = self.word_frequencies.get(word, 10000)  # Default high frequency\n",
    "\n",
    "                # Combine average complexity with rarity\n",
    "                complexity_score = (\n",
    "                    avg_complexity * 0.7 +\n",
    "                    min(frequency / 10000.0, 1.0) * 0.3  # Rarity factor\n",
    "                )\n",
    "\n",
    "                self.word_complexity_scores[word] = {\n",
    "                    'score': complexity_score,\n",
    "                    'frequency': len(scores),\n",
    "                    'avg_complexity': avg_complexity\n",
    "                }\n",
    "\n",
    "    def _learn_semantic_patterns(self, texts: List[str], complexity_scores: List[float]):\n",
    "        \"\"\"Learn semantic complexity patterns using TF-IDF\"\"\"\n",
    "        print(\"üéØ Learning semantic patterns...\")\n",
    "\n",
    "        try:\n",
    "            # Create TF-IDF vectorizer\n",
    "            self.tfidf_vectorizer = TfidfVectorizer(\n",
    "                max_features=5000,\n",
    "                ngram_range=(1, 2),\n",
    "                stop_words='english',\n",
    "                min_df=2,\n",
    "                max_df=0.95\n",
    "            )\n",
    "\n",
    "            tfidf_matrix = self.tfidf_vectorizer.fit_transform(texts)\n",
    "            feature_names = self.tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "            # Calculate correlation between features and complexity\n",
    "            feature_complexities = {}\n",
    "\n",
    "            for i, feature in enumerate(feature_names):\n",
    "                feature_scores = tfidf_matrix[:, i].toarray().flatten()\n",
    "                if np.std(feature_scores) > 0:\n",
    "                    correlation = np.corrcoef(feature_scores, complexity_scores)[0, 1]\n",
    "                    if not np.isnan(correlation):\n",
    "                        feature_complexities[feature] = correlation\n",
    "\n",
    "            # Store top complexity-correlated features\n",
    "            sorted_features = sorted(feature_complexities.items(),\n",
    "                                   key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "            self.semantic_features = {\n",
    "                'high_complexity': [f for f, c in sorted_features[:200] if c > 0],\n",
    "                'low_complexity': [f for f, c in sorted_features[-200:] if c < 0],\n",
    "                'correlations': feature_complexities\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è TF-IDF learning failed: {e}\")\n",
    "            self.semantic_features = {'high_complexity': [], 'low_complexity': [], 'correlations': {}}\n",
    "\n",
    "    def _learn_syntactic_correlations(self, texts: List[str], complexity_scores: List[float]):\n",
    "        \"\"\"Learn correlations between syntactic patterns and complexity\"\"\"\n",
    "        print(\"üîç Learning syntactic correlations...\")\n",
    "        pattern_correlations = {}\n",
    "\n",
    "        for pattern_name, pattern_regex in self.linguistic_patterns.items():\n",
    "            pattern_densities = []\n",
    "\n",
    "            for text in texts:\n",
    "                words = re.findall(r'\\b\\w+\\b', text)\n",
    "                matches = len(re.findall(pattern_regex, text, re.IGNORECASE))\n",
    "                density = matches / max(len(words), 1)\n",
    "                pattern_densities.append(density)\n",
    "\n",
    "            # Calculate correlation with complexity\n",
    "            if np.std(pattern_densities) > 0:\n",
    "                correlation = np.corrcoef(pattern_densities, complexity_scores)[0, 1]\n",
    "                if not np.isnan(correlation):\n",
    "                    pattern_correlations[pattern_name] = {\n",
    "                        'correlation': correlation,\n",
    "                        'avg_density': np.mean(pattern_densities)\n",
    "                    }\n",
    "\n",
    "        self.syntactic_patterns = pattern_correlations\n",
    "\n",
    "    def analyze_text_complexity(self, text: str) -> Dict[str, float]:\n",
    "        \"\"\"Main method to analyze text complexity using hybrid approach\"\"\"\n",
    "        if not text or len(text.strip()) < 10:\n",
    "            return {'overall_complexity': 0.0}\n",
    "\n",
    "        words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "        sentences = re.split(r'[.!?]+', text)\n",
    "        sentences = [s.strip() for s in sentences if s.strip()]\n",
    "\n",
    "        if not words or not sentences:\n",
    "            return {'overall_complexity': 0.0}\n",
    "\n",
    "        features = {}\n",
    "\n",
    "        # 1. Rule-based features\n",
    "        features.update(self._calculate_rule_based_features(text, words, sentences))\n",
    "\n",
    "        # 2. Data-driven features\n",
    "        features.update(self._calculate_data_driven_features(text, words))\n",
    "\n",
    "        # 3. Semantic features\n",
    "        features.update(self._calculate_semantic_features(text))\n",
    "\n",
    "        # 4. Overall complexity calculation\n",
    "        features['overall_complexity'] = self._calculate_overall_complexity(features)\n",
    "\n",
    "        return features\n",
    "\n",
    "    def _calculate_rule_based_features(self, text: str, words: List[str], sentences: List[str]) -> Dict[str, float]:\n",
    "        \"\"\"Calculate rule-based linguistic features\"\"\"\n",
    "        features = {}\n",
    "\n",
    "        # Lexical diversity\n",
    "        unique_words = len(set(words))\n",
    "        ttr = unique_words / len(words)\n",
    "        corrected_ttr = unique_words / math.sqrt(2 * len(words))\n",
    "        features['lexical_diversity'] = min(corrected_ttr, 1.0)\n",
    "\n",
    "        # Morphological complexity\n",
    "        avg_word_length = sum(len(word) for word in words) / len(words)\n",
    "        features['morphological_complexity'] = min((avg_word_length - 3) / 6, 1.0)\n",
    "\n",
    "        # Sentence complexity\n",
    "        sent_lengths = [len(re.findall(r'\\b\\w+\\b', s)) for s in sentences]\n",
    "        avg_sent_length = sum(sent_lengths) / len(sent_lengths)\n",
    "        features['sentence_length'] = min((avg_sent_length - 8) / 25, 1.0)\n",
    "\n",
    "        # Syntactic pattern density\n",
    "        syntactic_complexity = 0.0\n",
    "        for pattern_name, pattern_regex in self.linguistic_patterns.items():\n",
    "            matches = len(re.findall(pattern_regex, text, re.IGNORECASE))\n",
    "            density = matches / len(words)\n",
    "\n",
    "            # Weight by learned correlation if available\n",
    "            if self.is_trained and pattern_name in self.syntactic_patterns:\n",
    "                weight = abs(self.syntactic_patterns[pattern_name]['correlation'])\n",
    "            else:\n",
    "                weight = 1.0\n",
    "\n",
    "            syntactic_complexity += density * weight\n",
    "\n",
    "        features['syntactic_complexity'] = min(syntactic_complexity, 1.0)\n",
    "\n",
    "        # Academic word density\n",
    "        academic_count = sum(1 for word in words if word in self.academic_words)\n",
    "        features['academic_density'] = min(academic_count / len(words) * 10, 1.0)\n",
    "\n",
    "        return features\n",
    "\n",
    "    def _calculate_data_driven_features(self, text: str, words: List[str]) -> Dict[str, float]:\n",
    "        \"\"\"Calculate data-driven features\"\"\"\n",
    "        features = {}\n",
    "\n",
    "        if not self.is_trained:\n",
    "            features['learned_word_complexity'] = 0.5\n",
    "            features['rare_word_density'] = 0.5\n",
    "            return features\n",
    "\n",
    "        # Learned word complexity\n",
    "        word_complexities = []\n",
    "        rare_word_count = 0\n",
    "\n",
    "        for word in words:\n",
    "            if word in self.word_complexity_scores:\n",
    "                word_complexities.append(self.word_complexity_scores[word]['score'])\n",
    "            elif word not in self.word_frequencies or self.word_frequencies[word] > 5000:\n",
    "                rare_word_count += 1\n",
    "\n",
    "        if word_complexities:\n",
    "            features['learned_word_complexity'] = min(np.mean(word_complexities), 1.0)\n",
    "        else:\n",
    "            features['learned_word_complexity'] = 0.5\n",
    "\n",
    "        features['rare_word_density'] = min(rare_word_count / len(words) * 5, 1.0)\n",
    "\n",
    "        return features\n",
    "\n",
    "    def _calculate_semantic_features(self, text: str) -> Dict[str, float]:\n",
    "        \"\"\"Calculate semantic complexity features\"\"\"\n",
    "        features = {}\n",
    "\n",
    "        if not self.is_trained or not self.tfidf_vectorizer:\n",
    "            features['semantic_complexity'] = 0.5\n",
    "            return features\n",
    "\n",
    "        try:\n",
    "            # TF-IDF based semantic complexity\n",
    "            tfidf_vector = self.tfidf_vectorizer.transform([text])\n",
    "            feature_names = self.tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "            semantic_score = 0.0\n",
    "            total_weight = 0.0\n",
    "\n",
    "            for i, feature in enumerate(feature_names):\n",
    "                weight = tfidf_vector[0, i]\n",
    "                if weight > 0 and feature in self.semantic_features['correlations']:\n",
    "                    correlation = self.semantic_features['correlations'][feature]\n",
    "                    semantic_score += weight * max(correlation, 0)  # Only positive correlations\n",
    "                    total_weight += weight\n",
    "\n",
    "            if total_weight > 0:\n",
    "                features['semantic_complexity'] = min(semantic_score / total_weight, 1.0)\n",
    "            else:\n",
    "                features['semantic_complexity'] = 0.5\n",
    "\n",
    "        except Exception as e:\n",
    "            features['semantic_complexity'] = 0.5\n",
    "\n",
    "        return features\n",
    "\n",
    "    def _calculate_overall_complexity(self, features: Dict[str, float]) -> float:\n",
    "        \"\"\"Calculate overall complexity from all features\"\"\"\n",
    "\n",
    "        # Adaptive weights based on whether system is trained\n",
    "        if self.is_trained:\n",
    "            weights = {\n",
    "                'lexical_diversity': 0.12,\n",
    "                'morphological_complexity': 0.08,\n",
    "                'sentence_length': 0.10,\n",
    "                'syntactic_complexity': 0.15,\n",
    "                'academic_density': 0.08,\n",
    "                'learned_word_complexity': 0.20,\n",
    "                'rare_word_density': 0.10,\n",
    "                'semantic_complexity': 0.17\n",
    "            }\n",
    "        else:\n",
    "            # Rule-based weights when not trained\n",
    "            weights = {\n",
    "                'lexical_diversity': 0.20,\n",
    "                'morphological_complexity': 0.15,\n",
    "                'sentence_length': 0.15,\n",
    "                'syntactic_complexity': 0.25,\n",
    "                'academic_density': 0.15,\n",
    "                'learned_word_complexity': 0.05,\n",
    "                'rare_word_density': 0.05,\n",
    "                'semantic_complexity': 0.0\n",
    "            }\n",
    "\n",
    "        overall_score = sum(\n",
    "            features.get(feature, 0.0) * weight\n",
    "            for feature, weight in weights.items()\n",
    "        )\n",
    "\n",
    "        return min(max(overall_score, 0.01), 1.0)\n",
    "\n",
    "    def _calculate_basic_complexity(self, text: str) -> float:\n",
    "        \"\"\"Calculate basic complexity for initial learning\"\"\"\n",
    "        words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "        sentences = re.split(r'[.!?]+', text)\n",
    "        sentences = [s.strip() for s in sentences if s.strip()]\n",
    "\n",
    "        if not words or not sentences:\n",
    "            return 0.1\n",
    "\n",
    "        # Simple heuristics\n",
    "        avg_word_len = sum(len(w) for w in words) / len(words)\n",
    "        avg_sent_len = len(words) / len(sentences)\n",
    "        academic_ratio = sum(1 for w in words if w in self.academic_words) / len(words)\n",
    "\n",
    "        complexity = (\n",
    "            min((avg_word_len - 4) / 6, 1.0) * 0.3 +\n",
    "            min((avg_sent_len - 8) / 20, 1.0) * 0.4 +\n",
    "            min(academic_ratio * 10, 1.0) * 0.3\n",
    "        )\n",
    "\n",
    "        return min(max(complexity, 0.05), 0.95)\n",
    "\n",
    "    def _save_models(self):\n",
    "        \"\"\"Save all learned models\"\"\"\n",
    "        models_to_save = {\n",
    "            'word_complexity_scores': self.word_complexity_scores,\n",
    "            'semantic_features': self.semantic_features,\n",
    "            'syntactic_patterns': self.syntactic_patterns,\n",
    "            'tfidf_vectorizer': self.tfidf_vectorizer,\n",
    "            'complexity_clusters': self.complexity_clusters\n",
    "        }\n",
    "\n",
    "        models_file = os.path.join(self.save_dir, 'hybrid_complexity_models.pkl')\n",
    "        try:\n",
    "            with open(models_file, 'wb') as f:\n",
    "                pickle.dump(models_to_save, f)\n",
    "            print(f\"üíæ Models saved to {models_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Failed to save models: {e}\")\n",
    "\n",
    "# =============================================================================\n",
    "# ADAPTIVE CURRICULUM SAMPLER\n",
    "# =============================================================================\n",
    "\n",
    "class AdaptiveCurriculumSampler(Sampler):\n",
    "    \"\"\"Adaptive curriculum sampler that adjusts based on training progress\"\"\"\n",
    "\n",
    "    def __init__(self, dataset, current_epoch: int, max_epochs: int, strategy: str = 'progressive'):\n",
    "        self.dataset = dataset\n",
    "        self.current_epoch = current_epoch\n",
    "        self.max_epochs = max_epochs\n",
    "        self.strategy = strategy\n",
    "\n",
    "        # Calculate progress\n",
    "        self.progress = min(current_epoch / max(max_epochs - 1, 1), 1.0)\n",
    "\n",
    "        # Determine active levels\n",
    "        self.active_levels = self._get_active_levels()\n",
    "\n",
    "        # Get active indices\n",
    "        self.active_indices = self._get_active_indices()\n",
    "\n",
    "        print(f\"üìö Curriculum Epoch {current_epoch}: Levels {self.active_levels}, \"\n",
    "              f\"{len(self.active_indices)} examples\")\n",
    "\n",
    "    def _get_active_levels(self) -> List[int]:\n",
    "        \"\"\"Determine which curriculum levels to include\"\"\"\n",
    "\n",
    "        if self.strategy == 'progressive':\n",
    "            if self.progress < 0.2:\n",
    "                return [0]\n",
    "            elif self.progress < 0.4:\n",
    "                return [0, 1]\n",
    "            elif self.progress < 0.6:\n",
    "                return [0, 1, 2]\n",
    "            elif self.progress < 0.8:\n",
    "                return [0, 1, 2, 3]\n",
    "            else:\n",
    "                return [0, 1, 2, 3, 4]\n",
    "\n",
    "        elif self.strategy == 'mixed':\n",
    "            # Always include easier levels, gradually add harder ones\n",
    "            levels = [0, 1]\n",
    "            if self.progress > 0.3:\n",
    "                levels.append(2)\n",
    "            if self.progress > 0.6:\n",
    "                levels.append(3)\n",
    "            if self.progress > 0.8:\n",
    "                levels.append(4)\n",
    "            return levels\n",
    "\n",
    "        else:  # uniform\n",
    "            return list(range(5))\n",
    "\n",
    "    def _get_active_indices(self) -> List[int]:\n",
    "        \"\"\"Get indices for active curriculum levels\"\"\"\n",
    "        all_indices = []\n",
    "\n",
    "        # Sample from each active level\n",
    "        for level in self.active_levels:\n",
    "            level_indices = self.dataset.get_curriculum_level_data(level)\n",
    "\n",
    "            # Weight simpler levels more heavily early in training\n",
    "            if len(self.active_levels) > 1:\n",
    "                weight = 1.0 + (len(self.active_levels) - level - 1) * 0.2\n",
    "                sample_size = int(len(level_indices) * weight / sum(\n",
    "                    1.0 + (len(self.active_levels) - l - 1) * 0.2\n",
    "                    for l in self.active_levels\n",
    "                ))\n",
    "                sample_size = min(sample_size, len(level_indices))\n",
    "            else:\n",
    "                sample_size = len(level_indices)\n",
    "\n",
    "            if sample_size > 0:\n",
    "                sampled = random.sample(level_indices, sample_size)\n",
    "                all_indices.extend(sampled)\n",
    "\n",
    "        random.shuffle(all_indices)\n",
    "        return all_indices\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.active_indices)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.active_indices)\n",
    "\n",
    "# =============================================================================\n",
    "# T5 CURRICULUM DATASET\n",
    "# =============================================================================\n",
    "\n",
    "class T5CurriculumDataset(Dataset):\n",
    "    \"\"\"T5 dataset with hybrid complexity analysis and curriculum learning\"\"\"\n",
    "\n",
    "    def __init__(self, data_path: str, tokenizer: T5Tokenizer,\n",
    "                 complexity_analyzer: HybridComplexityAnalyzer,\n",
    "                 max_source_length: int = 512, max_target_length: int = 256,\n",
    "                 cache_dir: str = None, split: str = \"train\",\n",
    "                 max_examples: int = None, corruption_probability: float = 0.15):\n",
    "\n",
    "        self.data_path = data_path\n",
    "        self.tokenizer = tokenizer\n",
    "        self.complexity_analyzer = complexity_analyzer\n",
    "        self.max_source_length = max_source_length\n",
    "        self.max_target_length = max_target_length\n",
    "        self.split = split\n",
    "        self.max_examples = max_examples\n",
    "        self.corruption_probability = corruption_probability\n",
    "\n",
    "\n",
    "        # Cache setup - FIXED to include all relevant parameters\n",
    "        self.cache_dir = cache_dir\n",
    "        if cache_dir:\n",
    "            os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "            # Create a unique cache filename based on all relevant parameters\n",
    "            import hashlib\n",
    "            cache_params = f\"{split}_{max_source_length}_{max_target_length}_{max_examples}_{corruption_probability}\"\n",
    "            cache_hash = hashlib.md5(cache_params.encode()).hexdigest()[:8]\n",
    "\n",
    "            self.cache_file = os.path.join(cache_dir, f\"t5_curriculum_{split}_{cache_hash}.pkl\")\n",
    "            print(f\"üìÅ Cache file: {self.cache_file}\")\n",
    "        else:\n",
    "            self.cache_file = None\n",
    "\n",
    "        self.examples = []\n",
    "        self.complexity_scores = []\n",
    "        self.curriculum_levels = {}\n",
    "\n",
    "        self._load_or_process_data()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.examples[idx]\n",
    "\n",
    "    def get_curriculum_level_data(self, level: int) -> List[int]:\n",
    "        \"\"\"Get indices for a specific curriculum level\"\"\"\n",
    "        return self.curriculum_levels.get(level, [])\n",
    "\n",
    "    def _load_or_process_data(self):\n",
    "        \"\"\"Load cached data or process from scratch\"\"\"\n",
    "\n",
    "        # Try loading from cache\n",
    "        if self.cache_file and os.path.exists(self.cache_file):\n",
    "            try:\n",
    "                print(f\"üì¶ Loading cached data from {self.cache_file}\")\n",
    "                with open(self.cache_file, 'rb') as f:\n",
    "                    cached_data = pickle.load(f)\n",
    "                    self.examples = cached_data['examples']\n",
    "                    self.complexity_scores = cached_data['complexity_scores']\n",
    "                    self.curriculum_levels = cached_data.get('curriculum_levels', {})\n",
    "                print(f\"‚úÖ Loaded {len(self.examples)} cached examples\")\n",
    "                return\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Failed to load cache: {e}\")\n",
    "\n",
    "        # Process from scratch\n",
    "        self._process_raw_data()\n",
    "\n",
    "        # Create curriculum if training data\n",
    "        if self.split == 'train' and len(self.examples) > 100:\n",
    "            self._create_curriculum()\n",
    "\n",
    "        # Save to cache\n",
    "        if self.cache_file:\n",
    "            try:\n",
    "                cached_data = {\n",
    "                    'examples': self.examples,\n",
    "                    'complexity_scores': self.complexity_scores,\n",
    "                    'curriculum_levels': self.curriculum_levels\n",
    "                }\n",
    "                with open(self.cache_file, 'wb') as f:\n",
    "                    pickle.dump(cached_data, f)\n",
    "                print(f\"üíæ Cached {len(self.examples)} examples\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Failed to save cache: {e}\")\n",
    "\n",
    "    def _process_raw_data(self):\n",
    "        \"\"\"Process raw data into T5 tasks\"\"\"\n",
    "\n",
    "        if not os.path.exists(self.data_path):\n",
    "            print(f\"‚ùå Data file not found: {self.data_path}\")\n",
    "            return\n",
    "\n",
    "        # Read data\n",
    "        try:\n",
    "            with open(self.data_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                text = f.read()\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Failed to read data: {e}\")\n",
    "            return\n",
    "\n",
    "        # Clean and split into documents\n",
    "        documents = self._clean_and_split_text(text)\n",
    "        print(f\"üìù Processing {len(documents)} documents...\")\n",
    "\n",
    "        processed_count = 0\n",
    "        all_texts_for_learning = []\n",
    "        all_complexities_for_learning = []\n",
    "\n",
    "        # First pass: collect all texts for complexity learning\n",
    "        if self.complexity_analyzer.learning_mode and self.split == 'train':\n",
    "            print(\"üîç First pass: collecting texts for complexity learning...\")\n",
    "            for doc in tqdm(documents[:min(2000, len(documents))], desc=\"Collecting texts\"):\n",
    "                if len(doc.split()) < 20 or len(doc.split()) > 500:\n",
    "                    continue\n",
    "                all_texts_for_learning.append(doc)\n",
    "                # Calculate basic complexity for initial learning\n",
    "                basic_complexity = self.complexity_analyzer._calculate_basic_complexity(doc)\n",
    "                all_complexities_for_learning.append(basic_complexity)\n",
    "\n",
    "            # Train the complexity analyzer\n",
    "            if all_texts_for_learning:\n",
    "                self.complexity_analyzer.learn_from_data(\n",
    "                    all_texts_for_learning,\n",
    "                    all_complexities_for_learning\n",
    "                )\n",
    "\n",
    "        # Second pass: process into T5 tasks with learned complexity analysis\n",
    "        for doc in tqdm(documents, desc=\"Processing documents\"):\n",
    "            if self.max_examples and processed_count >= self.max_examples:\n",
    "                break\n",
    "\n",
    "            try:\n",
    "                word_count = len(doc.split())\n",
    "                if word_count < 20 or word_count > 500:\n",
    "                    continue\n",
    "\n",
    "                # Analyze complexity with hybrid analyzer\n",
    "                complexity_analysis = self.complexity_analyzer.analyze_text_complexity(doc)\n",
    "                complexity_score = complexity_analysis['overall_complexity']\n",
    "\n",
    "                # Skip very low complexity texts in training\n",
    "                if self.split == 'train' and complexity_score < 0.02:\n",
    "                    continue\n",
    "\n",
    "                # Create T5 tasks\n",
    "                tasks = self._create_t5_tasks(doc)\n",
    "\n",
    "                for task in tasks:\n",
    "                    if task and processed_count < (self.max_examples or float('inf')):\n",
    "                        self.examples.append(task)\n",
    "                        self.complexity_scores.append(complexity_score)\n",
    "                        processed_count += 1\n",
    "\n",
    "            except Exception as e:\n",
    "                continue\n",
    "\n",
    "        print(f\"‚úÖ Processed {len(self.examples)} examples\")\n",
    "\n",
    "    def _clean_and_split_text(self, text: str) -> List[str]:\n",
    "        \"\"\"Clean text and split into documents\"\"\"\n",
    "\n",
    "        # Basic cleaning\n",
    "        text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)\n",
    "        text = re.sub(r'[ \\t]+', ' ', text)\n",
    "        text = re.sub(r'\\s+([,.!?;:])', r'\\1', text)\n",
    "\n",
    "        # Split into paragraphs\n",
    "        paragraphs = text.split('\\n\\n')\n",
    "        documents = []\n",
    "\n",
    "        for para in paragraphs:\n",
    "            para = para.strip()\n",
    "            if len(para.split()) < 20:\n",
    "                continue\n",
    "\n",
    "            # If paragraph is too long, split by sentences\n",
    "            if len(para.split()) > 400:\n",
    "                sentences = re.split(r'(?<=[.!?])\\s+', para)\n",
    "                current_doc = []\n",
    "                current_length = 0\n",
    "\n",
    "                for sent in sentences:\n",
    "                    sent_length = len(sent.split())\n",
    "                    if current_length + sent_length > 300 and current_doc:\n",
    "                        documents.append(' '.join(current_doc))\n",
    "                        current_doc = [sent]\n",
    "                        current_length = sent_length\n",
    "                    else:\n",
    "                        current_doc.append(sent)\n",
    "                        current_length += sent_length\n",
    "\n",
    "                if current_doc and current_length >= 20:\n",
    "                    documents.append(' '.join(current_doc))\n",
    "            else:\n",
    "                documents.append(para)\n",
    "\n",
    "        return documents\n",
    "\n",
    "    def _create_t5_tasks(self, text: str) -> List[Dict]:\n",
    "        \"\"\"Create various T5 tasks from text\"\"\"\n",
    "\n",
    "        tasks = []\n",
    "        sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "        sentences = [s.strip() for s in sentences if len(s.split()) >= 3]\n",
    "\n",
    "        if len(sentences) < 2:\n",
    "            return tasks\n",
    "\n",
    "        # 1. Span Corruption (T5's main pretraining task)\n",
    "        corrupted_task = self._create_span_corruption_task(text)\n",
    "        if corrupted_task:\n",
    "            tasks.append(corrupted_task)\n",
    "\n",
    "        # 2. Summarization\n",
    "        if len(sentences) >= 3:\n",
    "            if len(sentences) <= 5:\n",
    "                summary = sentences[0]\n",
    "            else:\n",
    "                num_summary = max(1, len(sentences) // 3)\n",
    "                summary = ' '.join(sentences[:num_summary])\n",
    "\n",
    "            source_text = f\"summarize: {text}\"\n",
    "            task = self._tokenize_example(source_text, summary, 'summarization')\n",
    "            if task:\n",
    "                tasks.append(task)\n",
    "\n",
    "        # 3. Text Completion\n",
    "        if len(sentences) >= 3:\n",
    "            split_point = len(sentences) // 2\n",
    "            prefix = ' '.join(sentences[:split_point])\n",
    "            completion = ' '.join(sentences[split_point:])\n",
    "\n",
    "            source_text = f\"complete: {prefix}\"\n",
    "            task = self._tokenize_example(source_text, completion, 'completion')\n",
    "            if task:\n",
    "                tasks.append(task)\n",
    "\n",
    "        return tasks\n",
    "\n",
    "    def _create_span_corruption_task(self, text: str) -> Optional[Dict]:\n",
    "        \"\"\"Create T5-style span corruption task\"\"\"\n",
    "\n",
    "        try:\n",
    "            tokens = self.tokenizer.tokenize(text)\n",
    "\n",
    "            if len(tokens) < 10:\n",
    "                return None\n",
    "\n",
    "            # Calculate spans to corrupt\n",
    "            num_tokens_to_corrupt = max(1, int(len(tokens) * self.corruption_probability))\n",
    "            corrupted_tokens = tokens.copy()\n",
    "            target_spans = []\n",
    "            sentinel_count = 0\n",
    "\n",
    "            i = 0\n",
    "            tokens_corrupted = 0\n",
    "\n",
    "            while i < len(corrupted_tokens) and tokens_corrupted < num_tokens_to_corrupt:\n",
    "                if random.random() < 0.3 and tokens_corrupted < num_tokens_to_corrupt:\n",
    "                    # Determine span length (exponential distribution)\n",
    "                    span_length = max(1, min(5, int(random.expovariate(1.0/3.0))))\n",
    "                    span_end = min(i + span_length, len(corrupted_tokens))\n",
    "\n",
    "                    # Extract span\n",
    "                    span_tokens = corrupted_tokens[i:span_end]\n",
    "                    sentinel_token = f'<extra_id_{sentinel_count}>'\n",
    "                    target_spans.append(f'{sentinel_token} {\" \".join(span_tokens)}')\n",
    "\n",
    "                    # Replace in source\n",
    "                    corrupted_tokens[i:span_end] = [sentinel_token]\n",
    "\n",
    "                    tokens_corrupted += len(span_tokens)\n",
    "                    sentinel_count += 1\n",
    "                    i += 1\n",
    "                else:\n",
    "                    i += 1\n",
    "\n",
    "            # Create source and target\n",
    "            source_text = self.tokenizer.convert_tokens_to_string(corrupted_tokens)\n",
    "            target_text = ' '.join(target_spans) + f' <extra_id_{sentinel_count}>'\n",
    "\n",
    "            return self._tokenize_example(source_text, target_text, 'span_corruption')\n",
    "\n",
    "        except Exception as e:\n",
    "            return None\n",
    "\n",
    "    def _tokenize_example(self, source_text: str, target_text: str, task_type: str) -> Optional[Dict]:\n",
    "        \"\"\"Tokenize a source-target pair into model inputs\"\"\"\n",
    "\n",
    "        try:\n",
    "            # Tokenize source\n",
    "            source_encoding = self.tokenizer(\n",
    "                source_text,\n",
    "                max_length=self.max_source_length,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "\n",
    "            # Tokenize target\n",
    "            target_encoding = self.tokenizer(\n",
    "                target_text,\n",
    "                max_length=self.max_target_length,\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "\n",
    "            return {\n",
    "                'input_ids': source_encoding['input_ids'].squeeze(),\n",
    "                'attention_mask': source_encoding['attention_mask'].squeeze(),\n",
    "                'labels': target_encoding['input_ids'].squeeze(),\n",
    "                'task_type': task_type\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            return None\n",
    "\n",
    "    def _create_curriculum(self):\n",
    "        \"\"\"Create curriculum levels based on complexity scores\"\"\"\n",
    "\n",
    "        if not self.complexity_scores:\n",
    "            return\n",
    "\n",
    "        print(\"üéØ Creating curriculum levels...\")\n",
    "\n",
    "        # Sort examples by complexity\n",
    "        complexity_indices = [(score, idx) for idx, score in enumerate(self.complexity_scores)]\n",
    "        complexity_indices.sort()\n",
    "\n",
    "        # Create 5 curriculum levels\n",
    "        total_examples = len(complexity_indices)\n",
    "        level_size = total_examples // 5\n",
    "\n",
    "        for level in range(5):\n",
    "            start_idx = level * level_size\n",
    "            if level == 4:  # Last level gets remaining examples\n",
    "                end_idx = total_examples\n",
    "            else:\n",
    "                end_idx = (level + 1) * level_size\n",
    "\n",
    "            level_indices = [idx for _, idx in complexity_indices[start_idx:end_idx]]\n",
    "            self.curriculum_levels[level] = level_indices\n",
    "\n",
    "            if level_indices:\n",
    "                avg_complexity = np.mean([self.complexity_scores[idx] for idx in level_indices])\n",
    "                print(f\"   Level {level}: {len(level_indices)} examples, avg complexity: {avg_complexity:.3f}\")\n",
    "\n",
    "# =============================================================================\n",
    "# ADAPTIVE VECTOR TRANSITION T5 MODEL\n",
    "# =============================================================================\n",
    "\n",
    "class AdaptiveVectorTransitionT5(T5ForConditionalGeneration):\n",
    "    \"\"\"\n",
    "    Enhanced T5 model with Vector Transition (VT) embeddings featuring:\n",
    "    - Adaptive delta weights based on token properties\n",
    "    - Improved boundary handling\n",
    "    - Layer-specific delta application\n",
    "    - Memory-optimized operations\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, delta_weight: float = 0.4,\n",
    "                 apply_to_encoder: bool = True, apply_to_decoder: bool = True,\n",
    "                 adaptive_delta: bool = True, warmup_steps: int = 1000):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.delta_weight = delta_weight\n",
    "        self.apply_to_encoder = apply_to_encoder\n",
    "        self.apply_to_decoder = apply_to_decoder\n",
    "        self.adaptive_delta = adaptive_delta\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.training_step = 0\n",
    "\n",
    "        # Adaptive delta components\n",
    "        if self.adaptive_delta:\n",
    "            self.token_type_embeddings = nn.Embedding(4, 1)  # 4 token types\n",
    "            self.position_delta_layer = nn.Linear(config.d_model, 1)\n",
    "            self.delta_gate = nn.Linear(config.d_model * 2, 1)  # For gating mechanism\n",
    "\n",
    "        # Statistics tracking\n",
    "        self.delta_stats = {\n",
    "            'applications': 0,\n",
    "            'avg_delta_magnitude': 0.0,\n",
    "            'boundary_corrections': 0\n",
    "        }\n",
    "\n",
    "        print(f\"üöÄ Initialized AdaptiveVectorTransitionT5:\")\n",
    "        print(f\"   Delta weight: {self.delta_weight}\")\n",
    "        print(f\"   Apply to encoder: {self.apply_to_encoder}\")\n",
    "        print(f\"   Apply to decoder: {self.apply_to_decoder}\")\n",
    "        print(f\"   Adaptive delta: {self.adaptive_delta}\")\n",
    "\n",
    "    def _get_token_types(self, input_ids: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Classify tokens into types for adaptive delta weighting:\n",
    "        0: Special tokens (pad, eos, etc.)\n",
    "        1: Content words (nouns, verbs, adjectives)\n",
    "        2: Function words (articles, prepositions, etc.)\n",
    "        3: Punctuation and numbers\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "        token_types = torch.zeros((batch_size, seq_len), dtype=torch.long, device=input_ids.device)\n",
    "\n",
    "        # Build special token set only from attributes that exist and are not None\n",
    "        special_tokens = set()\n",
    "        for attr in (\"pad_token_id\", \"eos_token_id\", \"bos_token_id\", \"unk_token_id\"):\n",
    "            if hasattr(self.config, attr):\n",
    "                val = getattr(self.config, attr)\n",
    "                if val is not None:\n",
    "                    special_tokens.add(val)\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            for j in range(seq_len):\n",
    "                token_id = int(input_ids[i, j].item())\n",
    "\n",
    "                if token_id in special_tokens:\n",
    "                    token_types[i, j] = 0\n",
    "                elif token_id < 1000:  # heuristic: frequent tokens -> function words\n",
    "                    token_types[i, j] = 2\n",
    "                else:\n",
    "                    token_types[i, j] = 1  # content words\n",
    "\n",
    "        return token_types\n",
    "\n",
    "    def _apply_adaptive_vt_embeddings(self, embeddings: torch.Tensor,\n",
    "                                    input_ids: torch.Tensor = None) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Apply Vector Transition embeddings with adaptive weighting\n",
    "\n",
    "        Args:\n",
    "            embeddings: Token embeddings [batch_size, seq_len, d_model]\n",
    "            input_ids: Input token IDs [batch_size, seq_len] (for adaptive weighting)\n",
    "\n",
    "        Returns:\n",
    "            Modified embeddings with VT applied\n",
    "        \"\"\"\n",
    "        if self.delta_weight == 0.0:\n",
    "            return embeddings\n",
    "\n",
    "        batch_size, seq_len, d_model = embeddings.shape\n",
    "\n",
    "        # Calculate base deltas (difference from previous token)\n",
    "        previous_embeddings = torch.roll(embeddings, shifts=1, dims=1)\n",
    "\n",
    "        # IMPROVED BOUNDARY HANDLING: Use average of all embeddings for first token\n",
    "        # instead of zeros, which reduces artificial discontinuity\n",
    "        if seq_len > 1:\n",
    "            avg_embedding = embeddings.mean(dim=1, keepdim=True)  # [batch_size, 1, d_model]\n",
    "            previous_embeddings[:, 0, :] = avg_embedding.squeeze(1)\n",
    "        else:\n",
    "            previous_embeddings[:, 0, :] = embeddings[:, 0, :]\n",
    "\n",
    "        # Compute base delta (semantic velocity)\n",
    "        base_delta = embeddings - previous_embeddings\n",
    "\n",
    "        # Track statistics\n",
    "        self.delta_stats['applications'] += 1\n",
    "        self.delta_stats['avg_delta_magnitude'] = (\n",
    "            self.delta_stats['avg_delta_magnitude'] * 0.9 +\n",
    "            base_delta.abs().mean().item() * 0.1\n",
    "        )\n",
    "\n",
    "        if self.adaptive_delta and input_ids is not None:\n",
    "            # Get adaptive weights\n",
    "            delta_weights = self._compute_adaptive_weights(embeddings, base_delta, input_ids)\n",
    "            final_delta = base_delta * delta_weights.unsqueeze(-1)\n",
    "        else:\n",
    "            # Use fixed delta weight with warmup\n",
    "            current_weight = self._get_warmup_weight()\n",
    "            final_delta = base_delta * current_weight\n",
    "\n",
    "        # Apply delta to embeddings (core VT operation: addition of semantic velocity)\n",
    "        modified_embeddings = embeddings + final_delta\n",
    "\n",
    "        return modified_embeddings\n",
    "\n",
    "    def _compute_adaptive_weights(self, embeddings: torch.Tensor,\n",
    "                                deltas: torch.Tensor, input_ids: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute adaptive delta weights based on token properties\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, d_model = embeddings.shape\n",
    "\n",
    "        # Token type based weighting\n",
    "        token_types = self._get_token_types(input_ids)  # long tensor on correct device\n",
    "        type_weights = self.token_type_embeddings(token_types).squeeze(-1)  # [batch_size, seq_len]\n",
    "\n",
    "        # Position-based weighting (content words get higher weights)\n",
    "        position_weights = torch.sigmoid(self.position_delta_layer(embeddings)).squeeze(-1)\n",
    "\n",
    "        # Context-aware gating (combine current and previous embeddings)\n",
    "        previous_embeddings = torch.roll(embeddings, shifts=1, dims=1)\n",
    "        combined_context = torch.cat([embeddings, previous_embeddings], dim=-1)\n",
    "        context_gates = torch.sigmoid(self.delta_gate(combined_context)).squeeze(-1)\n",
    "\n",
    "        # Combine all weights\n",
    "        base_weight = float(self.delta_weight)\n",
    "        adaptive_weights = base_weight * (\n",
    "            0.3 * torch.sigmoid(type_weights) +\n",
    "            0.3 * position_weights +\n",
    "            0.4 * context_gates\n",
    "        )\n",
    "\n",
    "        # Apply warmup ‚Äî make warmup_factor a tensor on the same device/dtype\n",
    "        warmup_factor = float(self._get_warmup_weight())\n",
    "        warmup_tensor = torch.tensor(warmup_factor, device=adaptive_weights.device, dtype=adaptive_weights.dtype)\n",
    "        adaptive_weights = adaptive_weights * warmup_tensor\n",
    "\n",
    "        return adaptive_weights\n",
    "\n",
    "    def _get_warmup_weight(self) -> float:\n",
    "        \"\"\"Get current warmup weight for delta application\"\"\"\n",
    "        if self.training_step < self.warmup_steps:\n",
    "            return (self.training_step / self.warmup_steps) * self.delta_weight\n",
    "        return self.delta_weight\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, decoder_input_ids=None,\n",
    "                decoder_attention_mask=None, head_mask=None, decoder_head_mask=None,\n",
    "                cross_attn_head_mask=None, encoder_outputs=None, past_key_values=None,\n",
    "                inputs_embeds=None, decoder_inputs_embeds=None, labels=None,\n",
    "                use_cache=None, output_attentions=None, output_hidden_states=None,\n",
    "                return_dict=None, **kwargs):\n",
    "\n",
    "        self.training_step += 1\n",
    "\n",
    "        # Handle encoder inputs with VT embeddings\n",
    "        if encoder_outputs is None and self.apply_to_encoder:\n",
    "            if input_ids is not None and inputs_embeds is None:\n",
    "                inputs_embeds = self.shared(input_ids)\n",
    "\n",
    "                # Apply VT embeddings to encoder\n",
    "                inputs_embeds = self._apply_adaptive_vt_embeddings(\n",
    "                    inputs_embeds, input_ids\n",
    "                )\n",
    "                input_ids = None\n",
    "\n",
    "        # Handle decoder inputs with VT embeddings\n",
    "        if self.apply_to_decoder:\n",
    "            if labels is not None and decoder_input_ids is None and decoder_inputs_embeds is None:\n",
    "                decoder_input_ids = self._shift_right(labels)\n",
    "\n",
    "            if decoder_input_ids is not None and decoder_inputs_embeds is None:\n",
    "                decoder_inputs_embeds = self.shared(decoder_input_ids)\n",
    "\n",
    "                # Apply VT embeddings to decoder\n",
    "                decoder_inputs_embeds = self._apply_adaptive_vt_embeddings(\n",
    "                    decoder_inputs_embeds, decoder_input_ids\n",
    "                )\n",
    "                decoder_input_ids = None\n",
    "\n",
    "        # Filter kwargs for parent class\n",
    "        forward_args = signature(super().forward).parameters\n",
    "        filtered_kwargs = {k: v for k, v in kwargs.items() if k in forward_args}\n",
    "\n",
    "        return super().forward(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            decoder_input_ids=decoder_input_ids,\n",
    "            decoder_attention_mask=decoder_attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            decoder_head_mask=decoder_head_mask,\n",
    "            cross_attn_head_mask=cross_attn_head_mask,\n",
    "            encoder_outputs=encoder_outputs,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            decoder_inputs_embeds=decoder_inputs_embeds,\n",
    "            labels=labels,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            **filtered_kwargs\n",
    "        )\n",
    "\n",
    "    def get_delta_statistics(self) -> Dict:\n",
    "        \"\"\"Get statistics about delta applications\"\"\"\n",
    "        return self.delta_stats.copy()\n",
    "\n",
    "# =============================================================================\n",
    "# VT T5 CURRICULUM TRAINER\n",
    "# =============================================================================\n",
    "\n",
    "class VTT5CurriculumTrainer:\n",
    "    \"\"\"VT T5 trainer with curriculum learning\"\"\"\n",
    "\n",
    "    def __init__(self, model: AdaptiveVectorTransitionT5, tokenizer: T5Tokenizer,\n",
    "                 save_dir: str, learning_rate: float = 1e-4, weight_decay: float = 0.01):\n",
    "\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.save_dir = save_dir\n",
    "\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "        # Optimizer setup\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters()\n",
    "                          if not any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters()\n",
    "                          if any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        self.optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "        self.scheduler = None\n",
    "        self.global_step = 0\n",
    "\n",
    "        # Training metrics\n",
    "        self.training_stats = {\n",
    "            'epoch_losses': [],\n",
    "            'val_losses': [],\n",
    "            'curriculum_levels': [],  # Track curriculum progression\n",
    "            'vt_statistics': []  # VT-specific stats\n",
    "        }\n",
    "\n",
    "        print(\"‚úÖ VT T5 Curriculum Trainer initialized\")\n",
    "\n",
    "    def train(self, train_dataset, val_dataset: Optional = None,\n",
    "              num_epochs: int = 3, batch_size: int = 8, curriculum_strategy: str = 'progressive',\n",
    "              device: str = 'cuda', patience: int = 3):\n",
    "\n",
    "        print(f\"üöÄ Starting VT T5 curriculum training\")\n",
    "        print(f\"   Epochs: {num_epochs}, Batch size: {batch_size}\")\n",
    "        print(f\"   Strategy: {curriculum_strategy}\")\n",
    "        print(f\"   Delta weight: {self.model.delta_weight}\")\n",
    "        print(f\"   Adaptive delta: {self.model.adaptive_delta}\")\n",
    "\n",
    "        # Setup scheduler\n",
    "        steps_per_epoch = max(1, len(train_dataset) // batch_size)\n",
    "        total_steps = steps_per_epoch * num_epochs\n",
    "        self.scheduler = get_linear_schedule_with_warmup(\n",
    "            self.optimizer, num_warmup_steps=total_steps // 10, num_training_steps=total_steps\n",
    "        )\n",
    "\n",
    "        # Move model to device\n",
    "        self.model.to(device)\n",
    "\n",
    "        # Early stopping\n",
    "        best_val_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"\\n{'='*50}\")\n",
    "            print(f\"EPOCH {epoch + 1}/{num_epochs}\")\n",
    "            print(f\"{'='*50}\")\n",
    "\n",
    "            # Create curriculum sampler\n",
    "            curriculum_sampler = AdaptiveCurriculumSampler(\n",
    "                train_dataset, epoch, num_epochs, curriculum_strategy\n",
    "            )\n",
    "\n",
    "            # Data loader with curriculum sampler\n",
    "            train_loader = DataLoader(\n",
    "                train_dataset,\n",
    "                batch_size=batch_size,\n",
    "                sampler=curriculum_sampler,\n",
    "                collate_fn=self._collate_fn,\n",
    "                num_workers=0\n",
    "            )\n",
    "\n",
    "            # Train epoch\n",
    "            epoch_loss = self._train_epoch(train_loader, device)\n",
    "\n",
    "            # Get VT statistics\n",
    "            vt_stats = self.model.get_delta_statistics()\n",
    "            self.training_stats['vt_statistics'].append(vt_stats)\n",
    "\n",
    "            # Validation\n",
    "            val_loss = None\n",
    "            if val_dataset:\n",
    "                val_loss = self._evaluate(val_dataset, device, batch_size)\n",
    "\n",
    "                # Early stopping\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    patience_counter = 0\n",
    "                    self._save_checkpoint(epoch, is_best=True)\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "\n",
    "                if patience_counter >= patience:\n",
    "                    print(f\"üõë Early stopping at epoch {epoch + 1}\")\n",
    "                    break\n",
    "\n",
    "            # Save stats\n",
    "            self.training_stats['epoch_losses'].append(epoch_loss)\n",
    "            self.training_stats['val_losses'].append(val_loss or 0)\n",
    "            self.training_stats['curriculum_levels'].append(curriculum_sampler.active_levels)\n",
    "\n",
    "            print(f\"üìä Epoch {epoch + 1} - Train Loss: {epoch_loss:.4f}\")\n",
    "            if val_loss:\n",
    "                print(f\"üìä Epoch {epoch + 1} - Val Loss: {val_loss:.4f}\")\n",
    "            print(f\"üìä VT Applications: {vt_stats['applications']}\")\n",
    "            print(f\"üìä Avg Delta Magnitude: {vt_stats['avg_delta_magnitude']:.6f}\")\n",
    "\n",
    "        # Save final model\n",
    "        self._save_final_model()\n",
    "        return self.training_stats\n",
    "\n",
    "    def _train_epoch(self, train_loader: DataLoader, device: str) -> float:\n",
    "        \"\"\"Train one epoch\"\"\"\n",
    "\n",
    "        self.model.train()\n",
    "        epoch_loss = 0.0\n",
    "        num_batches = 0\n",
    "\n",
    "        progress_bar = tqdm(train_loader, desc='Training')\n",
    "\n",
    "        for batch in progress_bar:\n",
    "            # Move to device\n",
    "            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v\n",
    "                    for k, v in batch.items()}\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = self.model(\n",
    "                input_ids=batch['input_ids'],\n",
    "                attention_mask=batch['attention_mask'],\n",
    "                labels=batch['labels']\n",
    "            )\n",
    "\n",
    "            loss = outputs.loss\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "\n",
    "            self.optimizer.step()\n",
    "            self.scheduler.step()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            # Update metrics\n",
    "            epoch_loss += loss.item()\n",
    "            num_batches += 1\n",
    "            self.global_step += 1\n",
    "\n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({\n",
    "                'Loss': f'{loss.item():.4f}',\n",
    "                'Avg': f'{epoch_loss/num_batches:.4f}'\n",
    "            })\n",
    "\n",
    "        return epoch_loss / max(num_batches, 1)\n",
    "\n",
    "    def _evaluate(self, val_dataset, device: str, batch_size: int = 8) -> float:\n",
    "        \"\"\"Evaluate model on validation set\"\"\"\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "        # Sample validation examples\n",
    "        val_indices = list(range(min(500, len(val_dataset))))\n",
    "        random.shuffle(val_indices)\n",
    "\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=batch_size,\n",
    "            sampler=val_indices,\n",
    "            collate_fn=self._collate_fn\n",
    "        )\n",
    "\n",
    "        total_loss = 0.0\n",
    "        num_batches = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc='Validation', leave=False):\n",
    "                batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v\n",
    "                        for k, v in batch.items()}\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = self.model(\n",
    "                    input_ids=batch['input_ids'],\n",
    "                    attention_mask=batch['attention_mask'],\n",
    "                    labels=batch['labels']\n",
    "                )\n",
    "\n",
    "                total_loss += outputs.loss.item()\n",
    "                num_batches += 1\n",
    "\n",
    "        return total_loss / max(num_batches, 1)\n",
    "\n",
    "    def _collate_fn(self, batch):\n",
    "        \"\"\"Collate function for T5 batch processing\"\"\"\n",
    "\n",
    "        input_ids = torch.stack([item['input_ids'] for item in batch])\n",
    "        attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
    "        labels = torch.stack([item['labels'] for item in batch])\n",
    "\n",
    "        # Mask padding tokens in labels\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels,\n",
    "            'task_types': [item['task_type'] for item in batch]\n",
    "        }\n",
    "\n",
    "    def _save_checkpoint(self, epoch: int, is_best: bool = False):\n",
    "        \"\"\"Save model checkpoint\"\"\"\n",
    "\n",
    "        checkpoint_dir = os.path.join(self.save_dir, f'checkpoint-epoch-{epoch}')\n",
    "        if is_best:\n",
    "            checkpoint_dir = os.path.join(self.save_dir, 'best_model')\n",
    "\n",
    "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "        self.model.save_pretrained(checkpoint_dir)\n",
    "        self.tokenizer.save_pretrained(checkpoint_dir)\n",
    "\n",
    "        # Save training state\n",
    "        training_state = {\n",
    "            'epoch': epoch,\n",
    "            'global_step': self.global_step,\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'scheduler_state_dict': self.scheduler.state_dict() if self.scheduler else None,\n",
    "            'training_stats': self.training_stats\n",
    "        }\n",
    "\n",
    "        torch.save(training_state, os.path.join(checkpoint_dir, 'training_state.pt'))\n",
    "\n",
    "    def _save_final_model(self):\n",
    "        \"\"\"Save final model\"\"\"\n",
    "\n",
    "        final_dir = os.path.join(self.save_dir, 'final_model')\n",
    "        os.makedirs(final_dir, exist_ok=True)\n",
    "\n",
    "        self.model.save_pretrained(final_dir)\n",
    "        self.tokenizer.save_pretrained(final_dir)\n",
    "\n",
    "        # Save training stats\n",
    "        with open(os.path.join(final_dir, 'training_stats.json'), 'w') as f:\n",
    "            json.dump(self.training_stats, f, indent=2)\n",
    "\n",
    "        # Save VT model configuration\n",
    "        vt_config = {\n",
    "            'delta_weight': self.model.delta_weight,\n",
    "            'apply_to_encoder': self.model.apply_to_encoder,\n",
    "            'apply_to_decoder': self.model.apply_to_decoder,\n",
    "            'adaptive_delta': self.model.adaptive_delta,\n",
    "            'warmup_steps': self.model.warmup_steps,\n",
    "            'final_vt_stats': self.model.get_delta_statistics()\n",
    "        }\n",
    "\n",
    "        with open(os.path.join(final_dir, 'vt_config.json'), 'w') as f:\n",
    "            json.dump(vt_config, f, indent=2)\n",
    "\n",
    "        print(f\"üèÜ Final VT model saved to {final_dir}\")\n",
    "\n",
    "# =============================================================================\n",
    "# PERFORMANCE OPTIMIZATIONS FOR COLAB T4\n",
    "# =============================================================================\n",
    "\n",
    "def optimize_for_colab_t4():\n",
    "    \"\"\"Apply performance optimizations for Colab T4 GPU\"\"\"\n",
    "\n",
    "    # Enable cudnn benchmark for consistent input sizes\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    # Set environment variables for faster training\n",
    "    os.environ['TOKENIZERS_PARALLELISM'] = 'false'  # Avoid tokenizer warnings\n",
    "    os.environ['CUDA_LAUNCH_BLOCKING'] = '0'  # Allow async CUDA operations\n",
    "\n",
    "    # Clear GPU cache\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"üöÄ GPU optimizations applied\")\n",
    "\n",
    "# =============================================================================\n",
    "# MAIN TRAINING FUNCTION\n",
    "# =============================================================================\n",
    "\n",
    "def train_vt_curriculum_model(config: VTCurriculumConfig = None):\n",
    "    \"\"\"Main training function for VT T5 model with curriculum learning - FROM SCRATCH\"\"\"\n",
    "\n",
    "    if config is None:\n",
    "        config = VTCurriculumConfig()\n",
    "\n",
    "    print(f\"üöÄ VT T5 Curriculum Training FROM SCRATCH\")\n",
    "    print(f\"   Model: {config.model_name}\")\n",
    "    print(f\"   Strategy: {config.curriculum_strategy}\")\n",
    "    print(f\"   Delta weight: {config.delta_weight}\")\n",
    "    print(f\"   Device: {config.device}\")\n",
    "\n",
    "    # Speed optimizations for Colab T4\n",
    "    if hasattr(config, 'use_fp16') and config.use_fp16:\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    # Create directories\n",
    "    os.makedirs(config.save_dir, exist_ok=True)\n",
    "    os.makedirs(config.cache_dir, exist_ok=True)\n",
    "\n",
    "    # FIXED: Initialize model from scratch (no pretrained weights)\n",
    "    print(\"üî§ Creating VT model from scratch...\")\n",
    "    base_config = T5Config.from_pretrained(config.model_name)\n",
    "\n",
    "    # Create VT model with random weights (FROM SCRATCH)\n",
    "    model = AdaptiveVectorTransitionT5(\n",
    "        config=base_config,\n",
    "        delta_weight=config.delta_weight,\n",
    "        apply_to_encoder=config.apply_to_encoder,\n",
    "        apply_to_decoder=config.apply_to_decoder,\n",
    "        adaptive_delta=config.adaptive_delta,\n",
    "        warmup_steps=config.warmup_steps\n",
    "    )\n",
    "\n",
    "    # Explicitly initialize weights randomly\n",
    "    def init_weights(module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "    # Apply random initialization\n",
    "    model.apply(init_weights)\n",
    "    print(\"‚úÖ Model initialized from scratch with random weights\")\n",
    "\n",
    "    # Create tokenizer (only config needed, not weights)\n",
    "    tokenizer = T5Tokenizer.from_pretrained(config.model_name)\n",
    "\n",
    "    # Create complexity analyzer\n",
    "    complexity_analyzer = HybridComplexityAnalyzer(\n",
    "        tokenizer=tokenizer,\n",
    "        save_dir=os.path.join(config.cache_dir, 'complexity_models'),\n",
    "        learning_mode=True\n",
    "    )\n",
    "\n",
    "    datasets = {}\n",
    "    file_mapping = {\n",
    "        'train': 'babylm_train.txt',\n",
    "        'dev': 'babylm_dev.txt'\n",
    "    }\n",
    "\n",
    "    # Create datasets\n",
    "    for split, filename in file_mapping.items():\n",
    "        filepath = os.path.join(config.data_dir, filename)\n",
    "\n",
    "        if not os.path.exists(filepath):\n",
    "            print(f\"‚ö†Ô∏è File not found: {filepath}\")\n",
    "            continue\n",
    "\n",
    "        max_examples = (config.max_train_examples if split == 'train'\n",
    "                       else config.max_val_examples)\n",
    "\n",
    "        dataset = T5CurriculumDataset(\n",
    "            data_path=filepath,\n",
    "            tokenizer=tokenizer,\n",
    "            complexity_analyzer=complexity_analyzer,\n",
    "            max_source_length=config.max_source_length,\n",
    "            max_target_length=config.max_target_length,\n",
    "            cache_dir=os.path.join(config.cache_dir, 'dataset_cache'),\n",
    "            split=split,\n",
    "            max_examples=max_examples,\n",
    "            corruption_probability=config.corruption_probability\n",
    "        )\n",
    "\n",
    "        if len(dataset) > 0:\n",
    "            datasets[split] = dataset\n",
    "            print(f\"‚úÖ Created {split} dataset: {len(dataset)} examples\")\n",
    "\n",
    "    if 'train' not in datasets:\n",
    "        raise ValueError(\"‚ùå No training dataset available!\")\n",
    "\n",
    "    print(f\"üìä Dataset sizes:\")\n",
    "    for split, dataset in datasets.items():\n",
    "        print(f\"   {split}: {len(dataset)} examples\")\n",
    "\n",
    "    # Initialize trainer\n",
    "    trainer = VTT5CurriculumTrainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        save_dir=config.save_dir,\n",
    "        learning_rate=config.learning_rate,\n",
    "        weight_decay=config.weight_decay\n",
    "    )\n",
    "\n",
    "    # Train model from scratch\n",
    "    training_stats = trainer.train(\n",
    "        train_dataset=datasets['train'],\n",
    "        val_dataset=datasets.get('dev'),\n",
    "        num_epochs=config.num_epochs,\n",
    "        batch_size=config.batch_size,\n",
    "        curriculum_strategy=config.curriculum_strategy,\n",
    "        device=config.device,\n",
    "        patience=config.patience\n",
    "    )\n",
    "\n",
    "    print(\"üèÅ VT Curriculum Training completed!\")\n",
    "    print(\"‚úÖ Model trained completely from scratch (no pretrained weights used)\")\n",
    "\n",
    "    return {\n",
    "        'model': model,\n",
    "        'tokenizer': tokenizer,\n",
    "        'complexity_analyzer': complexity_analyzer,\n",
    "        'training_stats': training_stats,\n",
    "        'config': config\n",
    "    }\n",
    "# =============================================================================\n",
    "# EVALUATION AND TESTING FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def evaluate_vt_model(model_path: str, test_data_path: str, batch_size: int = 8, device: str = 'cuda'):\n",
    "    \"\"\"Evaluate trained VT model on test data\"\"\"\n",
    "\n",
    "    print(f\"üîç Evaluating VT model from {model_path}\")\n",
    "\n",
    "    # Load model and tokenizer\n",
    "    model = AdaptiveVectorTransitionT5.from_pretrained(model_path)\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Load VT config if available\n",
    "    vt_config_path = os.path.join(model_path, 'vt_config.json')\n",
    "    if os.path.exists(vt_config_path):\n",
    "        with open(vt_config_path, 'r') as f:\n",
    "            vt_config = json.load(f)\n",
    "        print(f\"üìã VT Config: {vt_config}\")\n",
    "\n",
    "    # Create test dataset (simplified)\n",
    "    complexity_analyzer = HybridComplexityAnalyzer(\n",
    "        tokenizer=tokenizer,\n",
    "        save_dir=os.path.dirname(model_path),\n",
    "        learning_mode=False\n",
    "    )\n",
    "\n",
    "    test_dataset = T5CurriculumDataset(\n",
    "        data_path=test_data_path,\n",
    "        tokenizer=tokenizer,\n",
    "        complexity_analyzer=complexity_analyzer,\n",
    "        split='test',\n",
    "        max_examples=1000,  # Limit for evaluation\n",
    "        cache_dir=None  # No caching for evaluation\n",
    "    )\n",
    "\n",
    "    print(f\"üìä Test dataset: {len(test_dataset)} examples\")\n",
    "\n",
    "    # Evaluate\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=lambda batch: _collate_fn_standalone(batch, tokenizer)\n",
    "    )\n",
    "\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc='Evaluating'):\n",
    "            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v\n",
    "                    for k, v in batch.items()}\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=batch['input_ids'],\n",
    "                attention_mask=batch['attention_mask'],\n",
    "                labels=batch['labels']\n",
    "            )\n",
    "\n",
    "            total_loss += outputs.loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "    avg_loss = total_loss / max(num_batches, 1)\n",
    "    perplexity = math.exp(avg_loss)\n",
    "\n",
    "    print(f\"üìä Evaluation Results:\")\n",
    "    print(f\"   Average Loss: {avg_loss:.4f}\")\n",
    "    print(f\"   Perplexity: {perplexity:.2f}\")\n",
    "\n",
    "    # Get final VT statistics\n",
    "    vt_stats = model.get_delta_statistics()\n",
    "    print(f\"   VT Applications: {vt_stats['applications']}\")\n",
    "    print(f\"   Avg Delta Magnitude: {vt_stats['avg_delta_magnitude']:.6f}\")\n",
    "\n",
    "    return {\n",
    "        'avg_loss': avg_loss,\n",
    "        'perplexity': perplexity,\n",
    "        'vt_statistics': vt_stats\n",
    "    }\n",
    "\n",
    "def _collate_fn_standalone(batch, tokenizer):\n",
    "    \"\"\"Standalone collate function for evaluation\"\"\"\n",
    "    input_ids = torch.stack([item['input_ids'] for item in batch])\n",
    "    attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
    "    labels = torch.stack([item['labels'] for item in batch])\n",
    "\n",
    "    # Mask padding tokens in labels\n",
    "    labels[labels == tokenizer.pad_token_id] = -100\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'labels': labels,\n",
    "        'task_types': [item['task_type'] for item in batch]\n",
    "    }\n",
    "\n",
    "def generate_text_with_vt(model_path: str, prompt: str, max_length: int = 100, device: str = 'cuda'):\n",
    "    \"\"\"Generate text using trained VT model\"\"\"\n",
    "\n",
    "    # Load model and tokenizer\n",
    "    model = AdaptiveVectorTransitionT5.from_pretrained(model_path)\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(prompt, return_tensors='pt', max_length=512, truncation=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            num_beams=4,\n",
    "            early_stopping=True,\n",
    "            do_sample=False\n",
    "        )\n",
    "\n",
    "    # Decode\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    # Get VT statistics\n",
    "    vt_stats = model.get_delta_statistics()\n",
    "\n",
    "    print(f\"üéØ Generated Text:\")\n",
    "    print(f\"   Input: {prompt}\")\n",
    "    print(f\"   Output: {generated_text}\")\n",
    "    print(f\"   VT Applications: {vt_stats['applications']}\")\n",
    "    print(f\"   Avg Delta Magnitude: {vt_stats['avg_delta_magnitude']:.6f}\")\n",
    "\n",
    "    return generated_text, vt_stats\n",
    "\n",
    "# =============================================================================\n",
    "# EXAMPLE USAGE AND MAIN EXECUTION\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üéØ VT T5 with Curriculum Learning - Complete Training Pipeline\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Configuration\n",
    "    config = VTCurriculumConfig()\n",
    "\n",
    "    # Customize paths for your environment\n",
    "    config.data_dir = '/content/baseline-pretraining/babylm_data'\n",
    "    config.save_dir = '/content/drive/MyDrive/llm-project/t5-small-new-base_datapreparation/vt_t5_curriculum_training_first_layer_modification'\n",
    "    config.cache_dir = '/content/drive/MyDrive/llm-project/t5-small-new-base_datapreparation/vt_t5_curriculum_cache'\n",
    "\n",
    "    # Training parameters\n",
    "    config.num_epochs = 5\n",
    "    config.batch_size = 4\n",
    "    config.max_train_examples = 30000\n",
    "    config.max_val_examples = 2000\n",
    "    config.curriculum_strategy = 'progressive'\n",
    "\n",
    "    # VT-specific parameters\n",
    "    config.delta_weight = 0.4\n",
    "    config.adaptive_delta = True\n",
    "    config.apply_to_encoder = True\n",
    "    config.apply_to_decoder = True\n",
    "\n",
    "    try:\n",
    "        # Apply optimizations\n",
    "        optimize_for_colab_t4()\n",
    "\n",
    "        # Train VT model with curriculum\n",
    "        print(\"üöÄ Starting VT T5 training...\")\n",
    "        results = train_vt_curriculum_model(config)\n",
    "\n",
    "        print(\"\\n‚úÖ VT Curriculum Training completed successfully!\")\n",
    "        print(f\"üìÅ Model saved to: {config.save_dir}\")\n",
    "\n",
    "        # Print training statistics\n",
    "        stats = results['training_stats']\n",
    "        print(f\"\\nüìä Training Statistics:\")\n",
    "        print(f\"   Final train loss: {stats['epoch_losses'][-1]:.4f}\")\n",
    "        if stats['val_losses'] and stats['val_losses'][-1] > 0:\n",
    "            print(f\"   Final val loss: {stats['val_losses'][-1]:.4f}\")\n",
    "        print(f\"   Curriculum progression: {stats['curriculum_levels']}\")\n",
    "\n",
    "        # VT-specific statistics\n",
    "        if stats['vt_statistics']:\n",
    "            final_vt_stats = stats['vt_statistics'][-1]\n",
    "            print(f\"   VT applications: {final_vt_stats['applications']:,}\")\n",
    "            print(f\"   Avg delta magnitude: {final_vt_stats['avg_delta_magnitude']:.6f}\")\n",
    "\n",
    "        # Optional: Test the trained model\n",
    "        print(\"\\nüß™ Testing trained model...\")\n",
    "        model_path = os.path.join(config.save_dir, 'final_model')\n",
    "\n",
    "        # Example text generation\n",
    "        test_prompts = [\n",
    "            \"summarize: The weather today is very nice.\",\n",
    "            \"complete: Once upon a time, there was a\",\n",
    "            \"The quick brown fox\"\n",
    "        ]\n",
    "\n",
    "        for prompt in test_prompts:\n",
    "            try:\n",
    "                generated, vt_stats = generate_text_with_vt(model_path, prompt, max_length=50, device=config.device)\n",
    "                print(f\"‚úÖ Generated successfully for: '{prompt[:30]}...'\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Generation failed for '{prompt[:30]}...': {e}\")\n",
    "\n",
    "        # Optional: Evaluate on test data\n",
    "        test_data_path = os.path.join(config.data_dir, 'babylm_dev.txt')\n",
    "        if os.path.exists(test_data_path):\n",
    "            print(f\"\\nüìä Evaluating on test data...\")\n",
    "            try:\n",
    "                eval_results = evaluate_vt_model(model_path, test_data_path, batch_size=config.batch_size, device=config.device)\n",
    "                print(f\"‚úÖ Evaluation completed successfully!\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Evaluation failed: {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Training failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "    print(\"\\nüèÅ Script execution completed!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
