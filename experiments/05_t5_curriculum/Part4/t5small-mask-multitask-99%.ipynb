{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# t5-small finetuned char word sentence level tokenized and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "!pip uninstall -y numpy transformers datasets\n",
    "!pip install numpy --force-reinstall --no-cache-dir\n",
    "!pip install transformers datasets --force-reinstall --no-cache-dir\n",
    "\n",
    "\n",
    "os.kill(os.getpid(), 9)  # Restart the Colab runtime (REQUIRED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/babylm/baseline-pretraining.git\n",
    "%cd baseline-pretraining\n",
    "!wget -O babylm_data.zip \"https://files.osf.io/v1/resources/ad7qg/providers/osfstorage/661517db943bee3731dfec25/?zip=\"\n",
    "!unzip babylm_data.zip -d babylm_data\n",
    "!unzip babylm_data/train_10M.zip -d babylm_data/train_10M\n",
    "!unzip babylm_data/dev.zip -d babylm_data/dev\n",
    "!unzip babylm_data/test.zip -d babylm_data/test\n",
    "!cat babylm_data/train_10M/train_10M/*.train > babylm_data/babylm_train.txt\n",
    "!cat babylm_data/dev/dev/*.dev > babylm_data/babylm_dev.txt\n",
    "!cat babylm_data/test/test/*.test > babylm_data/babylm_test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "from datasets import Dataset, DatasetDict\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# ==========\n",
    "# Setup\n",
    "# ==========\n",
    "DATA_DIR = Path(\"/content/baseline-pretraining/babylm_data\")\n",
    "assert DATA_DIR.exists(), \"Dataset folder not found!\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# ==========\n",
    "# Load Model & Tokenizer\n",
    "# ==========\n",
    "model_path = \"/content/drive/MyDrive/llm-project/t5-small-babylm-denoised\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_path, extra_ids=0)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_path).to(device)\n",
    "\n",
    "# ==========\n",
    "# Load & Clean Dataset\n",
    "# ==========\n",
    "def load_text_file_as_dataset(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "    lines = [line.strip() for line in lines if line.strip()]\n",
    "    return Dataset.from_list([{\"text\": line} for line in lines])\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": load_text_file_as_dataset(DATA_DIR / \"babylm_train.txt\"),\n",
    "    \"validation\": load_text_file_as_dataset(DATA_DIR / \"babylm_dev.txt\"),\n",
    "    \"test\": load_text_file_as_dataset(DATA_DIR / \"babylm_test.txt\"),\n",
    "})\n",
    "\n",
    "# Clean and filter\n",
    "dataset = dataset.filter(lambda x: x['text'] and x['text'].strip())\n",
    "\n",
    "# Optional: reduce size\n",
    "dataset['train'] = dataset['train'].select(range(min(100000, len(dataset['train']))))\n",
    "dataset['validation'] = dataset['validation'].select(range(min(5000, len(dataset['validation']))))\n",
    "dataset['test'] = dataset['test'].select(range(min(5000, len(dataset['test']))))\n",
    "\n",
    "# Filter short lines\n",
    "def is_valid(example):\n",
    "    return len(example['text'].split()) >= 3\n",
    "\n",
    "dataset = dataset.filter(is_valid)\n",
    "\n",
    "# ==========\n",
    "# Multi-Level Tokenization\n",
    "# ==========\n",
    "MAX_LEN = 64\n",
    "\n",
    "# Prepares input-target pairs for next-char, next-word, next-sentence\n",
    "# Join them with <sep> so the model gets all tasks in parallel\n",
    "\n",
    "def multi_task_example(example):\n",
    "    text = example[\"text\"]\n",
    "\n",
    "    # Char-level\n",
    "    char_input = text[:-1]\n",
    "    char_target = text[-1]\n",
    "\n",
    "    # Word-level\n",
    "    words = text.split()\n",
    "    word_input = \" \".join(words[:-1])\n",
    "    word_target = words[-1]\n",
    "\n",
    "    # Sentence-level (very simple split)\n",
    "    sentences = text.split(\".\")\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    if len(sentences) < 2:\n",
    "        sent_input = text\n",
    "        sent_target = \"\"\n",
    "    else:\n",
    "        sent_input = \". \".join(sentences[:-1])\n",
    "        sent_target = sentences[-1]\n",
    "\n",
    "    input_text = f\"char: {char_input} <sep> word: {word_input} <sep> sent: {sent_input}\"\n",
    "    target_text = f\"{char_target} <sep> {word_target} <sep> {sent_target}\"\n",
    "\n",
    "    tokenized = tokenizer(\n",
    "        input_text,\n",
    "        text_target=target_text,\n",
    "        max_length=MAX_LEN,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": tokenized[\"input_ids\"],\n",
    "        \"attention_mask\": tokenized[\"attention_mask\"],\n",
    "        \"labels\": tokenized[\"labels\"]\n",
    "    }\n",
    "\n",
    "\n",
    "# Apply tokenization\n",
    "for split in dataset:\n",
    "    dataset[split] = dataset[split].map(multi_task_example, remove_columns=[\"text\"])\n",
    "    dataset[split].set_format(type=\"torch\")\n",
    "\n",
    "# ==========\n",
    "# Data Collator\n",
    "# ==========\n",
    "collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=-100,\n",
    "    padding=\"longest\"\n",
    ")\n",
    "\n",
    "# ==========\n",
    "# Training\n",
    "# ==========\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"/content/drive/MyDrive/llm-project/t5-small-multitask-finetuned\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=200,\n",
    "    logging_steps=500,\n",
    "    eval_steps=1000,\n",
    "    save_steps=1000,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=[],\n",
    "    label_smoothing_factor=0.1,\n",
    "    remove_unused_columns=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    data_collator=collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# ==========\n",
    "# Evaluate & Save\n",
    "# ==========\n",
    "test_results = trainer.evaluate(dataset[\"test\"], metric_key_prefix=\"test\")\n",
    "print(\"Test Results:\", test_results)\n",
    "\n",
    "trainer.save_model()\n",
    "tokenizer.save_pretrained(args.output_dir)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
